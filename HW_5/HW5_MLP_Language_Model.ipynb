{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "624b39df",
      "metadata": {
        "id": "624b39df"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import tqdm\n",
        "import math\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "939a704e",
      "metadata": {
        "id": "939a704e"
      },
      "outputs": [],
      "source": [
        "FILL_IN = \"FILL_IN\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "names = []\n",
        "names_path = '/content/drive/MyDrive/ADL_HW/names.txt'\n",
        "with open(names_path, 'r') as f:\n",
        "    names = f.read().splitlines()"
      ],
      "metadata": {
        "id": "ENGCaduhcDV4",
        "outputId": "9836fb64-8668-465e-fe0b-6f21a655d284",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ENGCaduhcDV4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names"
      ],
      "metadata": {
        "id": "qJ79WW1Uc4Of",
        "outputId": "e0777eab-18ab-48e9-a7c2-824faf0446c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qJ79WW1Uc4Of",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn',\n",
              " 'abigail',\n",
              " 'emily',\n",
              " 'elizabeth',\n",
              " 'mila',\n",
              " 'ella',\n",
              " 'avery',\n",
              " 'sofia',\n",
              " 'camila',\n",
              " 'aria',\n",
              " 'scarlett',\n",
              " 'victoria',\n",
              " 'madison',\n",
              " 'luna',\n",
              " 'grace',\n",
              " 'chloe',\n",
              " 'penelope',\n",
              " 'layla',\n",
              " 'riley',\n",
              " 'zoey',\n",
              " 'nora',\n",
              " 'lily',\n",
              " 'eleanor',\n",
              " 'hannah',\n",
              " 'lillian',\n",
              " 'addison',\n",
              " 'aubrey',\n",
              " 'ellie',\n",
              " 'stella',\n",
              " 'natalie',\n",
              " 'zoe',\n",
              " 'leah',\n",
              " 'hazel',\n",
              " 'violet',\n",
              " 'aurora',\n",
              " 'savannah',\n",
              " 'audrey',\n",
              " 'brooklyn',\n",
              " 'bella',\n",
              " 'claire',\n",
              " 'skylar',\n",
              " 'lucy',\n",
              " 'paisley',\n",
              " 'everly',\n",
              " 'anna',\n",
              " 'caroline',\n",
              " 'nova',\n",
              " 'genesis',\n",
              " 'emilia',\n",
              " 'kennedy',\n",
              " 'samantha',\n",
              " 'maya',\n",
              " 'willow',\n",
              " 'kinsley',\n",
              " 'naomi',\n",
              " 'aaliyah',\n",
              " 'elena',\n",
              " 'sarah',\n",
              " 'ariana',\n",
              " 'allison',\n",
              " 'gabriella',\n",
              " 'alice',\n",
              " 'madelyn',\n",
              " 'cora',\n",
              " 'ruby',\n",
              " 'eva',\n",
              " 'serenity',\n",
              " 'autumn',\n",
              " 'adeline',\n",
              " 'hailey',\n",
              " 'gianna',\n",
              " 'valentina',\n",
              " 'isla',\n",
              " 'eliana',\n",
              " 'quinn',\n",
              " 'nevaeh',\n",
              " 'ivy',\n",
              " 'sadie',\n",
              " 'piper',\n",
              " 'lydia',\n",
              " 'alexa',\n",
              " 'josephine',\n",
              " 'emery',\n",
              " 'julia',\n",
              " 'delilah',\n",
              " 'arianna',\n",
              " 'vivian',\n",
              " 'kaylee',\n",
              " 'sophie',\n",
              " 'brielle',\n",
              " 'madeline',\n",
              " 'peyton',\n",
              " 'rylee',\n",
              " 'clara',\n",
              " 'hadley',\n",
              " 'melanie',\n",
              " 'mackenzie',\n",
              " 'reagan',\n",
              " 'adalynn',\n",
              " 'liliana',\n",
              " 'aubree',\n",
              " 'jade',\n",
              " 'katherine',\n",
              " 'isabelle',\n",
              " 'natalia',\n",
              " 'raelynn',\n",
              " 'maria',\n",
              " 'athena',\n",
              " 'ximena',\n",
              " 'arya',\n",
              " 'leilani',\n",
              " 'taylor',\n",
              " 'faith',\n",
              " 'rose',\n",
              " 'kylie',\n",
              " 'alexandra',\n",
              " 'mary',\n",
              " 'margaret',\n",
              " 'lyla',\n",
              " 'ashley',\n",
              " 'amaya',\n",
              " 'eliza',\n",
              " 'brianna',\n",
              " 'bailey',\n",
              " 'andrea',\n",
              " 'khloe',\n",
              " 'jasmine',\n",
              " 'melody',\n",
              " 'iris',\n",
              " 'isabel',\n",
              " 'norah',\n",
              " 'annabelle',\n",
              " 'valeria',\n",
              " 'emerson',\n",
              " 'adalyn',\n",
              " 'ryleigh',\n",
              " 'eden',\n",
              " 'emersyn',\n",
              " 'anastasia',\n",
              " 'kayla',\n",
              " 'alyssa',\n",
              " 'juliana',\n",
              " 'charlie',\n",
              " 'esther',\n",
              " 'ariel',\n",
              " 'cecilia',\n",
              " 'valerie',\n",
              " 'alina',\n",
              " 'molly',\n",
              " 'reese',\n",
              " 'aliyah',\n",
              " 'lilly',\n",
              " 'parker',\n",
              " 'finley',\n",
              " 'morgan',\n",
              " 'sydney',\n",
              " 'jordyn',\n",
              " 'eloise',\n",
              " 'trinity',\n",
              " 'daisy',\n",
              " 'kimberly',\n",
              " 'lauren',\n",
              " 'genevieve',\n",
              " 'sara',\n",
              " 'arabella',\n",
              " 'harmony',\n",
              " 'elise',\n",
              " 'remi',\n",
              " 'teagan',\n",
              " 'alexis',\n",
              " 'london',\n",
              " 'sloane',\n",
              " 'laila',\n",
              " 'lucia',\n",
              " 'diana',\n",
              " 'juliette',\n",
              " 'sienna',\n",
              " 'elliana',\n",
              " 'londyn',\n",
              " 'ayla',\n",
              " 'callie',\n",
              " 'gracie',\n",
              " 'josie',\n",
              " 'amara',\n",
              " 'jocelyn',\n",
              " 'daniela',\n",
              " 'everleigh',\n",
              " 'mya',\n",
              " 'rachel',\n",
              " 'summer',\n",
              " 'alana',\n",
              " 'brooke',\n",
              " 'alaina',\n",
              " 'mckenzie',\n",
              " 'catherine',\n",
              " 'amy',\n",
              " 'presley',\n",
              " 'journee',\n",
              " 'rosalie',\n",
              " 'ember',\n",
              " 'brynlee',\n",
              " 'rowan',\n",
              " 'joanna',\n",
              " 'paige',\n",
              " 'rebecca',\n",
              " 'ana',\n",
              " 'sawyer',\n",
              " 'mariah',\n",
              " 'nicole',\n",
              " 'brooklynn',\n",
              " 'payton',\n",
              " 'marley',\n",
              " 'fiona',\n",
              " 'georgia',\n",
              " 'lila',\n",
              " 'harley',\n",
              " 'adelyn',\n",
              " 'alivia',\n",
              " 'noelle',\n",
              " 'gemma',\n",
              " 'vanessa',\n",
              " 'journey',\n",
              " 'makayla',\n",
              " 'angelina',\n",
              " 'adaline',\n",
              " 'catalina',\n",
              " 'alayna',\n",
              " 'julianna',\n",
              " 'leila',\n",
              " 'lola',\n",
              " 'adriana',\n",
              " 'june',\n",
              " 'juliet',\n",
              " 'jayla',\n",
              " 'river',\n",
              " 'tessa',\n",
              " 'lia',\n",
              " 'dakota',\n",
              " 'delaney',\n",
              " 'selena',\n",
              " 'blakely',\n",
              " 'ada',\n",
              " 'camille',\n",
              " 'zara',\n",
              " 'malia',\n",
              " 'hope',\n",
              " 'samara',\n",
              " 'vera',\n",
              " 'mckenna',\n",
              " 'briella',\n",
              " 'izabella',\n",
              " 'hayden',\n",
              " 'raegan',\n",
              " 'michelle',\n",
              " 'angela',\n",
              " 'ruth',\n",
              " 'freya',\n",
              " 'kamila',\n",
              " 'vivienne',\n",
              " 'aspen',\n",
              " 'olive',\n",
              " 'kendall',\n",
              " 'elaina',\n",
              " 'thea',\n",
              " 'kali',\n",
              " 'destiny',\n",
              " 'amiyah',\n",
              " 'evangeline',\n",
              " 'cali',\n",
              " 'blake',\n",
              " 'elsie',\n",
              " 'juniper',\n",
              " 'alexandria',\n",
              " 'myla',\n",
              " 'ariella',\n",
              " 'kate',\n",
              " 'mariana',\n",
              " 'lilah',\n",
              " 'charlee',\n",
              " 'daleyza',\n",
              " 'nyla',\n",
              " 'jane',\n",
              " 'maggie',\n",
              " 'zuri',\n",
              " 'aniyah',\n",
              " 'lucille',\n",
              " 'leia',\n",
              " 'melissa',\n",
              " 'adelaide',\n",
              " 'amina',\n",
              " 'giselle',\n",
              " 'lena',\n",
              " 'camilla',\n",
              " 'miriam',\n",
              " 'millie',\n",
              " 'brynn',\n",
              " 'gabrielle',\n",
              " 'sage',\n",
              " 'annie',\n",
              " 'logan',\n",
              " 'lilliana',\n",
              " 'haven',\n",
              " 'jessica',\n",
              " 'kaia',\n",
              " 'magnolia',\n",
              " 'amira',\n",
              " 'adelynn',\n",
              " 'makenzie',\n",
              " 'stephanie',\n",
              " 'nina',\n",
              " 'phoebe',\n",
              " 'arielle',\n",
              " 'evie',\n",
              " 'lyric',\n",
              " 'alessandra',\n",
              " 'gabriela',\n",
              " 'paislee',\n",
              " 'raelyn',\n",
              " 'madilyn',\n",
              " 'paris',\n",
              " 'makenna',\n",
              " 'kinley',\n",
              " 'gracelyn',\n",
              " 'talia',\n",
              " 'maeve',\n",
              " 'rylie',\n",
              " 'kiara',\n",
              " 'evelynn',\n",
              " 'brinley',\n",
              " 'jacqueline',\n",
              " 'laura',\n",
              " 'gracelynn',\n",
              " 'lexi',\n",
              " 'ariah',\n",
              " 'fatima',\n",
              " 'jennifer',\n",
              " 'kehlani',\n",
              " 'alani',\n",
              " 'ariyah',\n",
              " 'luciana',\n",
              " 'allie',\n",
              " 'heidi',\n",
              " 'maci',\n",
              " 'phoenix',\n",
              " 'felicity',\n",
              " 'joy',\n",
              " 'kenzie',\n",
              " 'veronica',\n",
              " 'margot',\n",
              " 'addilyn',\n",
              " 'lana',\n",
              " 'cassidy',\n",
              " 'remington',\n",
              " 'saylor',\n",
              " 'ryan',\n",
              " 'keira',\n",
              " 'harlow',\n",
              " 'miranda',\n",
              " 'angel',\n",
              " 'amanda',\n",
              " 'daniella',\n",
              " 'royalty',\n",
              " 'gwendolyn',\n",
              " 'ophelia',\n",
              " 'heaven',\n",
              " 'jordan',\n",
              " 'madeleine',\n",
              " 'esmeralda',\n",
              " 'kira',\n",
              " 'miracle',\n",
              " 'elle',\n",
              " 'amari',\n",
              " 'danielle',\n",
              " 'daphne',\n",
              " 'willa',\n",
              " 'haley',\n",
              " 'gia',\n",
              " 'kaitlyn',\n",
              " 'oakley',\n",
              " 'kailani',\n",
              " 'winter',\n",
              " 'alicia',\n",
              " 'serena',\n",
              " 'nadia',\n",
              " 'aviana',\n",
              " 'demi',\n",
              " 'jada',\n",
              " 'braelynn',\n",
              " 'dylan',\n",
              " 'ainsley',\n",
              " 'alison',\n",
              " 'camryn',\n",
              " 'avianna',\n",
              " 'bianca',\n",
              " 'skyler',\n",
              " 'scarlet',\n",
              " 'maddison',\n",
              " 'nylah',\n",
              " 'sarai',\n",
              " 'regina',\n",
              " 'dahlia',\n",
              " 'nayeli',\n",
              " 'raven',\n",
              " 'helen',\n",
              " 'adrianna',\n",
              " 'averie',\n",
              " 'skye',\n",
              " 'kelsey',\n",
              " 'tatum',\n",
              " 'kensley',\n",
              " 'maliyah',\n",
              " 'erin',\n",
              " 'viviana',\n",
              " 'jenna',\n",
              " 'anaya',\n",
              " 'carolina',\n",
              " 'shelby',\n",
              " 'sabrina',\n",
              " 'mikayla',\n",
              " 'annalise',\n",
              " 'octavia',\n",
              " 'lennon',\n",
              " 'blair',\n",
              " 'carmen',\n",
              " 'yaretzi',\n",
              " 'kennedi',\n",
              " 'mabel',\n",
              " 'zariah',\n",
              " 'kyla',\n",
              " 'christina',\n",
              " 'selah',\n",
              " 'celeste',\n",
              " 'eve',\n",
              " 'mckinley',\n",
              " 'milani',\n",
              " 'frances',\n",
              " 'jimena',\n",
              " 'kylee',\n",
              " 'leighton',\n",
              " 'katie',\n",
              " 'aitana',\n",
              " 'kayleigh',\n",
              " 'sierra',\n",
              " 'kathryn',\n",
              " 'rosemary',\n",
              " 'jolene',\n",
              " 'alondra',\n",
              " 'elisa',\n",
              " 'helena',\n",
              " 'charleigh',\n",
              " 'hallie',\n",
              " 'lainey',\n",
              " 'avah',\n",
              " 'jazlyn',\n",
              " 'kamryn',\n",
              " 'mira',\n",
              " 'cheyenne',\n",
              " 'francesca',\n",
              " 'antonella',\n",
              " 'wren',\n",
              " 'chelsea',\n",
              " 'amber',\n",
              " 'emory',\n",
              " 'lorelei',\n",
              " 'nia',\n",
              " 'abby',\n",
              " 'april',\n",
              " 'emelia',\n",
              " 'carter',\n",
              " 'aylin',\n",
              " 'cataleya',\n",
              " 'bethany',\n",
              " 'marlee',\n",
              " 'carly',\n",
              " 'kaylani',\n",
              " 'emely',\n",
              " 'liana',\n",
              " 'madelynn',\n",
              " 'cadence',\n",
              " 'matilda',\n",
              " 'sylvia',\n",
              " 'myra',\n",
              " 'fernanda',\n",
              " 'oaklyn',\n",
              " 'elianna',\n",
              " 'hattie',\n",
              " 'dayana',\n",
              " 'kendra',\n",
              " 'maisie',\n",
              " 'malaysia',\n",
              " 'kara',\n",
              " 'katelyn',\n",
              " 'maia',\n",
              " 'celine',\n",
              " 'cameron',\n",
              " 'renata',\n",
              " 'jayleen',\n",
              " 'charli',\n",
              " 'emmalyn',\n",
              " 'holly',\n",
              " 'azalea',\n",
              " 'leona',\n",
              " 'alejandra',\n",
              " 'bristol',\n",
              " 'collins',\n",
              " 'imani',\n",
              " 'meadow',\n",
              " 'alexia',\n",
              " 'edith',\n",
              " 'kaydence',\n",
              " 'leslie',\n",
              " 'lilith',\n",
              " 'kora',\n",
              " 'aisha',\n",
              " 'meredith',\n",
              " 'danna',\n",
              " 'wynter',\n",
              " 'emberly',\n",
              " 'julieta',\n",
              " 'michaela',\n",
              " 'alayah',\n",
              " 'jemma',\n",
              " 'reign',\n",
              " 'colette',\n",
              " 'kaliyah',\n",
              " 'elliott',\n",
              " 'johanna',\n",
              " 'remy',\n",
              " 'sutton',\n",
              " 'emmy',\n",
              " 'virginia',\n",
              " 'briana',\n",
              " 'oaklynn',\n",
              " 'adelina',\n",
              " 'everlee',\n",
              " 'megan',\n",
              " 'angelica',\n",
              " 'justice',\n",
              " 'mariam',\n",
              " 'khaleesi',\n",
              " 'macie',\n",
              " 'karsyn',\n",
              " 'alanna',\n",
              " 'aleah',\n",
              " 'mae',\n",
              " 'mallory',\n",
              " 'esme',\n",
              " 'skyla',\n",
              " 'madilynn',\n",
              " 'charley',\n",
              " 'allyson',\n",
              " 'hanna',\n",
              " 'shiloh',\n",
              " 'henley',\n",
              " 'macy',\n",
              " 'maryam',\n",
              " 'ivanna',\n",
              " 'ashlynn',\n",
              " 'lorelai',\n",
              " 'amora',\n",
              " 'ashlyn',\n",
              " 'sasha',\n",
              " 'baylee',\n",
              " 'beatrice',\n",
              " 'itzel',\n",
              " 'priscilla',\n",
              " 'marie',\n",
              " 'jayda',\n",
              " 'liberty',\n",
              " 'rory',\n",
              " 'alessia',\n",
              " 'alaia',\n",
              " 'janelle',\n",
              " 'kalani',\n",
              " 'gloria',\n",
              " 'sloan',\n",
              " 'dorothy',\n",
              " 'greta',\n",
              " 'julie',\n",
              " 'zahra',\n",
              " 'savanna',\n",
              " 'annabella',\n",
              " 'poppy',\n",
              " 'amalia',\n",
              " 'zaylee',\n",
              " 'cecelia',\n",
              " 'coraline',\n",
              " 'kimber',\n",
              " 'emmie',\n",
              " 'anne',\n",
              " 'karina',\n",
              " 'kassidy',\n",
              " 'kynlee',\n",
              " 'monroe',\n",
              " 'anahi',\n",
              " 'jaliyah',\n",
              " 'jazmin',\n",
              " 'maren',\n",
              " 'monica',\n",
              " 'siena',\n",
              " 'marilyn',\n",
              " 'reyna',\n",
              " 'kyra',\n",
              " 'lilian',\n",
              " 'jamie',\n",
              " 'melany',\n",
              " 'alaya',\n",
              " 'ariya',\n",
              " 'kelly',\n",
              " 'rosie',\n",
              " 'adley',\n",
              " 'dream',\n",
              " 'jaylah',\n",
              " 'laurel',\n",
              " 'jazmine',\n",
              " 'mina',\n",
              " 'karla',\n",
              " 'bailee',\n",
              " 'aubrie',\n",
              " 'katalina',\n",
              " 'melina',\n",
              " 'harlee',\n",
              " 'elliot',\n",
              " 'hayley',\n",
              " 'elaine',\n",
              " 'karen',\n",
              " 'dallas',\n",
              " 'irene',\n",
              " 'lylah',\n",
              " 'ivory',\n",
              " 'chaya',\n",
              " 'rosa',\n",
              " 'aleena',\n",
              " 'braelyn',\n",
              " 'nola',\n",
              " 'alma',\n",
              " 'leyla',\n",
              " 'pearl',\n",
              " 'addyson',\n",
              " 'roselyn',\n",
              " 'lacey',\n",
              " 'lennox',\n",
              " 'reina',\n",
              " 'aurelia',\n",
              " 'noa',\n",
              " 'janiyah',\n",
              " 'jessie',\n",
              " 'madisyn',\n",
              " 'saige',\n",
              " 'alia',\n",
              " 'tiana',\n",
              " 'astrid',\n",
              " 'cassandra',\n",
              " 'kyleigh',\n",
              " 'romina',\n",
              " 'stevie',\n",
              " 'haylee',\n",
              " 'zelda',\n",
              " 'lillie',\n",
              " 'aileen',\n",
              " 'brylee',\n",
              " 'eileen',\n",
              " 'yara',\n",
              " 'ensley',\n",
              " 'lauryn',\n",
              " 'giuliana',\n",
              " 'livia',\n",
              " 'anya',\n",
              " 'mikaela',\n",
              " 'palmer',\n",
              " 'lyra',\n",
              " 'mara',\n",
              " 'marina',\n",
              " 'kailey',\n",
              " 'liv',\n",
              " 'clementine',\n",
              " 'kenna',\n",
              " 'briar',\n",
              " 'emerie',\n",
              " 'galilea',\n",
              " 'tiffany',\n",
              " 'bonnie',\n",
              " 'elyse',\n",
              " 'cynthia',\n",
              " 'frida',\n",
              " 'kinslee',\n",
              " 'tatiana',\n",
              " 'joelle',\n",
              " 'armani',\n",
              " 'jolie',\n",
              " 'nalani',\n",
              " 'rayna',\n",
              " 'yareli',\n",
              " 'meghan',\n",
              " 'rebekah',\n",
              " 'addilynn',\n",
              " 'faye',\n",
              " 'zariyah',\n",
              " 'lea',\n",
              " 'aliza',\n",
              " 'julissa',\n",
              " 'lilyana',\n",
              " 'anika',\n",
              " 'kairi',\n",
              " 'aniya',\n",
              " 'noemi',\n",
              " 'angie',\n",
              " 'crystal',\n",
              " 'bridget',\n",
              " 'ari',\n",
              " 'davina',\n",
              " 'amelie',\n",
              " 'amirah',\n",
              " 'annika',\n",
              " 'elora',\n",
              " 'xiomara',\n",
              " 'linda',\n",
              " 'hana',\n",
              " 'laney',\n",
              " 'mercy',\n",
              " 'hadassah',\n",
              " 'madalyn',\n",
              " 'louisa',\n",
              " 'simone',\n",
              " 'kori',\n",
              " 'jillian',\n",
              " 'alena',\n",
              " 'malaya',\n",
              " 'miley',\n",
              " 'milan',\n",
              " 'sariyah',\n",
              " 'malani',\n",
              " 'clarissa',\n",
              " 'nala',\n",
              " 'princess',\n",
              " 'amani',\n",
              " 'analia',\n",
              " 'estella',\n",
              " 'milana',\n",
              " 'aya',\n",
              " 'chana',\n",
              " 'jayde',\n",
              " 'tenley',\n",
              " 'zaria',\n",
              " 'itzayana',\n",
              " 'penny',\n",
              " 'ailani',\n",
              " 'lara',\n",
              " 'aubriella',\n",
              " 'clare',\n",
              " 'lina',\n",
              " 'rhea',\n",
              " 'bria',\n",
              " 'thalia',\n",
              " 'keyla',\n",
              " 'haisley',\n",
              " 'ryann',\n",
              " 'addisyn',\n",
              " 'amaia',\n",
              " 'chanel',\n",
              " 'ellen',\n",
              " 'harmoni',\n",
              " 'aliana',\n",
              " 'tinsley',\n",
              " 'landry',\n",
              " 'paisleigh',\n",
              " 'lexie',\n",
              " 'myah',\n",
              " 'rylan',\n",
              " 'deborah',\n",
              " 'emilee',\n",
              " 'laylah',\n",
              " 'novalee',\n",
              " 'ellis',\n",
              " 'emmeline',\n",
              " 'avalynn',\n",
              " 'hadlee',\n",
              " 'legacy',\n",
              " 'braylee',\n",
              " 'elisabeth',\n",
              " 'kaylie',\n",
              " 'ansley',\n",
              " 'dior',\n",
              " 'paula',\n",
              " 'belen',\n",
              " 'corinne',\n",
              " 'maleah',\n",
              " 'martha',\n",
              " 'teresa',\n",
              " 'salma',\n",
              " 'louise',\n",
              " 'averi',\n",
              " 'lilianna',\n",
              " 'amiya',\n",
              " 'milena',\n",
              " 'royal',\n",
              " 'aubrielle',\n",
              " 'calliope',\n",
              " 'frankie',\n",
              " 'natasha',\n",
              " 'kamilah',\n",
              " 'meilani',\n",
              " 'raina',\n",
              " 'amayah',\n",
              " 'lailah',\n",
              " 'rayne',\n",
              " 'zaniyah',\n",
              " 'isabela',\n",
              " 'nathalie',\n",
              " 'miah',\n",
              " 'opal',\n",
              " 'kenia',\n",
              " 'azariah',\n",
              " 'hunter',\n",
              " 'tori',\n",
              " 'andi',\n",
              " 'keily',\n",
              " 'leanna',\n",
              " 'scarlette',\n",
              " 'jaelyn',\n",
              " 'saoirse',\n",
              " 'selene',\n",
              " 'dalary',\n",
              " 'lindsey',\n",
              " 'marianna',\n",
              " 'ramona',\n",
              " 'estelle',\n",
              " 'giovanna',\n",
              " 'holland',\n",
              " 'nancy',\n",
              " 'emmalynn',\n",
              " 'mylah',\n",
              " 'rosalee',\n",
              " 'sariah',\n",
              " 'zoie',\n",
              " 'blaire',\n",
              " 'lyanna',\n",
              " 'maxine',\n",
              " 'anais',\n",
              " 'dana',\n",
              " 'judith',\n",
              " 'kiera',\n",
              " 'jaelynn',\n",
              " 'noor',\n",
              " 'kai',\n",
              " 'adalee',\n",
              " 'oaklee',\n",
              " 'amaris',\n",
              " 'jaycee',\n",
              " 'belle',\n",
              " 'carolyn',\n",
              " 'della',\n",
              " 'karter',\n",
              " 'sky',\n",
              " 'treasure',\n",
              " 'vienna',\n",
              " 'jewel',\n",
              " 'rivka',\n",
              " 'rosalyn',\n",
              " 'alannah',\n",
              " 'ellianna',\n",
              " 'sunny',\n",
              " 'claudia',\n",
              " 'cara',\n",
              " 'hailee',\n",
              " 'estrella',\n",
              " 'harleigh',\n",
              " 'zhavia',\n",
              " 'alianna',\n",
              " 'brittany',\n",
              " 'jaylene',\n",
              " 'journi',\n",
              " 'marissa',\n",
              " 'mavis',\n",
              " 'iliana',\n",
              " 'jurnee',\n",
              " 'aislinn',\n",
              " 'alyson',\n",
              " 'elsa',\n",
              " 'kamiyah',\n",
              " 'kiana',\n",
              " 'lisa',\n",
              " 'arlette',\n",
              " 'kadence',\n",
              " 'kathleen',\n",
              " 'halle',\n",
              " 'erika',\n",
              " 'sylvie',\n",
              " 'adele',\n",
              " 'erica',\n",
              " 'veda',\n",
              " 'whitney',\n",
              " 'bexley',\n",
              " 'emmaline',\n",
              " 'guadalupe',\n",
              " 'august',\n",
              " 'brynleigh',\n",
              " 'gwen',\n",
              " 'promise',\n",
              " 'alisson',\n",
              " 'india',\n",
              " 'madalynn',\n",
              " 'paloma',\n",
              " 'patricia',\n",
              " 'samira',\n",
              " 'aliya',\n",
              " 'casey',\n",
              " 'jazlynn',\n",
              " 'paulina',\n",
              " 'dulce',\n",
              " 'kallie',\n",
              " 'perla',\n",
              " 'adrienne',\n",
              " 'alora',\n",
              " 'nataly',\n",
              " 'ayleen',\n",
              " 'christine',\n",
              " 'kaiya',\n",
              " 'ariadne',\n",
              " 'karlee',\n",
              " 'barbara',\n",
              " 'lillianna',\n",
              " 'raquel',\n",
              " 'saniyah',\n",
              " 'yamileth',\n",
              " 'arely',\n",
              " 'celia',\n",
              " 'heavenly',\n",
              " 'kaylin',\n",
              " 'marisol',\n",
              " 'marleigh',\n",
              " 'avalyn',\n",
              " 'berkley',\n",
              " 'kataleya',\n",
              " 'zainab',\n",
              " 'dani',\n",
              " 'egypt',\n",
              " 'joyce',\n",
              " 'kenley',\n",
              " 'annabel',\n",
              " 'kaelyn',\n",
              " 'etta',\n",
              " 'hadleigh',\n",
              " 'joselyn',\n",
              " 'luella',\n",
              " 'jaylee',\n",
              " 'zola',\n",
              " 'alisha',\n",
              " 'ezra',\n",
              " 'queen',\n",
              " 'amia',\n",
              " 'annalee',\n",
              " 'bellamy',\n",
              " 'paola',\n",
              " 'tinley',\n",
              " 'violeta',\n",
              " 'jenesis',\n",
              " 'arden',\n",
              " 'giana',\n",
              " 'wendy',\n",
              " 'ellison',\n",
              " 'florence',\n",
              " 'margo',\n",
              " 'naya',\n",
              " 'robin',\n",
              " 'sandra',\n",
              " 'scout',\n",
              " 'waverly',\n",
              " 'janessa',\n",
              " 'jayden',\n",
              " 'micah',\n",
              " 'novah',\n",
              " 'zora',\n",
              " 'ann',\n",
              " 'jana',\n",
              " 'taliyah',\n",
              " 'vada',\n",
              " 'giavanna',\n",
              " 'ingrid',\n",
              " 'valery',\n",
              " 'azaria',\n",
              " 'emmarie',\n",
              " 'esperanza',\n",
              " 'kailyn',\n",
              " 'aiyana',\n",
              " 'keilani',\n",
              " 'austyn',\n",
              " 'whitley',\n",
              " 'elina',\n",
              " 'kimora',\n",
              " 'maliah',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fa935c21",
      "metadata": {
        "id": "fa935c21"
      },
      "outputs": [],
      "source": [
        "# Dictionaries, {idx -> ch} and {ch -> idx}\n",
        "itos = defaultdict(int)\n",
        "stoi = defaultdict(int)\n",
        "# Number of characters used to predict the target character in the MLP Language Model\n",
        "block_size = 3\n",
        "# Batch size used in MLP Language Model\n",
        "batch_size = 32\n",
        "# Embedding dimension, per character\n",
        "d_model = 10\n",
        "# Hidden dimension for RNN and also MLP Language Models\n",
        "d_h = 200\n",
        "\n",
        "# START = START token\n",
        "stoi['.'] = 0\n",
        "itos[0] = '.'\n",
        "\n",
        "count = 1\n",
        "# Loop over all names and create mappings itos and stoi mapping a unique character to a idx\n",
        "for name in names:\n",
        "  for ch in name:\n",
        "    if not ch in stoi:\n",
        "      stoi[ch] = count\n",
        "      itos[count] = ch\n",
        "      count = count + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d556793e",
      "metadata": {
        "id": "d556793e"
      },
      "outputs": [],
      "source": [
        "assert len(stoi) == len(itos)\n",
        "vocab_size = len(stoi)\n",
        "assert vocab_size == 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3765e0fe",
      "metadata": {
        "id": "3765e0fe",
        "outputId": "5a9d421f-451e-46cc-97a9-751a41b2415f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'.': 0,\n",
              "             'e': 1,\n",
              "             'm': 2,\n",
              "             'a': 3,\n",
              "             'o': 4,\n",
              "             'l': 5,\n",
              "             'i': 6,\n",
              "             'v': 7,\n",
              "             's': 8,\n",
              "             'b': 9,\n",
              "             'p': 10,\n",
              "             'h': 11,\n",
              "             'c': 12,\n",
              "             'r': 13,\n",
              "             't': 14,\n",
              "             'y': 15,\n",
              "             'n': 16,\n",
              "             'g': 17,\n",
              "             'z': 18,\n",
              "             'f': 19,\n",
              "             'd': 20,\n",
              "             'u': 21,\n",
              "             'k': 22,\n",
              "             'w': 23,\n",
              "             'q': 24,\n",
              "             'x': 25,\n",
              "             'j': 26})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "stoi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d706797e",
      "metadata": {
        "id": "d706797e"
      },
      "source": [
        "## BiGram Language Model\n",
        "- Implement the Bigram Language Model\n",
        "- Get all the relevent counts, then get the train dataset Perplexity\n",
        "- Use the class notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "9cb554a5",
      "metadata": {
        "id": "9cb554a5"
      },
      "outputs": [],
      "source": [
        "# Using the formulas in class, loop over each name and get the parameters\n",
        "c1 = defaultdict(int)\n",
        "c2 = defaultdict(int)\n",
        "c2_sum = defaultdict(int)\n",
        "for name in open(names_path, 'r'):\n",
        "    # Lowercase and remove any whitespace at the end\n",
        "    name = name.lower().strip()\n",
        "    # Pad with START = '.' and STOP = '.'\n",
        "    name = '.'+name+'.'\n",
        "    # Transform to integer\n",
        "    name = [stoi[ch] for ch in name]\n",
        "    # Get the counts for Bigrams and Unigrams\n",
        "    for i in range(len(name)):\n",
        "      c1[name[i]] += 1\n",
        "\n",
        "      if i < len(name) - 1:\n",
        "        bigram = (name[i], name[i + 1])\n",
        "        c2[bigram] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "507e4525",
      "metadata": {
        "id": "507e4525",
        "outputId": "d6904092-4de8-44cf-9c2c-c6b265eabf3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preplexity:  14.83566951751709\n"
          ]
        }
      ],
      "source": [
        "# Get perplexity\n",
        "\n",
        "sumneglogp = 0\n",
        "T = 0\n",
        "for name in open(names_path, 'r'):\n",
        "    # Get rid of white space and lowercase\n",
        "    name = name.lower().strip()\n",
        "    # Get the length of the word, without padding\n",
        "    T += len(name)\n",
        "    # Don't pad the STOP since we are not generating; pad with START\n",
        "    name = '.' + name\n",
        "    # Transform to integrs\n",
        "    name = [stoi[ch] for ch in name]\n",
        "    # Get the loss -log(p(name)); use that the log of the product is the sum of the logs\n",
        "    for i in range(len(name) - 1):\n",
        "      x, y = name[i], name[i + 1]\n",
        "      p = c2[(x, y)] / c1[x]\n",
        "\n",
        "      sumneglogp += -torch.log2(torch.tensor(p))\n",
        "# Print the Perplexity\n",
        "print('Preplexity: ', torch.pow(2.0, sumneglogp / T).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b931b726",
      "metadata": {
        "id": "b931b726",
        "outputId": "8d47cbe7-4705-44fb-bfb9-adcd69453210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated name:  keltim\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# When you generate STOP, terminate\n",
        "name = '.'\n",
        "while True:\n",
        "    c = stoi[name[-1]]\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    p = []\n",
        "    for d in range(vocab_size):\n",
        "        # Use the same indicies as the dictionary we set up\n",
        "        # Populate p\n",
        "        p.append(c2[(c,d)]/c1[c])\n",
        "    #print(p)\n",
        "    assert len(p) == vocab_size\n",
        "    # Sample randmly from the probability using torch.Categorical\n",
        "    c = torch.distributions.Categorical(torch.tensor(p)).sample()\n",
        "    # Offset by 1 since we want indices [1, 2, ..., vocab_size]\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[1:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561cdacb",
      "metadata": {
        "id": "561cdacb"
      },
      "source": [
        "## MLP Language Model\n",
        "\n",
        "- Implement the MLP language model from below\n",
        "- Look at page 7, Equation (1)\n",
        "- https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "982566bf",
      "metadata": {
        "id": "982566bf"
      },
      "outputs": [],
      "source": [
        "# Number of characters used to predict the target character in the MLP Language Model\n",
        "block_size = 8\n",
        "# Batch size used in MLP Language Model\n",
        "batch_size = 64\n",
        "# Embedding dimension, per character\n",
        "d_model = 64\n",
        "# Hidden dimension for RNN and also MLP Language Models\n",
        "d_h = 512\n",
        "lr = 0.003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "fb6328a5",
      "metadata": {
        "id": "fb6328a5"
      },
      "outputs": [],
      "source": [
        "x_data = []\n",
        "y_data = []\n",
        "for name in open(names_path, 'r'):\n",
        "    name = name.lower().strip()\n",
        "    # Pad with block_size START tokens and 1 STOP token\n",
        "    name = ''.join(block_size * ['.']) + name + '.'\n",
        "    # Loop through name and get the (x, y) pairs\n",
        "    # Add (x, y) to x_data and y_data and make sure you transform to characters\n",
        "    # Make sure x_data and y_data have integers, use stoi\n",
        "    for i in range(len(name) - block_size):\n",
        "      x = name[i: i + block_size]\n",
        "      y = name[i + block_size]\n",
        "      x_data.append([stoi[ch] for ch in x])\n",
        "      y_data.append(stoi[y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "3142dd13",
      "metadata": {
        "id": "3142dd13"
      },
      "outputs": [],
      "source": [
        "class MLPLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # An embedding for each character; vocab_size of them\n",
        "        self.e = nn.Embedding(vocab_size, d_model)\n",
        "        # H; should take in block_size * d_model vector and output d_h\n",
        "        self.H = nn.Linear(block_size * d_model, d_h)\n",
        "        # U; should take in d_h vector and output vocab_size\n",
        "        self.U = nn.Linear(d_h, vocab_size)\n",
        "        # W; for the skip connection, should take in block_size * d_model and output vocab_size\n",
        "        self.W = nn.Linear(block_size * d_model, vocab_size, bias=False)\n",
        "\n",
        "    # x should be (batch_size, block_size)\n",
        "    def forward(self, x):\n",
        "        x = self.e(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        logits = self.U(torch.tanh(self.H(x))) + self.W(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "4a4f35f7",
      "metadata": {
        "id": "4a4f35f7",
        "outputId": "2dd77b1a-1ca9-4cd7-c2f1-c688f3e12d75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 0, 0, 0, 0, 0, 0, 0], 1)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "x_data[0], y_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "_qfL4H_Bvaxh",
        "outputId": "cec8bc7c-5e6a-4f5b-f168-9dad53641572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_qfL4H_Bvaxh",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "17bf7a0a",
      "metadata": {
        "id": "17bf7a0a"
      },
      "outputs": [],
      "source": [
        "# Define a dataloader with x_data and y_data with batch_size\n",
        "dl = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(torch.tensor(x_data), torch.tensor(y_data)),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "a56ce82f",
      "metadata": {
        "id": "a56ce82f"
      },
      "outputs": [],
      "source": [
        "for xb, yb in dl:\n",
        "    assert xb.shape == (batch_size, block_size)\n",
        "    assert yb.shape == (batch_size,)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTnVktytvRX4"
      },
      "id": "nTnVktytvRX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "1013903d",
      "metadata": {
        "id": "1013903d"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "# Define the MLP model and the Adam optimizer learning rate 0.001\n",
        "model = MLPLanguageModel().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr = lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "0155215c",
      "metadata": {
        "id": "0155215c",
        "outputId": "ca7ccf54-8a57-4f94-e228-75e0b63d0010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 loss: 2.5611164202690127\n",
            "1000 loss: 2.489165368437767\n",
            "1500 loss: 2.4499320187568663\n",
            "2000 loss: 2.4167115367650984\n",
            "2500 loss: 2.3940182425022125\n",
            "3000 loss: 2.375398099541664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [00:09<02:53,  9.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 2.3601437604767934\n",
            "500 loss: 2.2163252949714662\n",
            "1000 loss: 2.211215467095375\n",
            "1500 loss: 2.207372493902842\n",
            "2000 loss: 2.201061165630817\n",
            "2500 loss: 2.197271681165695\n",
            "3000 loss: 2.1934944974184036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [00:18<02:45,  9.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 2.188078442505428\n",
            "500 loss: 2.111725174188614\n",
            "1000 loss: 2.1161309732198714\n",
            "1500 loss: 2.1154384338855743\n",
            "2000 loss: 2.1164331203103064\n",
            "2500 loss: 2.118770332860947\n",
            "3000 loss: 2.118202536702156\n",
            "3500 loss: 2.118408908673695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [00:27<02:38,  9.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 loss: 2.0760478515625\n",
            "1000 loss: 2.0802063245773317\n",
            "1500 loss: 2.0843080811500547\n",
            "2000 loss: 2.083667406618595\n",
            "2500 loss: 2.084157512140274\n",
            "3000 loss: 2.084213160395622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [00:37<02:28,  9.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 2.084597380842481\n",
            "500 loss: 2.0416115779876707\n",
            "1000 loss: 2.0502407104969023\n",
            "1500 loss: 2.0555665842692057\n",
            "2000 loss: 2.056458196103573\n",
            "2500 loss: 2.0590847339630125\n",
            "3000 loss: 2.0582721779346467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [00:46<02:18,  9.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 2.0586400611060007\n",
            "500 loss: 2.0329619674682617\n",
            "1000 loss: 2.037450213551521\n",
            "1500 loss: 2.038813616673152\n",
            "2000 loss: 2.039634259700775\n",
            "2500 loss: 2.0433903454780578\n",
            "3000 loss: 2.0436253490845364\n",
            "3500 loss: 2.0426417781284876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [00:55<02:08,  9.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 loss: 2.012514153718948\n",
            "1000 loss: 2.016347921848297\n",
            "1500 loss: 2.0211620190938313\n",
            "2000 loss: 2.02196610224247\n",
            "2500 loss: 2.025038762140274\n",
            "3000 loss: 2.0268607984781264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [01:03<01:56,  8.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 2.029783913680485\n",
            "500 loss: 1.9934889681339263\n",
            "1000 loss: 1.9989560486078262\n",
            "1500 loss: 2.007384384950002\n",
            "2000 loss: 2.01281718736887\n",
            "2500 loss: 2.015912192726135\n",
            "3000 loss: 2.0184144772291184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [01:16<02:01, 10.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 2.0226218744005475\n",
            "500 loss: 1.982764821767807\n",
            "1000 loss: 2.00087779712677\n",
            "1500 loss: 2.0044391496976215\n",
            "2000 loss: 2.0106909151077272\n",
            "2500 loss: 2.0097941460609436\n",
            "3000 loss: 2.0109155536492667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [01:27<01:56, 10.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 2.0116475039890833\n",
            "500 loss: 1.984785900115967\n",
            "1000 loss: 1.9879874362945558\n",
            "1500 loss: 1.9898955252965291\n",
            "2000 loss: 1.9945658376216888\n",
            "2500 loss: 1.999732579946518\n",
            "3000 loss: 2.0033679784933724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [01:37<01:41, 10.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 2.004262808050428\n",
            "500 loss: 1.9725176441669463\n",
            "1000 loss: 1.9853141927719116\n",
            "1500 loss: 1.9862928802967073\n",
            "2000 loss: 1.9890793601870538\n",
            "2500 loss: 1.9948865338802337\n",
            "3000 loss: 1.9967194394667944\n",
            "3500 loss: 1.9997211315972465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [01:45<01:27,  9.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 loss: 1.9628675954341888\n",
            "1000 loss: 1.973725993514061\n",
            "1500 loss: 1.982476757367452\n",
            "2000 loss: 1.9852666084766388\n",
            "2500 loss: 1.9884591714859008\n",
            "3000 loss: 1.992039138952891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [01:54<01:15,  9.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 1.995900652578899\n",
            "500 loss: 1.962950962305069\n",
            "1000 loss: 1.971451884508133\n",
            "1500 loss: 1.9770104428132376\n",
            "2000 loss: 1.9802120569348336\n",
            "2500 loss: 1.9844180331707\n",
            "3000 loss: 1.9858562526305517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [02:03<01:05,  9.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 1.990312652008874\n",
            "500 loss: 1.9573540699481964\n",
            "1000 loss: 1.9679741817712784\n",
            "1500 loss: 1.9761723442077637\n",
            "2000 loss: 1.9806292098760605\n",
            "2500 loss: 1.9835231051445008\n",
            "3000 loss: 1.987294478138288\n",
            "3500 loss: 1.9896175619534084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [02:12<00:55,  9.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 loss: 1.9597421765327454\n",
            "1000 loss: 1.9663698095083237\n",
            "1500 loss: 1.9684392546017964\n",
            "2000 loss: 1.9733223776221276\n",
            "2500 loss: 1.9749959829330443\n",
            "3000 loss: 1.9787156161864599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [02:21<00:45,  9.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 1.9833571960926055\n",
            "500 loss: 1.9470286870002746\n",
            "1000 loss: 1.952020588517189\n",
            "1500 loss: 1.9649359625975291\n",
            "2000 loss: 1.9718404035568238\n",
            "2500 loss: 1.9778567059993744\n",
            "3000 loss: 1.981751800775528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [02:30<00:36,  9.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 1.9848427129813604\n",
            "500 loss: 1.9537422556877135\n",
            "1000 loss: 1.9606625082492828\n",
            "1500 loss: 1.9642695383230846\n",
            "2000 loss: 1.9712240637540817\n",
            "2500 loss: 1.975498152732849\n",
            "3000 loss: 1.9777770899534226\n",
            "3500 loss: 1.9803833802086965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [02:39<00:27,  9.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 loss: 1.9437099406719207\n",
            "1000 loss: 1.9535646675825118\n",
            "1500 loss: 1.9585260221163432\n",
            "2000 loss: 1.9645810103416443\n",
            "2500 loss: 1.9689536073684692\n",
            "3000 loss: 1.9741898080507914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [02:48<00:17,  8.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 1.9770412889208113\n",
            "500 loss: 1.9449399709701538\n",
            "1000 loss: 1.9599355936050415\n",
            "1500 loss: 1.9620868391990662\n",
            "2000 loss: 1.9657794203758239\n",
            "2500 loss: 1.9697951674461365\n",
            "3000 loss: 1.973549701889356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 19/20 [02:58<00:09,  9.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 1.9765898958274297\n",
            "500 loss: 1.9435881385803222\n",
            "1000 loss: 1.9522997392416\n",
            "1500 loss: 1.9539048233826954\n",
            "2000 loss: 1.9627580479383469\n",
            "2500 loss: 1.967536694097519\n",
            "3000 loss: 1.9687123293876647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [03:08<00:00,  9.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500 loss: 1.9720457191467284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "total_loss = 0\n",
        "total_ct = 0\n",
        "total_epochs = 20\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "for _ in tqdm.tqdm(range(total_epochs)):\n",
        "    total_loss = 0\n",
        "    total_ct = 0\n",
        "    for xb, yb in dl:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the logits\n",
        "        logits = model(xb)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(logits, yb)\n",
        "\n",
        "        # Get the new gradient\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to max norm 0.1\n",
        "        # Use clid_grad_norm from torch\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "\n",
        "        # Do a gradient update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the loss for the batch and get the number of batches\n",
        "        total_loss += loss.item()\n",
        "        total_ct += 1\n",
        "\n",
        "        # Print the loss\n",
        "        if total_ct % 500 == 0:\n",
        "            print(f\"{total_ct} loss: {total_loss / total_ct}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "4ecf5fb3",
      "metadata": {
        "id": "4ecf5fb3",
        "outputId": "091bd7a2-2815-44f7-edfc-5e48d57a6723",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preplexity:  8.160454750061035\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    # Get perplexity\n",
        "    sumneglogp = 0\n",
        "    T = 0\n",
        "    for name in open(names_path, 'r'):\n",
        "        name = name.lower().strip()\n",
        "        T += len(name)\n",
        "        # Pad with block_size START tokens\n",
        "        name = ''.join(['.'] * block_size) + name\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        # Gather all the terms over the loss\n",
        "        # Notice that we compute -log p(...abc)\n",
        "        # Which is -log p(a | ...) - log p(b | a..) - log p(c | ba.)\n",
        "        for i in range(len(name) - block_size):\n",
        "            x = name[i:i+block_size]\n",
        "            y = name[i+block_size]\n",
        "            x_data.append([stoi[ch] for ch in x])\n",
        "            y_data.append(stoi[y])\n",
        "        # Gather the loss over the name, for each term\n",
        "        # You need to get the softmax loss for each term\n",
        "        # Can either use the CrossEntropyLoss or do this manually\n",
        "        # Compute the loss\n",
        "        xb = torch.tensor(x_data).to(device)\n",
        "        yb = torch.tensor(y_data).to(device)\n",
        "        logits = model(xb)\n",
        "\n",
        "        # Use reduction \"sum\" so you don't need to worry about N\n",
        "        loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")(logits, yb)\n",
        "\n",
        "        # Change to log base 2\n",
        "        loss /= torch.log(torch.tensor(2.0))\n",
        "\n",
        "        sumneglogp += loss\n",
        "\n",
        "    print('Preplexity: ', torch.pow(2.0, sumneglogp.clone().detach() / T).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "e77131c8",
      "metadata": {
        "id": "e77131c8",
        "outputId": "e4d891f1-41f1-4316-aa7d-9027b41f6f19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated name:  harney\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# When you generate STOP, terminate\n",
        "name = ''.join(block_size * ['.'])\n",
        "while True:\n",
        "    # Get the idx\n",
        "    c = torch.tensor([stoi[ch] for ch in name[-block_size:]]).unsqueeze(0).to(device)\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    logits = model(c)\n",
        "    p = torch.softmax(logits, dim=1)\n",
        "    # Randomly sample from p a new character\n",
        "    c = torch.distributions.Categorical(p).sample()\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[block_size:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1b01ed",
      "metadata": {
        "id": "bb1b01ed"
      },
      "source": [
        "## RNN Language Model\n",
        "- For each name, run an RNN character by character\n",
        "- Use the recursion x = Tanh()(Wh @ h + Wx @ x + bh + bx) and y = Softmax()(Wy h + by)\n",
        "- Do not use the RNN Cell from PyTorch, do this manually as hinted below"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of characters used to predict the target character in the MLP Language Model\n",
        "block_size = 8\n",
        "# Batch size used in MLP Language Model\n",
        "batch_size = 64\n",
        "# Embedding dimension, per character\n",
        "d_model = 64\n",
        "# Hidden dimension for RNN and also MLP Language Models\n",
        "d_h = 512\n",
        "lr = 1e-4"
      ],
      "metadata": {
        "id": "HNgff0MtH506"
      },
      "id": "HNgff0MtH506",
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "c765e7fc",
      "metadata": {
        "id": "c765e7fc"
      },
      "outputs": [],
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Each token has an embedding of size vocab_size\n",
        "        self.e = nn.Embedding(vocab_size, d_model)\n",
        "        # Wh used to map hidden to hidden\n",
        "        self.Wh = nn.Linear(d_h, d_h)\n",
        "        self.Wx = nn.Linear(d_model, d_h)\n",
        "        self.Wy = nn.Linear(d_h, vocab_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Run through to get the embedding for the token\n",
        "        # The embedding per token is the feature vector x  we pass into the\n",
        "        # Represent x as an embedding\n",
        "        x = self.e(x)\n",
        "        # Get the hidden state\n",
        "        h = torch.tanh(self.Wx(x) + self.Wh(h))\n",
        "        # Get the logits we use to predict y\n",
        "        y = self.Wy(h)\n",
        "        # Return the z predicting y for the timestep we are at and the next hidden state\n",
        "        return y, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "cd798358",
      "metadata": {
        "id": "cd798358"
      },
      "outputs": [],
      "source": [
        "model = RNNLanguageModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "7f3a49ff",
      "metadata": {
        "id": "7f3a49ff",
        "outputId": "771f500f-8d2d-45de-dadd-31dd8dbaa9f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loss: 3.024248788356781\n",
            "200 loss: 2.7756699472665787\n",
            "300 loss: 2.6327125302950543\n",
            "400 loss: 2.5524486884474755\n",
            "500 loss: 2.495197642326355\n",
            "600 loss: 2.460022052923838\n",
            "700 loss: 2.412215451342719\n",
            "800 loss: 2.376108872294426\n",
            "900 loss: 2.3537119347519346\n",
            "1000 loss: 2.3414170775413514\n",
            "1100 loss: 2.32774516582489\n",
            "1200 loss: 2.3134298036495844\n",
            "1300 loss: 2.301156713504058\n",
            "1400 loss: 2.294263433473451\n",
            "1500 loss: 2.283769733985265\n",
            "1600 loss: 2.273359841555357\n",
            "1700 loss: 2.267136492027956\n",
            "1800 loss: 2.260103164845043\n",
            "1900 loss: 2.2556384072805704\n",
            "2000 loss: 2.247425367832184\n",
            "2100 loss: 2.245547396909623\n",
            "2200 loss: 2.241198167042299\n",
            "2300 loss: 2.2366849151901578\n",
            "2400 loss: 2.236392797281345\n",
            "2500 loss: 2.2315450875759124\n",
            "2600 loss: 2.22719523865443\n",
            "2700 loss: 2.223278357408665\n",
            "2800 loss: 2.218814762873309\n",
            "2900 loss: 2.215516412011508\n",
            "3000 loss: 2.2101728307008743\n",
            "3100 loss: 2.2071302610443486\n",
            "3200 loss: 2.205136521793902\n",
            "3300 loss: 2.201382686875083\n",
            "3400 loss: 2.197571717711056\n",
            "3500 loss: 2.195320548091616\n",
            "3600 loss: 2.192945956620905\n",
            "3700 loss: 2.192214561346415\n",
            "3800 loss: 2.1915405868229114\n",
            "3900 loss: 2.1897296560727635\n",
            "4000 loss: 2.1877028743326665\n",
            "4100 loss: 2.1859314351256303\n",
            "4200 loss: 2.185963231268383\n",
            "4300 loss: 2.1846353673380476\n",
            "4400 loss: 2.1838393849134445\n",
            "4500 loss: 2.1822170731491513\n",
            "4600 loss: 2.1811187793379245\n",
            "4700 loss: 2.1799663733675123\n",
            "4800 loss: 2.1778469356397787\n",
            "4900 loss: 2.176419259382754\n",
            "5000 loss: 2.175214663052559\n",
            "5100 loss: 2.174164702120949\n",
            "5200 loss: 2.173769920582955\n",
            "5300 loss: 2.1726852158780368\n",
            "5400 loss: 2.1724424588459508\n",
            "5500 loss: 2.172433185685765\n",
            "5600 loss: 2.171809862319912\n",
            "5700 loss: 2.170079504899811\n",
            "5800 loss: 2.1697501098904115\n",
            "5900 loss: 2.1679099942668008\n",
            "6000 loss: 2.1680161362290384\n",
            "6100 loss: 2.1677160874937402\n",
            "6200 loss: 2.1678378351465346\n",
            "6300 loss: 2.1657358851319266\n",
            "6400 loss: 2.164346665292978\n",
            "6500 loss: 2.1648710181529704\n",
            "6600 loss: 2.163969234210072\n",
            "6700 loss: 2.163587780319043\n",
            "6800 loss: 2.162622451501734\n",
            "6900 loss: 2.161754265781762\n",
            "7000 loss: 2.1612007980857575\n",
            "7100 loss: 2.1606049939276466\n",
            "7200 loss: 2.160232063515319\n",
            "7300 loss: 2.159948322528029\n",
            "7400 loss: 2.1584994003579423\n",
            "7500 loss: 2.1569194396177926\n",
            "7600 loss: 2.1549670531090936\n",
            "7700 loss: 2.1556029173306057\n",
            "7800 loss: 2.155633818782293\n",
            "7900 loss: 2.1555029499983487\n",
            "8000 loss: 2.1545030194222927\n",
            "8100 loss: 2.1545864542030992\n",
            "8200 loss: 2.153499660143038\n",
            "8300 loss: 2.1534384387803365\n",
            "8400 loss: 2.151837848481678\n",
            "8500 loss: 2.151589418228935\n",
            "8600 loss: 2.15160850315593\n",
            "8700 loss: 2.1509951442822643\n",
            "8800 loss: 2.151684079996564\n",
            "8900 loss: 2.1504081233967556\n",
            "9000 loss: 2.150058612399631\n",
            "9100 loss: 2.1506594274201234\n",
            "9200 loss: 2.1495097025840177\n",
            "9300 loss: 2.148340925721712\n",
            "9400 loss: 2.1478519486873706\n",
            "9500 loss: 2.1470116615169927\n",
            "9600 loss: 2.1481565875187516\n",
            "9700 loss: 2.1485155840509944\n",
            "9800 loss: 2.148159833939708\n",
            "9900 loss: 2.1467011946259125\n",
            "10000 loss: 2.146597912287712\n",
            "10100 loss: 2.146520915479943\n",
            "10200 loss: 2.1458671304408243\n",
            "10300 loss: 2.146591502266023\n",
            "10400 loss: 2.1472497182511368\n",
            "10500 loss: 2.148695262591044\n",
            "10600 loss: 2.147211098974606\n",
            "10700 loss: 2.1465153365157477\n",
            "10800 loss: 2.147529292768902\n",
            "10900 loss: 2.1465163358089026\n",
            "11000 loss: 2.1455699765790595\n",
            "11100 loss: 2.1452297172567865\n",
            "11200 loss: 2.1460399883453336\n",
            "11300 loss: 2.146149842612511\n",
            "11400 loss: 2.147172859938521\n",
            "11500 loss: 2.1455299178621043\n",
            "11600 loss: 2.1448443957135597\n",
            "11700 loss: 2.1454463967502626\n",
            "11800 loss: 2.145993696612827\n",
            "11900 loss: 2.1435518989843483\n",
            "12000 loss: 2.143491558790207\n",
            "12100 loss: 2.143183924127216\n",
            "12200 loss: 2.1451229909892944\n",
            "12300 loss: 2.145294095355321\n",
            "12400 loss: 2.1464741292884275\n",
            "12500 loss: 2.144925957403183\n",
            "12600 loss: 2.143451235445719\n",
            "12700 loss: 2.143089717851849\n",
            "12800 loss: 2.1428420382365583\n",
            "12900 loss: 2.143921275406845\n",
            "13000 loss: 2.1421173134308593\n",
            "13100 loss: 2.139649391620214\n",
            "13200 loss: 2.1402831959453494\n",
            "13300 loss: 2.139474701746962\n",
            "13400 loss: 2.1406247819181696\n",
            "13500 loss: 2.139797719169546\n",
            "13600 loss: 2.1402426615269743\n",
            "13700 loss: 2.1414749517231964\n",
            "13800 loss: 2.14177385378575\n",
            "13900 loss: 2.1398411825921038\n",
            "14000 loss: 2.1393216510500226\n",
            "14100 loss: 2.138051629074922\n",
            "14200 loss: 2.1371278196825108\n",
            "14300 loss: 2.1376610916977996\n",
            "14400 loss: 2.1396093211157456\n",
            "14500 loss: 2.1389976793239858\n",
            "14600 loss: 2.1370784688281685\n",
            "14700 loss: 2.134794579634861\n",
            "14800 loss: 2.1348941220262567\n",
            "14900 loss: 2.1348366548511004\n",
            "15000 loss: 2.1337122146566707\n",
            "15100 loss: 2.133912793804478\n",
            "15200 loss: 2.1347715460979626\n",
            "15300 loss: 2.1344855542627035\n",
            "15400 loss: 2.134196289680995\n",
            "15500 loss: 2.1346149424545224\n",
            "15600 loss: 2.1341419196931217\n",
            "15700 loss: 2.1340006705644026\n",
            "15800 loss: 2.131932366448867\n",
            "15900 loss: 2.1305503174231486\n",
            "16000 loss: 2.130135473009199\n",
            "16100 loss: 2.12885590419636\n",
            "16200 loss: 2.1275662488223595\n",
            "16300 loss: 2.127435129804114\n",
            "16400 loss: 2.127917917309011\n",
            "16500 loss: 2.1287215075167745\n",
            "16600 loss: 2.127856238983482\n",
            "16700 loss: 2.1257065215617597\n",
            "16800 loss: 2.123912880814501\n",
            "16900 loss: 2.121742668240028\n",
            "17000 loss: 2.122088202381835\n",
            "17100 loss: 2.1218152987991856\n",
            "17200 loss: 2.119293371840965\n",
            "17300 loss: 2.1187113634974972\n",
            "17400 loss: 2.119430232335781\n",
            "17500 loss: 2.1206429679461887\n",
            "17600 loss: 2.120601412078196\n",
            "17700 loss: 2.1198271470352754\n",
            "17800 loss: 2.1199763093168817\n",
            "17900 loss: 2.1204271911908794\n",
            "18000 loss: 2.120622814224826\n",
            "18100 loss: 2.1214532378958073\n",
            "18200 loss: 2.1221700782369783\n",
            "18300 loss: 2.122517619041797\n",
            "18400 loss: 2.1231106343152732\n",
            "18500 loss: 2.123789648951711\n",
            "18600 loss: 2.124079670989385\n",
            "18700 loss: 2.1246904378587548\n",
            "18800 loss: 2.1251231905120482\n",
            "18900 loss: 2.12548063556984\n",
            "19000 loss: 2.1259642756612678\n",
            "19100 loss: 2.126689341492678\n",
            "19200 loss: 2.127489317059517\n",
            "19300 loss: 2.1276578353847246\n",
            "19400 loss: 2.1278109124456486\n",
            "19500 loss: 2.127997210533191\n",
            "19600 loss: 2.1281646157223353\n",
            "19700 loss: 2.128338526886732\n",
            "19800 loss: 2.1286707331496055\n",
            "19900 loss: 2.1290987384499016\n",
            "20000 loss: 2.1297038637518884\n",
            "20100 loss: 2.1303442810957707\n",
            "20200 loss: 2.130880727213208\n",
            "20300 loss: 2.131229024373839\n",
            "20400 loss: 2.13198429197073\n",
            "20500 loss: 2.1323478756997645\n",
            "20600 loss: 2.132650228565179\n",
            "20700 loss: 2.1330206636127067\n",
            "20800 loss: 2.1331324167320362\n",
            "20900 loss: 2.133383246260967\n",
            "21000 loss: 2.1331513498680934\n",
            "21100 loss: 2.1335106033905986\n",
            "21200 loss: 2.1338955631795917\n",
            "21300 loss: 2.134147430579987\n",
            "21400 loss: 2.134616083547334\n",
            "21500 loss: 2.1348041763305665\n",
            "21600 loss: 2.1352646245448677\n",
            "21700 loss: 2.1355961337726788\n",
            "21800 loss: 2.1359285966081356\n",
            "21900 loss: 2.136555532690597\n",
            "22000 loss: 2.1367637522166425\n",
            "22100 loss: 2.1376596952887144\n",
            "22200 loss: 2.1377336498692228\n",
            "22300 loss: 2.137854998619567\n",
            "22400 loss: 2.1384857556809274\n",
            "22500 loss: 2.1388745817608306\n",
            "22600 loss: 2.1390823522816715\n",
            "22700 loss: 2.139407032344835\n",
            "22800 loss: 2.1403185884053246\n",
            "22900 loss: 2.140964610878557\n",
            "23000 loss: 2.140859757138335\n",
            "23100 loss: 2.1415532007826354\n",
            "23200 loss: 2.1421181481922495\n",
            "23300 loss: 2.1426242522057546\n",
            "23400 loss: 2.1427677641567002\n",
            "23500 loss: 2.143330444477974\n",
            "23600 loss: 2.1437798921839666\n",
            "23700 loss: 2.1441422418703007\n",
            "23800 loss: 2.144703927500909\n",
            "23900 loss: 2.145223014324779\n",
            "24000 loss: 2.1456939941346644\n",
            "24100 loss: 2.1459644362194408\n",
            "24200 loss: 2.1462660493781742\n",
            "24300 loss: 2.1468393383840474\n",
            "24400 loss: 2.1472733762518303\n",
            "24500 loss: 2.147835380958051\n",
            "24600 loss: 2.1484925158285515\n",
            "24700 loss: 2.149209995815146\n",
            "24800 loss: 2.1492488564022127\n",
            "24900 loss: 2.14960863268519\n",
            "25000 loss: 2.150205800766945\n",
            "25100 loss: 2.1507303760678647\n",
            "25200 loss: 2.150661786960231\n",
            "25300 loss: 2.150960249627532\n",
            "25400 loss: 2.1503328989856825\n",
            "25500 loss: 2.1512511864035737\n",
            "25600 loss: 2.152145291948691\n",
            "25700 loss: 2.1525595755335885\n",
            "25800 loss: 2.153195330412813\n",
            "25900 loss: 2.152906279701984\n",
            "26000 loss: 2.15375092960321\n",
            "26100 loss: 2.154604411481441\n",
            "26200 loss: 2.155277498796696\n",
            "26300 loss: 2.1550654153316193\n",
            "26400 loss: 2.155795626346812\n",
            "26500 loss: 2.1553849875342173\n",
            "26600 loss: 2.1558403016301924\n",
            "26700 loss: 2.15623581751902\n",
            "26800 loss: 2.157121306608862\n",
            "26900 loss: 2.1577302867329253\n",
            "27000 loss: 2.1577187153189272\n",
            "27100 loss: 2.15807274035862\n",
            "27200 loss: 2.1575946198272353\n",
            "27300 loss: 2.157519832578771\n",
            "27400 loss: 2.1580961363942084\n",
            "27500 loss: 2.15867640432011\n",
            "27600 loss: 2.159206871541514\n",
            "27700 loss: 2.1599442694247415\n",
            "27800 loss: 2.16014334359615\n",
            "27900 loss: 2.1599452845896443\n",
            "28000 loss: 2.1606128552470887\n",
            "28100 loss: 2.1610885040318837\n",
            "28200 loss: 2.1610560365644753\n",
            "28300 loss: 2.1613311721663595\n",
            "28400 loss: 2.161737047530396\n",
            "28500 loss: 2.1626111159784753\n",
            "28600 loss: 2.162872439677065\n",
            "28700 loss: 2.1632166499709418\n",
            "28800 loss: 2.16376155315174\n",
            "28900 loss: 2.1637935176192684\n",
            "29000 loss: 2.1638164266224567\n",
            "29100 loss: 2.1635979816389246\n",
            "29200 loss: 2.1643231564066183\n",
            "29300 loss: 2.1650492378265787\n",
            "29400 loss: 2.164160115483667\n",
            "29500 loss: 2.1635586228168617\n",
            "29600 loss: 2.164148753970056\n",
            "29700 loss: 2.1642072895119084\n",
            "29800 loss: 2.165194765265356\n",
            "29900 loss: 2.165617201180761\n",
            "30000 loss: 2.165885444867611\n",
            "30100 loss: 2.166161813288432\n",
            "30200 loss: 2.1671432038371927\n",
            "30300 loss: 2.1668363105385215\n",
            "30400 loss: 2.1671294415467663\n",
            "30500 loss: 2.1669387632706125\n",
            "30600 loss: 2.166415499635771\n",
            "30700 loss: 2.1668584853430137\n",
            "30800 loss: 2.167591077112532\n",
            "30900 loss: 2.168400343494508\n",
            "31000 loss: 2.1675292136053885\n",
            "31100 loss: 2.1672307956755352\n",
            "31200 loss: 2.1663156422284935\n",
            "31300 loss: 2.1667398749601348\n",
            "31400 loss: 2.16703990501583\n",
            "31500 loss: 2.167558597727427\n",
            "31600 loss: 2.168462764620781\n",
            "31700 loss: 2.1686032419099415\n",
            "31800 loss: 2.1688235670253166\n",
            "31900 loss: 2.168686671955832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [03:49<15:19, 229.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000 loss: 2.1692749907784163\n",
            "100 loss: 2.316215195655823\n",
            "200 loss: 2.175766730904579\n",
            "300 loss: 2.1221226783593496\n",
            "400 loss: 2.0902100989222525\n",
            "500 loss: 2.069677738428116\n",
            "600 loss: 2.0548035273949306\n",
            "700 loss: 2.03210668189185\n",
            "800 loss: 2.017814715504646\n",
            "900 loss: 2.006661128732893\n",
            "1000 loss: 2.003008593082428\n",
            "1100 loss: 1.9993785857070576\n",
            "1200 loss: 1.993178129196167\n",
            "1300 loss: 1.989428836107254\n",
            "1400 loss: 1.9914348483937128\n",
            "1500 loss: 1.986277157306671\n",
            "1600 loss: 1.98311891913414\n",
            "1700 loss: 1.9823441082589766\n",
            "1800 loss: 1.981558616426256\n",
            "1900 loss: 1.9815525070617073\n",
            "2000 loss: 1.9785224974155426\n",
            "2100 loss: 1.9797314051219395\n",
            "2200 loss: 1.9794742646542463\n",
            "2300 loss: 1.9791785793719083\n",
            "2400 loss: 1.982382825712363\n",
            "2500 loss: 1.9802202798366546\n",
            "2600 loss: 1.979103046151308\n",
            "2700 loss: 1.9792557899157206\n",
            "2800 loss: 1.9779978179080147\n",
            "2900 loss: 1.9776421807963274\n",
            "3000 loss: 1.9743256594340006\n",
            "3100 loss: 1.9748936104774475\n",
            "3200 loss: 1.9752689960226417\n",
            "3300 loss: 1.9741230771035858\n",
            "3400 loss: 1.9720908580457464\n",
            "3500 loss: 1.9722346366814205\n",
            "3600 loss: 1.9727935275104311\n",
            "3700 loss: 1.973819423978393\n",
            "3800 loss: 1.9749312687233875\n",
            "3900 loss: 1.9751650797709441\n",
            "4000 loss: 1.9746545621454716\n",
            "4100 loss: 1.9742131132323568\n",
            "4200 loss: 1.9752131200688225\n",
            "4300 loss: 1.9756796949131543\n",
            "4400 loss: 1.976952616274357\n",
            "4500 loss: 1.9767549103366004\n",
            "4600 loss: 1.9772678765006688\n",
            "4700 loss: 1.9784110216130602\n",
            "4800 loss: 1.9777862718204657\n",
            "4900 loss: 1.9780492715689602\n",
            "5000 loss: 1.9783757271766662\n",
            "5100 loss: 1.9788699928919475\n",
            "5200 loss: 1.9799308435733503\n",
            "5300 loss: 1.9805709127884992\n",
            "5400 loss: 1.9815239741184094\n",
            "5500 loss: 1.9828882540572774\n",
            "5600 loss: 1.9835051443108491\n",
            "5700 loss: 1.983402182093838\n",
            "5800 loss: 1.9838128270774051\n",
            "5900 loss: 1.983101559008582\n",
            "6000 loss: 1.985039800008138\n",
            "6100 loss: 1.9858082265736627\n",
            "6200 loss: 1.9867043989896773\n",
            "6300 loss: 1.98585107572495\n",
            "6400 loss: 1.985276097189635\n",
            "6500 loss: 1.986683736874507\n",
            "6600 loss: 1.9865013299205085\n",
            "6700 loss: 1.9868860971927642\n",
            "6800 loss: 1.9874598019088017\n",
            "6900 loss: 1.987566001985384\n",
            "7000 loss: 1.9879795990160534\n",
            "7100 loss: 1.9883279245672092\n",
            "7200 loss: 1.9886697970992988\n",
            "7300 loss: 1.9892174072298285\n",
            "7400 loss: 1.9890220625175012\n",
            "7500 loss: 1.9882449062029521\n",
            "7600 loss: 1.9874299819375338\n",
            "7700 loss: 1.9885878725485369\n",
            "7800 loss: 1.989665670364331\n",
            "7900 loss: 1.9906016079232662\n",
            "8000 loss: 1.9904409546703101\n",
            "8100 loss: 1.9912246385032748\n",
            "8200 loss: 1.9909424100707216\n",
            "8300 loss: 1.9912508527629347\n",
            "8400 loss: 1.9903739739032018\n",
            "8500 loss: 1.9911697475489447\n",
            "8600 loss: 1.9921452848024146\n",
            "8700 loss: 1.9923138825646762\n",
            "8800 loss: 1.9933849979124285\n",
            "8900 loss: 1.9929012658488885\n",
            "9000 loss: 1.9933126035796271\n",
            "9100 loss: 1.9945554947722088\n",
            "9200 loss: 1.9945120926914008\n",
            "9300 loss: 1.994027996229869\n",
            "9400 loss: 1.994353881452946\n",
            "9500 loss: 1.9939745658573351\n",
            "9600 loss: 1.9959905942777794\n",
            "9700 loss: 1.9969059574235346\n",
            "9800 loss: 1.9975106797291309\n",
            "9900 loss: 1.9965676606423928\n",
            "10000 loss: 1.996953854417801\n",
            "10100 loss: 1.9973805349061984\n",
            "10200 loss: 1.9975861802171258\n",
            "10300 loss: 1.9989752379898886\n",
            "10400 loss: 2.000205924786054\n",
            "10500 loss: 2.0019413394814447\n",
            "10600 loss: 2.0015459136018214\n",
            "10700 loss: 2.0013533463210704\n",
            "10800 loss: 2.0030470214728955\n",
            "10900 loss: 2.0024452834063715\n",
            "11000 loss: 2.002195716424422\n",
            "11100 loss: 2.0024427796376716\n",
            "11200 loss: 2.0036458424691643\n",
            "11300 loss: 2.004358242370386\n",
            "11400 loss: 2.005881804426511\n",
            "11500 loss: 2.0051696323000865\n",
            "11600 loss: 2.0048920844443914\n",
            "11700 loss: 2.0059581820271974\n",
            "11800 loss: 2.006988949573646\n",
            "11900 loss: 2.005010684688552\n",
            "12000 loss: 2.005618564973275\n",
            "12100 loss: 2.006147556580788\n",
            "12200 loss: 2.008532913854865\n",
            "12300 loss: 2.009158791390861\n",
            "12400 loss: 2.0106486707445113\n",
            "12500 loss: 2.0101633813858033\n",
            "12600 loss: 2.0093674262175485\n",
            "12700 loss: 2.009584296350404\n",
            "12800 loss: 2.009760952675715\n",
            "12900 loss: 2.0114272179252417\n",
            "13000 loss: 2.0100735005140304\n",
            "13100 loss: 2.0080727924554402\n",
            "13200 loss: 2.0091040490341907\n",
            "13300 loss: 2.0087311238901955\n",
            "13400 loss: 2.0103557970363704\n",
            "13500 loss: 2.010205423761297\n",
            "13600 loss: 2.01116107054493\n",
            "13700 loss: 2.0126704126988013\n",
            "13800 loss: 2.013968866441561\n",
            "13900 loss: 2.012850395852713\n",
            "14000 loss: 2.0126641936387335\n",
            "14100 loss: 2.011956310382126\n",
            "14200 loss: 2.0116016697295955\n",
            "14300 loss: 2.0126919267977867\n",
            "14400 loss: 2.0151138655924137\n",
            "14500 loss: 2.0150821277273114\n",
            "14600 loss: 2.013563654381935\n",
            "14700 loss: 2.0117822077404073\n",
            "14800 loss: 2.0120744127679515\n",
            "14900 loss: 2.0123793035785624\n",
            "15000 loss: 2.011698223988215\n",
            "15100 loss: 2.0125205395790124\n",
            "15200 loss: 2.0138183374232366\n",
            "15300 loss: 2.014145062445036\n",
            "15400 loss: 2.014227110085549\n",
            "15500 loss: 2.0149506202974625\n",
            "15600 loss: 2.0147099576546594\n",
            "15700 loss: 2.0151003802809746\n",
            "15800 loss: 2.013708863175368\n",
            "15900 loss: 2.012773305814971\n",
            "16000 loss: 2.012652484193444\n",
            "16100 loss: 2.0119676247590816\n",
            "16200 loss: 2.0111658972722513\n",
            "16300 loss: 2.0113532171366404\n",
            "16400 loss: 2.012205517655466\n",
            "16500 loss: 2.013501779303406\n",
            "16600 loss: 2.0130157526070813\n",
            "16700 loss: 2.0112682871047607\n",
            "16800 loss: 2.009922938375246\n",
            "16900 loss: 2.00813699607313\n",
            "17000 loss: 2.009108692589928\n",
            "17100 loss: 2.0091388780889456\n",
            "17200 loss: 2.007152599803237\n",
            "17300 loss: 2.006884442809\n",
            "17400 loss: 2.008186726035743\n",
            "17500 loss: 2.0095608620711736\n",
            "17600 loss: 2.0098421242562208\n",
            "17700 loss: 2.009445468335502\n",
            "17800 loss: 2.010006298529968\n",
            "17900 loss: 2.0106569351630506\n",
            "18000 loss: 2.0110852269265385\n",
            "18100 loss: 2.0119530127522696\n",
            "18200 loss: 2.01255590406748\n",
            "18300 loss: 2.0129242598554478\n",
            "18400 loss: 2.01349465691525\n",
            "18500 loss: 2.0141779070673764\n",
            "18600 loss: 2.014496865650659\n",
            "18700 loss: 2.015149473984611\n",
            "18800 loss: 2.015698349900702\n",
            "18900 loss: 2.0162300121973433\n",
            "19000 loss: 2.016720397114754\n",
            "19100 loss: 2.0174806729536408\n",
            "19200 loss: 2.018402172562977\n",
            "19300 loss: 2.0186784556986757\n",
            "19400 loss: 2.0189943886724944\n",
            "19500 loss: 2.0192073039213816\n",
            "19600 loss: 2.0194171470160387\n",
            "19700 loss: 2.0196469221320856\n",
            "19800 loss: 2.0201323659251433\n",
            "19900 loss: 2.0206426521941045\n",
            "20000 loss: 2.0212198687016962\n",
            "20100 loss: 2.0218675358912246\n",
            "20200 loss: 2.0224622982917446\n",
            "20300 loss: 2.0229688338164626\n",
            "20400 loss: 2.023825606195366\n",
            "20500 loss: 2.024229539150145\n",
            "20600 loss: 2.0246571800778215\n",
            "20700 loss: 2.0252332329001406\n",
            "20800 loss: 2.0254631360505635\n",
            "20900 loss: 2.0258121189660434\n",
            "21000 loss: 2.025689666095234\n",
            "21100 loss: 2.026152277891105\n",
            "21200 loss: 2.026713346495943\n",
            "21300 loss: 2.027005780834547\n",
            "21400 loss: 2.0276295484616376\n",
            "21500 loss: 2.0279966616297878\n",
            "21600 loss: 2.0286138568984136\n",
            "21700 loss: 2.0291464521588267\n",
            "21800 loss: 2.0296195028025075\n",
            "21900 loss: 2.030324578987409\n",
            "22000 loss: 2.0306321153640745\n",
            "22100 loss: 2.031653377497358\n",
            "22200 loss: 2.031823552997263\n",
            "22300 loss: 2.0320544223667794\n",
            "22400 loss: 2.032766368719084\n",
            "22500 loss: 2.0332315746943155\n",
            "22600 loss: 2.0335894094842724\n",
            "22700 loss: 2.0340313373053127\n",
            "22800 loss: 2.0351486642841707\n",
            "22900 loss: 2.0359457515733212\n",
            "23000 loss: 2.035965438573257\n",
            "23100 loss: 2.036763888664576\n",
            "23200 loss: 2.037510939091444\n",
            "23300 loss: 2.0382014238425077\n",
            "23400 loss: 2.0384838767031317\n",
            "23500 loss: 2.039229128639749\n",
            "23600 loss: 2.0397922152533368\n",
            "23700 loss: 2.040257889243621\n",
            "23800 loss: 2.0409458476355096\n",
            "23900 loss: 2.0415844875748688\n",
            "24000 loss: 2.042267933105429\n",
            "24100 loss: 2.0427044297402333\n",
            "24200 loss: 2.04318876681249\n",
            "24300 loss: 2.043960856302285\n",
            "24400 loss: 2.0445982300844348\n",
            "24500 loss: 2.04530988218833\n",
            "24600 loss: 2.0461094068075583\n",
            "24700 loss: 2.0469783617562127\n",
            "24800 loss: 2.04720023154251\n",
            "24900 loss: 2.0476783416836137\n",
            "25000 loss: 2.0484210700559617\n",
            "25100 loss: 2.0490817625494118\n",
            "25200 loss: 2.049229743362419\n",
            "25300 loss: 2.049670390253482\n",
            "25400 loss: 2.049209034931003\n",
            "25500 loss: 2.05025661240372\n",
            "25600 loss: 2.05127387560904\n",
            "25700 loss: 2.051931855094108\n",
            "25800 loss: 2.052711217135422\n",
            "25900 loss: 2.0526491264532893\n",
            "26000 loss: 2.0535631887362555\n",
            "26100 loss: 2.0545777497163678\n",
            "26200 loss: 2.0554217532254357\n",
            "26300 loss: 2.055312074187137\n",
            "26400 loss: 2.0562296711676047\n",
            "26500 loss: 2.0560179776380645\n",
            "26600 loss: 2.056612881749196\n",
            "26700 loss: 2.0571640697847147\n",
            "26800 loss: 2.0581630841475813\n",
            "26900 loss: 2.0589148206471513\n",
            "27000 loss: 2.0590314671595893\n",
            "27100 loss: 2.0595841471075573\n",
            "27200 loss: 2.0591953691664866\n",
            "27300 loss: 2.059322092100814\n",
            "27400 loss: 2.0600423221170465\n",
            "27500 loss: 2.0607203519084236\n",
            "27600 loss: 2.0613782853060876\n",
            "27700 loss: 2.0622632788406814\n",
            "27800 loss: 2.0626582515068193\n",
            "27900 loss: 2.062638346200348\n",
            "28000 loss: 2.0634504708605155\n",
            "28100 loss: 2.0640466881816497\n",
            "28200 loss: 2.06420241427337\n",
            "28300 loss: 2.064681796880156\n",
            "28400 loss: 2.0652836563679533\n",
            "28500 loss: 2.06631915114637\n",
            "28600 loss: 2.066843958888854\n",
            "28700 loss: 2.0672859844887297\n",
            "28800 loss: 2.0679477231038943\n",
            "28900 loss: 2.06824746538611\n",
            "29000 loss: 2.0683858793316214\n",
            "29100 loss: 2.068410833271099\n",
            "29200 loss: 2.069339579780624\n",
            "29300 loss: 2.0701769909883114\n",
            "29400 loss: 2.069457863058363\n",
            "29500 loss: 2.069114184816005\n",
            "29600 loss: 2.0699717104918247\n",
            "29700 loss: 2.0701961753866085\n",
            "29800 loss: 2.0713163310969436\n",
            "29900 loss: 2.071951681997465\n",
            "30000 loss: 2.072396425553163\n",
            "30100 loss: 2.0727197002057616\n",
            "30200 loss: 2.0738206864153312\n",
            "30300 loss: 2.0737897738765176\n",
            "30400 loss: 2.0741451130964252\n",
            "30500 loss: 2.074180525357606\n",
            "30600 loss: 2.073895123125681\n",
            "30700 loss: 2.074536241762801\n",
            "30800 loss: 2.0754073624487046\n",
            "30900 loss: 2.076421506470461\n",
            "31000 loss: 2.0757274538893853\n",
            "31100 loss: 2.0755639082557518\n",
            "31200 loss: 2.0748490407451605\n",
            "31300 loss: 2.0754909560969845\n",
            "31400 loss: 2.0760167249258914\n",
            "31500 loss: 2.0766576198963893\n",
            "31600 loss: 2.077638773955876\n",
            "31700 loss: 2.078011543254371\n",
            "31800 loss: 2.0784434959723517\n",
            "31900 loss: 2.078452556200536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [07:36<11:24, 228.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000 loss: 2.079174462340772\n",
            "100 loss: 2.2284418034553526\n",
            "200 loss: 2.0797475188970567\n",
            "300 loss: 2.0288313154379525\n",
            "400 loss: 1.999308380484581\n",
            "500 loss: 1.9824516563415526\n",
            "600 loss: 1.9691199962298076\n",
            "700 loss: 1.9497620342458997\n",
            "800 loss: 1.938374394774437\n",
            "900 loss: 1.927795062859853\n",
            "1000 loss: 1.9251543939113618\n",
            "1100 loss: 1.9227168474414131\n",
            "1200 loss: 1.9157286987702051\n",
            "1300 loss: 1.9122876901809986\n",
            "1400 loss: 1.9150858548283578\n",
            "1500 loss: 1.910751604437828\n",
            "1600 loss: 1.908532452173531\n",
            "1700 loss: 1.9090614202092675\n",
            "1800 loss: 1.9092638703518443\n",
            "1900 loss: 1.9095768480238162\n",
            "2000 loss: 1.9074763894975186\n",
            "2100 loss: 1.9088069211585181\n",
            "2200 loss: 1.90882630280473\n",
            "2300 loss: 1.9086685123391773\n",
            "2400 loss: 1.9124937435736258\n",
            "2500 loss: 1.9102684148073197\n",
            "2600 loss: 1.9092979064125282\n",
            "2700 loss: 1.9097722494822962\n",
            "2800 loss: 1.9092272435554436\n",
            "2900 loss: 1.9096584394060332\n",
            "3000 loss: 1.9066509288152058\n",
            "3100 loss: 1.9076791391065044\n",
            "3200 loss: 1.9087987527996302\n",
            "3300 loss: 1.9078783187721715\n",
            "3400 loss: 1.9061278991839465\n",
            "3500 loss: 1.9063714950425283\n",
            "3600 loss: 1.907459991408719\n",
            "3700 loss: 1.9085770040911598\n",
            "3800 loss: 1.9098989619706805\n",
            "3900 loss: 1.9100761825916095\n",
            "4000 loss: 1.9096887921690942\n",
            "4100 loss: 1.9094134249919799\n",
            "4200 loss: 1.9101806736560094\n",
            "4300 loss: 1.9108277148146962\n",
            "4400 loss: 1.9124000895294275\n",
            "4500 loss: 1.9124661535686918\n",
            "4600 loss: 1.9129977116636607\n",
            "4700 loss: 1.9148584250186351\n",
            "4800 loss: 1.914578458542625\n",
            "4900 loss: 1.9150246770284614\n",
            "5000 loss: 1.9157117379665374\n",
            "5100 loss: 1.9163179122934155\n",
            "5200 loss: 1.9174081174685405\n",
            "5300 loss: 1.9184805671448977\n",
            "5400 loss: 1.91961300112583\n",
            "5500 loss: 1.9210623914545233\n",
            "5600 loss: 1.9219530663958617\n",
            "5700 loss: 1.922052425334328\n",
            "5800 loss: 1.9223754797516197\n",
            "5900 loss: 1.9218722176955918\n",
            "6000 loss: 1.9241586227814356\n",
            "6100 loss: 1.9249200955375296\n",
            "6200 loss: 1.925902409188209\n",
            "6300 loss: 1.9252058763920314\n",
            "6400 loss: 1.9249299175851047\n",
            "6500 loss: 1.926468490600586\n",
            "6600 loss: 1.926263559514826\n",
            "6700 loss: 1.9268195031472106\n",
            "6800 loss: 1.927863346829134\n",
            "6900 loss: 1.9281893982057987\n",
            "7000 loss: 1.9288207858630588\n",
            "7100 loss: 1.9294555005221299\n",
            "7200 loss: 1.9300139498710633\n",
            "7300 loss: 1.9307257198634213\n",
            "7400 loss: 1.930756881591436\n",
            "7500 loss: 1.9301285325050355\n",
            "7600 loss: 1.9295152924092192\n",
            "7700 loss: 1.9308075894628252\n",
            "7800 loss: 1.9320491729485683\n",
            "7900 loss: 1.9332344094409217\n",
            "8000 loss: 1.9332209628373385\n",
            "8100 loss: 1.9342126690899883\n",
            "8200 loss: 1.9341545609148538\n",
            "8300 loss: 1.9344585796867508\n",
            "8400 loss: 1.9337138087692716\n",
            "8500 loss: 1.9346624511830948\n",
            "8600 loss: 1.9356373919165413\n",
            "8700 loss: 1.9359865390432291\n",
            "8800 loss: 1.9370881409401244\n",
            "8900 loss: 1.9368113255634736\n",
            "9000 loss: 1.9373165275388293\n",
            "9100 loss: 1.9386812858529143\n",
            "9200 loss: 1.938844513297081\n",
            "9300 loss: 1.9385261081751957\n",
            "9400 loss: 1.9390080372957472\n",
            "9500 loss: 1.9388516418180968\n",
            "9600 loss: 1.9411145727336407\n",
            "9700 loss: 1.942114994280117\n",
            "9800 loss: 1.9430473698037012\n",
            "9900 loss: 1.942217864905945\n",
            "10000 loss: 1.942701205432415\n",
            "10100 loss: 1.9432966105772718\n",
            "10200 loss: 1.9436974021500233\n",
            "10300 loss: 1.9451198145486777\n",
            "10400 loss: 1.9464409013894888\n",
            "10500 loss: 1.9480792096455892\n",
            "10600 loss: 1.9479305473251163\n",
            "10700 loss: 1.9479378307311335\n",
            "10800 loss: 1.9496604695033144\n",
            "10900 loss: 1.9492744879328876\n",
            "11000 loss: 1.9492102095430548\n",
            "11100 loss: 1.949567114705438\n",
            "11200 loss: 1.9508426504369294\n",
            "11300 loss: 1.9517085017039713\n",
            "11400 loss: 1.9534052134501307\n",
            "11500 loss: 1.9530109631911567\n",
            "11600 loss: 1.9528133828475558\n",
            "11700 loss: 1.953936272166733\n",
            "11800 loss: 1.9551282433938173\n",
            "11900 loss: 1.9533778442655292\n",
            "12000 loss: 1.954110025604566\n",
            "12100 loss: 1.95485942604128\n",
            "12200 loss: 1.9572200876572092\n",
            "12300 loss: 1.958017531536459\n",
            "12400 loss: 1.9594058560363707\n",
            "12500 loss: 1.9593294889640809\n",
            "12600 loss: 1.958713755418384\n",
            "12700 loss: 1.9589899927424634\n",
            "12800 loss: 1.9594018073938786\n",
            "12900 loss: 1.9611189391354258\n",
            "13000 loss: 1.9600183399548898\n",
            "13100 loss: 1.9581675190416001\n",
            "13200 loss: 1.959201741299846\n",
            "13300 loss: 1.958946723749763\n",
            "13400 loss: 1.9606147126653302\n",
            "13500 loss: 1.9607475989306415\n",
            "13600 loss: 1.9617265958821073\n",
            "13700 loss: 1.9632579877428764\n",
            "13800 loss: 1.9648103572061097\n",
            "13900 loss: 1.9640576235338938\n",
            "14000 loss: 1.9639760868379048\n",
            "14100 loss: 1.9634478023204398\n",
            "14200 loss: 1.963333375445554\n",
            "14300 loss: 1.9646539594827\n",
            "14400 loss: 1.9670439444399541\n",
            "14500 loss: 1.9673122553989806\n",
            "14600 loss: 1.965923279303394\n",
            "14700 loss: 1.9643538622872359\n",
            "14800 loss: 1.9646173421112267\n",
            "14900 loss: 1.9649646484931844\n",
            "15000 loss: 1.9643395791769027\n",
            "15100 loss: 1.965191898606471\n",
            "15200 loss: 1.9666104231539525\n",
            "15300 loss: 1.9672110386222017\n",
            "15400 loss: 1.9673371443810401\n",
            "15500 loss: 1.9680578159593767\n",
            "15600 loss: 1.9678860166286811\n",
            "15700 loss: 1.9684932899854746\n",
            "15800 loss: 1.9673960281021987\n",
            "15900 loss: 1.9666350425414318\n",
            "16000 loss: 1.966439276598394\n",
            "16100 loss: 1.965967356255336\n",
            "16200 loss: 1.9653504417192789\n",
            "16300 loss: 1.965710489515878\n",
            "16400 loss: 1.9666716957528416\n",
            "16500 loss: 1.9679500500433373\n",
            "16600 loss: 1.9676125504453499\n",
            "16700 loss: 1.9659332934575167\n",
            "16800 loss: 1.9647712893074467\n",
            "16900 loss: 1.9630609445988074\n",
            "17000 loss: 1.9643324386337224\n",
            "17100 loss: 1.9645037107335197\n",
            "17200 loss: 1.9627377043109993\n",
            "17300 loss: 1.962547532847162\n",
            "17400 loss: 1.963901403686781\n",
            "17500 loss: 1.9653217477151326\n",
            "17600 loss: 1.9657113901085475\n",
            "17700 loss: 1.9654905999211942\n",
            "17800 loss: 1.966143317554104\n",
            "17900 loss: 1.9667383274985426\n",
            "18000 loss: 1.967167710042662\n",
            "18100 loss: 1.9678437005057519\n",
            "18200 loss: 1.9682141395813817\n",
            "18300 loss: 1.968479846517245\n",
            "18400 loss: 1.9689509124373612\n",
            "18500 loss: 1.9694306813800657\n",
            "18600 loss: 1.9696834470604057\n",
            "18700 loss: 1.9702369509565638\n",
            "18800 loss: 1.9707601876620282\n",
            "18900 loss: 1.9711950018765434\n",
            "19000 loss: 1.9715796712668319\n",
            "19100 loss: 1.9721956900932403\n",
            "19200 loss: 1.973009843059505\n",
            "19300 loss: 1.9732609665177647\n",
            "19400 loss: 1.9735499553920068\n",
            "19500 loss: 1.9736770912775627\n",
            "19600 loss: 1.973811022776122\n",
            "19700 loss: 1.9740278361805805\n",
            "19800 loss: 1.9744467386783975\n",
            "19900 loss: 1.9748667721682458\n",
            "20000 loss: 1.9753706961780786\n",
            "20100 loss: 1.9758929433603192\n",
            "20200 loss: 1.9764429675499993\n",
            "20300 loss: 1.9769300168165433\n",
            "20400 loss: 1.9777553015158458\n",
            "20500 loss: 1.9781399610129797\n",
            "20600 loss: 1.9785269736809639\n",
            "20700 loss: 1.979110549489657\n",
            "20800 loss: 1.9793543859886435\n",
            "20900 loss: 1.9796178695716355\n",
            "21000 loss: 1.979484416782856\n",
            "21100 loss: 1.979895702812344\n",
            "21200 loss: 1.9804892879752618\n",
            "21300 loss: 1.9807132473881814\n",
            "21400 loss: 1.9813032872181073\n",
            "21500 loss: 1.9816915815680527\n",
            "21600 loss: 1.98228353721952\n",
            "21700 loss: 1.98283968330803\n",
            "21800 loss: 1.983308209806954\n",
            "21900 loss: 1.9839388136422798\n",
            "22000 loss: 1.9842192522151905\n",
            "22100 loss: 1.985213666067404\n",
            "22200 loss: 1.9853678864858173\n",
            "22300 loss: 1.9855862300679288\n",
            "22400 loss: 1.986230991879212\n",
            "22500 loss: 1.986697952919536\n",
            "22600 loss: 1.9870853024431034\n",
            "22700 loss: 1.9874956357977989\n",
            "22800 loss: 1.9886795589981372\n",
            "22900 loss: 1.9894662930377185\n",
            "23000 loss: 1.989520726268706\n",
            "23100 loss: 1.9902750983924578\n",
            "23200 loss: 1.9910238196238361\n",
            "23300 loss: 1.991748633760751\n",
            "23400 loss: 1.9920541568418852\n",
            "23500 loss: 1.9928100426019506\n",
            "23600 loss: 1.993345722144438\n",
            "23700 loss: 1.9937716262224858\n",
            "23800 loss: 1.994420619198755\n",
            "23900 loss: 1.9950557590204303\n",
            "24000 loss: 1.9957647148395579\n",
            "24100 loss: 1.996187493556268\n",
            "24200 loss: 1.9966956557050224\n",
            "24300 loss: 1.9974489377187603\n",
            "24400 loss: 1.9981052136299062\n",
            "24500 loss: 1.998811245049749\n",
            "24600 loss: 1.9996240221315282\n",
            "24700 loss: 2.000523062338713\n",
            "24800 loss: 2.0007636126851844\n",
            "24900 loss: 2.001249621740307\n",
            "25000 loss: 2.0020146935153007\n",
            "25100 loss: 2.0027045891270694\n",
            "25200 loss: 2.0028967197192094\n",
            "25300 loss: 2.0032971942165623\n",
            "25400 loss: 2.0028763629859827\n",
            "25500 loss: 2.0039613705359254\n",
            "25600 loss: 2.0049525359948164\n",
            "25700 loss: 2.0056926782562576\n",
            "25800 loss: 2.0065032331356707\n",
            "25900 loss: 2.0065392835374967\n",
            "26000 loss: 2.0074430848153737\n",
            "26100 loss: 2.008454711900817\n",
            "26200 loss: 2.009350972874019\n",
            "26300 loss: 2.0092054951485574\n",
            "26400 loss: 2.010167719678897\n",
            "26500 loss: 2.0100223229403764\n",
            "26600 loss: 2.010650733680205\n",
            "26700 loss: 2.0112815040841086\n",
            "26800 loss: 2.012286698206592\n",
            "26900 loss: 2.0130681676470212\n",
            "27000 loss: 2.0131568974852563\n",
            "27100 loss: 2.013765538132499\n",
            "27200 loss: 2.0134229142591358\n",
            "27300 loss: 2.0135675604111984\n",
            "27400 loss: 2.014303624275392\n",
            "27500 loss: 2.0149745396722447\n",
            "27600 loss: 2.015641265898079\n",
            "27700 loss: 2.016554662869295\n",
            "27800 loss: 2.0170092869769753\n",
            "27900 loss: 2.0169730639521792\n",
            "28000 loss: 2.0177464151914632\n",
            "28100 loss: 2.018423381931417\n",
            "28200 loss: 2.0186258043951177\n",
            "28300 loss: 2.0191217830219035\n",
            "28400 loss: 2.0198050790458497\n",
            "28500 loss: 2.020835364180699\n",
            "28600 loss: 2.0214958470330373\n",
            "28700 loss: 2.0219072921537773\n",
            "28800 loss: 2.0225760109246607\n",
            "28900 loss: 2.0230240886454762\n",
            "29000 loss: 2.023164870991789\n",
            "29100 loss: 2.023188986542708\n",
            "29200 loss: 2.024199521343594\n",
            "29300 loss: 2.025041719965967\n",
            "29400 loss: 2.0244366309654955\n",
            "29500 loss: 2.0241474644268975\n",
            "29600 loss: 2.025040609520432\n",
            "29700 loss: 2.0253309768077097\n",
            "29800 loss: 2.026463290786983\n",
            "29900 loss: 2.0271885687950064\n",
            "30000 loss: 2.0276631547152997\n",
            "30100 loss: 2.027952704756363\n",
            "30200 loss: 2.0290832900507563\n",
            "30300 loss: 2.029179003291791\n",
            "30400 loss: 2.0295504548263392\n",
            "30500 loss: 2.0296811533228296\n",
            "30600 loss: 2.0294304706438693\n",
            "30700 loss: 2.030147081502874\n",
            "30800 loss: 2.0309883287684483\n",
            "30900 loss: 2.032066475139467\n",
            "31000 loss: 2.031556955181783\n",
            "31100 loss: 2.031388947237343\n",
            "31200 loss: 2.030765503176894\n",
            "31300 loss: 2.031433196591493\n",
            "31400 loss: 2.032069558431009\n",
            "31500 loss: 2.032752349030404\n",
            "31600 loss: 2.033747220203469\n",
            "31700 loss: 2.0341929569511383\n",
            "31800 loss: 2.0347048221650366\n",
            "31900 loss: 2.0347213605821692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [11:19<07:31, 225.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000 loss: 2.0354336080718785\n",
            "100 loss: 2.1586719655990603\n",
            "200 loss: 2.013407570719719\n",
            "300 loss: 1.9649694085121154\n",
            "400 loss: 1.9373201274871825\n",
            "500 loss: 1.922537273168564\n",
            "600 loss: 1.9104167364041011\n",
            "700 loss: 1.8938491087300438\n",
            "800 loss: 1.8843842719495296\n",
            "900 loss: 1.8744227877590391\n",
            "1000 loss: 1.8718134320378303\n",
            "1100 loss: 1.870057117342949\n",
            "1200 loss: 1.863064178576072\n",
            "1300 loss: 1.859809053998727\n",
            "1400 loss: 1.8632038320388113\n",
            "1500 loss: 1.8599049610296885\n",
            "1600 loss: 1.8581407311558724\n",
            "1700 loss: 1.8593858789696414\n",
            "1800 loss: 1.8601743181877666\n",
            "1900 loss: 1.8603240004652424\n",
            "2000 loss: 1.85891953638196\n",
            "2100 loss: 1.8604802079143978\n",
            "2200 loss: 1.8609415211731737\n",
            "2300 loss: 1.8608421130024868\n",
            "2400 loss: 1.8652355379859606\n",
            "2500 loss: 1.8629401886463166\n",
            "2600 loss: 1.8621170325462635\n",
            "2700 loss: 1.8627938245402442\n",
            "2800 loss: 1.8625850319862365\n",
            "2900 loss: 1.8631980790351999\n",
            "3000 loss: 1.8603467333316803\n",
            "3100 loss: 1.861727966685449\n",
            "3200 loss: 1.8631289121881127\n",
            "3300 loss: 1.8622160802104257\n",
            "3400 loss: 1.8606813043005326\n",
            "3500 loss: 1.8610109790733882\n",
            "3600 loss: 1.862267910970582\n",
            "3700 loss: 1.8635892092215047\n",
            "3800 loss: 1.8650330695666766\n",
            "3900 loss: 1.8653488995478704\n",
            "4000 loss: 1.8650701297223569\n",
            "4100 loss: 1.864950397479825\n",
            "4200 loss: 1.8656995377654122\n",
            "4300 loss: 1.8664877330147942\n",
            "4400 loss: 1.8681806235692717\n",
            "4500 loss: 1.8684649025069342\n",
            "4600 loss: 1.8690798337563224\n",
            "4700 loss: 1.871305759637914\n",
            "4800 loss: 1.87131297228237\n",
            "4900 loss: 1.8719140514792227\n",
            "5000 loss: 1.8728622249364852\n",
            "5100 loss: 1.873616194187426\n",
            "5200 loss: 1.8746731808323127\n",
            "5300 loss: 1.8761341637710356\n",
            "5400 loss: 1.8771910030532766\n",
            "5500 loss: 1.8786748591336337\n",
            "5600 loss: 1.8797256149351598\n",
            "5700 loss: 1.8800162349458327\n",
            "5800 loss: 1.8802600258177724\n",
            "5900 loss: 1.879827051021285\n",
            "6000 loss: 1.8823106686870257\n",
            "6100 loss: 1.8829975175857543\n",
            "6200 loss: 1.8840597995250457\n",
            "6300 loss: 1.8834218331556472\n",
            "6400 loss: 1.8833243866451084\n",
            "6500 loss: 1.884962565788856\n",
            "6600 loss: 1.8847082507971562\n",
            "6700 loss: 1.8854096896434898\n",
            "6800 loss: 1.8867713789554204\n",
            "6900 loss: 1.8872323798096697\n",
            "7000 loss: 1.8879346264941352\n",
            "7100 loss: 1.8887230551578629\n",
            "7200 loss: 1.889350180460347\n",
            "7300 loss: 1.8901995296674232\n",
            "7400 loss: 1.8902877966938791\n",
            "7500 loss: 1.8897218908627829\n",
            "7600 loss: 1.8892396431690768\n",
            "7700 loss: 1.8905750527784422\n",
            "7800 loss: 1.891871586212745\n",
            "7900 loss: 1.8932304197776166\n",
            "8000 loss: 1.8932760043591261\n",
            "8100 loss: 1.894307351215386\n",
            "8200 loss: 1.8944906409484583\n",
            "8300 loss: 1.894830698004688\n",
            "8400 loss: 1.8941811333241916\n",
            "8500 loss: 1.895281794758404\n",
            "8600 loss: 1.896325233329174\n",
            "8700 loss: 1.896679721577414\n",
            "8800 loss: 1.8978762180967765\n",
            "8900 loss: 1.8977402421061913\n",
            "9000 loss: 1.8983387794362174\n",
            "9100 loss: 1.8997715834208897\n",
            "9200 loss: 1.9000473520159722\n",
            "9300 loss: 1.8997653231954061\n",
            "9400 loss: 1.9002732252821009\n",
            "9500 loss: 1.900249596520474\n",
            "9600 loss: 1.90259803848962\n",
            "9700 loss: 1.9036099020353299\n",
            "9800 loss: 1.9047024078271826\n",
            "9900 loss: 1.903981326035779\n",
            "10000 loss: 1.90456299380064\n",
            "10100 loss: 1.9052637134448136\n",
            "10200 loss: 1.905738466522273\n",
            "10300 loss: 1.9072100023274283\n",
            "10400 loss: 1.9084962591987389\n",
            "10500 loss: 1.910044550600506\n",
            "10600 loss: 1.910149918592201\n",
            "10700 loss: 1.9101844295497252\n",
            "10800 loss: 1.9118838980352437\n",
            "10900 loss: 1.9116507487887637\n",
            "11000 loss: 1.9115800749063492\n",
            "11100 loss: 1.911998799545271\n",
            "11200 loss: 1.9132754105968135\n",
            "11300 loss: 1.9142386170298653\n",
            "11400 loss: 1.9159350379830913\n",
            "11500 loss: 1.9157983589690664\n",
            "11600 loss: 1.9156177263321548\n",
            "11700 loss: 1.9167524687143473\n",
            "11800 loss: 1.9180282805026587\n",
            "11900 loss: 1.9164520058712038\n",
            "12000 loss: 1.9172330444951853\n",
            "12100 loss: 1.9181367680356523\n",
            "12200 loss: 1.9204830330317137\n",
            "12300 loss: 1.9213693108791259\n",
            "12400 loss: 1.9226230251500684\n",
            "12500 loss: 1.9228389653015137\n",
            "12600 loss: 1.9223542196220822\n",
            "12700 loss: 1.9225821438455206\n",
            "12800 loss: 1.9231338479463012\n",
            "12900 loss: 1.9248417643243951\n",
            "13000 loss: 1.9240816944287373\n",
            "13100 loss: 1.922341359617146\n",
            "13200 loss: 1.923316145146435\n",
            "13300 loss: 1.9231745247240353\n",
            "13400 loss: 1.9248877804893165\n",
            "13500 loss: 1.9251424492950793\n",
            "13600 loss: 1.9261858400395688\n",
            "13700 loss: 1.9276424562974568\n",
            "13800 loss: 1.9293658640807954\n",
            "13900 loss: 1.9288950149575583\n",
            "14000 loss: 1.9288979667127133\n",
            "14100 loss: 1.9284388382933664\n",
            "14200 loss: 1.928415822751925\n",
            "14300 loss: 1.9299161764500024\n",
            "14400 loss: 1.9322486921110087\n",
            "14500 loss: 1.9326601996216282\n",
            "14600 loss: 1.9314451035571425\n",
            "14700 loss: 1.9300845638181077\n",
            "14800 loss: 1.9302380403956851\n",
            "14900 loss: 1.9305901909514562\n",
            "15000 loss: 1.93004423661232\n",
            "15100 loss: 1.9310287570558637\n",
            "15200 loss: 1.9324214271652072\n",
            "15300 loss: 1.9331232872352102\n",
            "15400 loss: 1.9333148981998494\n",
            "15500 loss: 1.9340011482008042\n",
            "15600 loss: 1.9338115297983853\n",
            "15700 loss: 1.9345429421458276\n",
            "15800 loss: 1.9336934205550182\n",
            "15900 loss: 1.9330657461754182\n",
            "16000 loss: 1.9327913547456264\n",
            "16100 loss: 1.932355003364338\n",
            "16200 loss: 1.9317860057766054\n",
            "16300 loss: 1.9322549748713254\n",
            "16400 loss: 1.933251127847811\n",
            "16500 loss: 1.9345261827815663\n",
            "16600 loss: 1.9342847433219472\n",
            "16700 loss: 1.9327428686333274\n",
            "16800 loss: 1.9316645540190593\n",
            "16900 loss: 1.9299968130348701\n",
            "17000 loss: 1.9314430528668796\n",
            "17100 loss: 1.931664407246294\n",
            "17200 loss: 1.9300551885920902\n",
            "17300 loss: 1.9298882272477784\n",
            "17400 loss: 1.9314031597526593\n",
            "17500 loss: 1.9327174344062805\n",
            "17600 loss: 1.9330985146557742\n",
            "17700 loss: 1.9329873005683813\n",
            "17800 loss: 1.9336986409144454\n",
            "17900 loss: 1.9341604515760304\n",
            "18000 loss: 1.9344895324905713\n",
            "18100 loss: 1.9351097945547895\n",
            "18200 loss: 1.9352953187580948\n",
            "18300 loss: 1.9354878653724337\n",
            "18400 loss: 1.9358820974438087\n",
            "18500 loss: 1.936193420867662\n",
            "18600 loss: 1.9363704941413735\n",
            "18700 loss: 1.9368171782799584\n",
            "18800 loss: 1.937309627044708\n",
            "18900 loss: 1.937654148661901\n",
            "19000 loss: 1.9379448741799907\n",
            "19100 loss: 1.938456321992175\n",
            "19200 loss: 1.9392133240153393\n",
            "19300 loss: 1.9394203493817483\n",
            "19400 loss: 1.9396893222061629\n",
            "19500 loss: 1.939729200112514\n",
            "19600 loss: 1.939793990296977\n",
            "19700 loss: 1.939981424439377\n",
            "19800 loss: 1.9403686581296151\n",
            "19900 loss: 1.9407014283942219\n",
            "20000 loss: 1.941165248376131\n",
            "20100 loss: 1.9416240392276898\n",
            "20200 loss: 1.942113113975761\n",
            "20300 loss: 1.9425774898904884\n",
            "20400 loss: 1.943360074063142\n",
            "20500 loss: 1.9437113679095013\n",
            "20600 loss: 1.944074697245672\n",
            "20700 loss: 1.9446482082899066\n",
            "20800 loss: 1.9448840544487422\n",
            "20900 loss: 1.9450985603013107\n",
            "21000 loss: 1.9449500175828025\n",
            "21100 loss: 1.9453393954801332\n",
            "21200 loss: 1.945927004673571\n",
            "21300 loss: 1.9461188133725538\n",
            "21400 loss: 1.9466643414374825\n",
            "21500 loss: 1.9470477272133495\n",
            "21600 loss: 1.9475931187029238\n",
            "21700 loss: 1.9481368827215537\n",
            "21800 loss: 1.9486221102782346\n",
            "21900 loss: 1.9492145697221364\n",
            "22000 loss: 1.9494851771593094\n",
            "22100 loss: 1.9504269777326022\n",
            "22200 loss: 1.9505644035339356\n",
            "22300 loss: 1.9507789947473415\n",
            "22400 loss: 1.9514118622136967\n",
            "22500 loss: 1.9518784162362417\n",
            "22600 loss: 1.952273664817346\n",
            "22700 loss: 1.9526285392045974\n",
            "22800 loss: 1.9538162501313185\n",
            "22900 loss: 1.9545822729996718\n",
            "23000 loss: 1.9546593671285588\n",
            "23100 loss: 1.9553627714063182\n",
            "23200 loss: 1.9561061688872248\n",
            "23300 loss: 1.9568741139449786\n",
            "23400 loss: 1.9571952728354014\n",
            "23500 loss: 1.957968517519058\n",
            "23600 loss: 1.9584659982106443\n",
            "23700 loss: 1.9588410058579868\n",
            "23800 loss: 1.9594609234137694\n",
            "23900 loss: 1.960083506967732\n",
            "24000 loss: 1.9607831173365315\n",
            "24100 loss: 1.9611942386008892\n",
            "24200 loss: 1.9617146270260337\n",
            "24300 loss: 1.9624537959192025\n",
            "24400 loss: 1.9630828507098017\n",
            "24500 loss: 1.9637837075870865\n",
            "24600 loss: 1.9646019845013696\n",
            "24700 loss: 1.9655264578343403\n",
            "24800 loss: 1.9657603069875509\n",
            "24900 loss: 1.9662201876309982\n",
            "25000 loss: 1.9669704037499427\n",
            "25100 loss: 1.9677063586916106\n",
            "25200 loss: 1.9679393659106323\n",
            "25300 loss: 1.9683153262369246\n",
            "25400 loss: 1.967926384884072\n",
            "25500 loss: 1.969019555028747\n",
            "25600 loss: 1.9699941364466214\n",
            "25700 loss: 1.9707848168184785\n",
            "25800 loss: 1.971615394869054\n",
            "25900 loss: 1.9717033388370713\n",
            "26000 loss: 1.9725931917176798\n",
            "26100 loss: 1.9735955638104472\n",
            "26200 loss: 1.9744926002212153\n",
            "26300 loss: 1.9743218380756704\n",
            "26400 loss: 1.9753099209544334\n",
            "26500 loss: 1.975210424929295\n",
            "26600 loss: 1.975839493182817\n",
            "26700 loss: 1.9765041601948077\n",
            "26800 loss: 1.9775184071664489\n",
            "26900 loss: 1.978299344155425\n",
            "27000 loss: 1.9783688426304746\n",
            "27100 loss: 1.9790142462460318\n",
            "27200 loss: 1.9787191662740182\n",
            "27300 loss: 1.978868919058597\n",
            "27400 loss: 1.9796204872875318\n",
            "27500 loss: 1.9802974619757046\n",
            "27600 loss: 1.9809855710833832\n",
            "27700 loss: 1.9818576445454725\n",
            "27800 loss: 1.9823535622731387\n",
            "27900 loss: 1.9823015796518668\n",
            "28000 loss: 1.9830246638293778\n",
            "28100 loss: 1.983707586622323\n",
            "28200 loss: 1.9839844257041073\n",
            "28300 loss: 1.984465968754182\n",
            "28400 loss: 1.9851890718831982\n",
            "28500 loss: 1.9862022288075665\n",
            "28600 loss: 1.9869394896017922\n",
            "28700 loss: 1.987314730652118\n",
            "28800 loss: 1.98797905160942\n",
            "28900 loss: 1.9885173609945601\n",
            "29000 loss: 1.9886684150798568\n",
            "29100 loss: 1.988719110765408\n",
            "29200 loss: 1.9897860092557456\n",
            "29300 loss: 1.9906102171102888\n",
            "29400 loss: 1.9900743687902989\n",
            "29500 loss: 1.9898295830569024\n",
            "29600 loss: 1.9907014066202415\n",
            "29700 loss: 1.9910186875769587\n",
            "29800 loss: 1.9921527096269114\n",
            "29900 loss: 1.992928523991419\n",
            "30000 loss: 1.9934313671290875\n",
            "30100 loss: 1.9936924755791097\n",
            "30200 loss: 1.9948417918157104\n",
            "30300 loss: 1.9950369294897559\n",
            "30400 loss: 1.9954022196935195\n",
            "30500 loss: 1.9955430451787886\n",
            "30600 loss: 1.9953747932603156\n",
            "30700 loss: 1.996110980493238\n",
            "30800 loss: 1.9969381581285557\n",
            "30900 loss: 1.9980593286961028\n",
            "31000 loss: 1.9976502836154353\n",
            "31100 loss: 1.9975098405409473\n",
            "31200 loss: 1.9969863772029295\n",
            "31300 loss: 1.9976395595664034\n",
            "31400 loss: 1.9983307781132162\n",
            "31500 loss: 1.999066553431844\n",
            "31600 loss: 2.00001339124162\n",
            "31700 loss: 2.0005234724960115\n",
            "31800 loss: 2.0010755500459823\n",
            "31900 loss: 2.0011082987520012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [14:57<03:42, 222.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000 loss: 2.001787384772673\n",
            "100 loss: 2.096184639930725\n",
            "200 loss: 1.9577952152490616\n",
            "300 loss: 1.9146470328172047\n",
            "400 loss: 1.8900034570693969\n",
            "500 loss: 1.8762060840129853\n",
            "600 loss: 1.8647828362385432\n",
            "700 loss: 1.8507091212272644\n",
            "800 loss: 1.842885570973158\n",
            "900 loss: 1.8335737211174434\n",
            "1000 loss: 1.8313607861995698\n",
            "1100 loss: 1.8293718166784807\n",
            "1200 loss: 1.8226569435993831\n",
            "1300 loss: 1.8197561091643113\n",
            "1400 loss: 1.8236978632637433\n",
            "1500 loss: 1.8210105921427409\n",
            "1600 loss: 1.819961281940341\n",
            "1700 loss: 1.821612182154375\n",
            "1800 loss: 1.823024149139722\n",
            "1900 loss: 1.823404287852739\n",
            "2000 loss: 1.8225304153561592\n",
            "2100 loss: 1.824157748733248\n",
            "2200 loss: 1.8249748174710707\n",
            "2300 loss: 1.8249196614389833\n",
            "2400 loss: 1.8294596226265034\n",
            "2500 loss: 1.827091033244133\n",
            "2600 loss: 1.8264132437109948\n",
            "2700 loss: 1.82730337286437\n",
            "2800 loss: 1.8273962673757758\n",
            "2900 loss: 1.828188793515337\n",
            "3000 loss: 1.8255954394141833\n",
            "3100 loss: 1.8273988133284353\n",
            "3200 loss: 1.8290126817114651\n",
            "3300 loss: 1.827938710252444\n",
            "3400 loss: 1.8265734339636914\n",
            "3500 loss: 1.8267901066541672\n",
            "3600 loss: 1.8281808889077769\n",
            "3700 loss: 1.8296041546802262\n",
            "3800 loss: 1.8311032599838157\n",
            "3900 loss: 1.8314739757929093\n",
            "4000 loss: 1.8311846868693828\n",
            "4100 loss: 1.8312276931797586\n",
            "4200 loss: 1.8319921845765341\n",
            "4300 loss: 1.8327723272179448\n",
            "4400 loss: 1.8346068544821306\n",
            "4500 loss: 1.8349937249289618\n",
            "4600 loss: 1.8356351672048155\n",
            "4700 loss: 1.838073230880372\n",
            "4800 loss: 1.8382962683091562\n",
            "4900 loss: 1.8390184201513018\n",
            "5000 loss: 1.840106335711479\n",
            "5100 loss: 1.8410279286141489\n",
            "5200 loss: 1.8420193637334383\n",
            "5300 loss: 1.8437088710856888\n",
            "5400 loss: 1.8446409474699585\n",
            "5500 loss: 1.8460474665381692\n",
            "5600 loss: 1.847163931599685\n",
            "5700 loss: 1.847649400087825\n",
            "5800 loss: 1.8478231679571087\n",
            "5900 loss: 1.8474606540849654\n",
            "6000 loss: 1.8500668982863426\n",
            "6100 loss: 1.850766048060089\n",
            "6200 loss: 1.8518833708186304\n",
            "6300 loss: 1.8512883900839185\n",
            "6400 loss: 1.8513325830549001\n",
            "6500 loss: 1.8530824175614578\n",
            "6600 loss: 1.8528277523770478\n",
            "6700 loss: 1.8536044179325672\n",
            "6800 loss: 1.8551666179474662\n",
            "6900 loss: 1.8556814386879188\n",
            "7000 loss: 1.856489815064839\n",
            "7100 loss: 1.8573634449193175\n",
            "7200 loss: 1.8580218020412658\n",
            "7300 loss: 1.8590398452379933\n",
            "7400 loss: 1.8591713568648776\n",
            "7500 loss: 1.8586238985061645\n",
            "7600 loss: 1.858219691954161\n",
            "7700 loss: 1.859583052133585\n",
            "7800 loss: 1.8608418290737347\n",
            "7900 loss: 1.8623010417026808\n",
            "8000 loss: 1.8623924137055874\n",
            "8100 loss: 1.863541498184204\n",
            "8200 loss: 1.8638673453650823\n",
            "8300 loss: 1.8642450124671661\n",
            "8400 loss: 1.8636527304848036\n",
            "8500 loss: 1.8648744349479676\n",
            "8600 loss: 1.8659800947266956\n",
            "8700 loss: 1.866349974706255\n",
            "8800 loss: 1.8675882124900818\n",
            "8900 loss: 1.867574732732237\n",
            "9000 loss: 1.8682719512846735\n",
            "9100 loss: 1.8697271737227072\n",
            "9200 loss: 1.870087909044131\n",
            "9300 loss: 1.8698856653385265\n",
            "9400 loss: 1.8703548216375898\n",
            "9500 loss: 1.87036301373808\n",
            "9600 loss: 1.87267881186679\n",
            "9700 loss: 1.8737307866393906\n",
            "9800 loss: 1.8749288126704644\n",
            "9900 loss: 1.8743116964053625\n",
            "10000 loss: 1.8749338491499423\n",
            "10100 loss: 1.8756742576738394\n",
            "10200 loss: 1.8762002493762502\n",
            "10300 loss: 1.8777078357078496\n",
            "10400 loss: 1.8790382594729846\n",
            "10500 loss: 1.880528708997227\n",
            "10600 loss: 1.8807771942131923\n",
            "10700 loss: 1.8807902770343228\n",
            "10800 loss: 1.8824991219959877\n",
            "10900 loss: 1.8823278459223038\n",
            "11000 loss: 1.8822602070082317\n",
            "11100 loss: 1.8826510068156699\n",
            "11200 loss: 1.883969101017075\n",
            "11300 loss: 1.8850566886853328\n",
            "11400 loss: 1.8867583077779988\n",
            "11500 loss: 1.886781773219938\n",
            "11600 loss: 1.8866131955436591\n",
            "11700 loss: 1.8877853607367248\n",
            "11800 loss: 1.8891017346917574\n",
            "11900 loss: 1.8876153447898496\n",
            "12000 loss: 1.8884293207873901\n",
            "12100 loss: 1.8892937008556256\n",
            "12200 loss: 1.8915950804411388\n",
            "12300 loss: 1.8925200796951123\n",
            "12400 loss: 1.8937295562173089\n",
            "12500 loss: 1.8940678583192825\n",
            "12600 loss: 1.893677072690593\n",
            "12700 loss: 1.8938315754753399\n",
            "12800 loss: 1.8945106654847041\n",
            "12900 loss: 1.8962281379450199\n",
            "13000 loss: 1.8955965379522397\n",
            "13100 loss: 1.8939769125485237\n",
            "13200 loss: 1.8948617822880094\n",
            "13300 loss: 1.894785529388521\n",
            "13400 loss: 1.8965136292621272\n",
            "13500 loss: 1.8969021107708965\n",
            "13600 loss: 1.898002400389489\n",
            "13700 loss: 1.8994066708105324\n",
            "13800 loss: 1.9011982361734778\n",
            "13900 loss: 1.9009483767070359\n",
            "14000 loss: 1.9010045822177615\n",
            "14100 loss: 1.9005835652097742\n",
            "14200 loss: 1.9006404979547984\n",
            "14300 loss: 1.9022612194331376\n",
            "14400 loss: 1.9045399511108796\n",
            "14500 loss: 1.9050164240557572\n",
            "14600 loss: 1.903901715674629\n",
            "14700 loss: 1.9026990231243122\n",
            "14800 loss: 1.9027517579133446\n",
            "14900 loss: 1.9031026288886999\n",
            "15000 loss: 1.9025440288623174\n",
            "15100 loss: 1.9036233333483437\n",
            "15200 loss: 1.905035970022804\n",
            "15300 loss: 1.905832017216028\n",
            "15400 loss: 1.9060794908892025\n",
            "15500 loss: 1.906736791095426\n",
            "15600 loss: 1.9065397168122804\n",
            "15700 loss: 1.907303863665101\n",
            "15800 loss: 1.9066755005679552\n",
            "15900 loss: 1.906162113041248\n",
            "16000 loss: 1.905886849552393\n",
            "16100 loss: 1.9054590705166692\n",
            "16200 loss: 1.9049152318452611\n",
            "16300 loss: 1.9054397402255814\n",
            "16400 loss: 1.9064609677362734\n",
            "16500 loss: 1.9077432787743482\n",
            "16600 loss: 1.907566213539566\n",
            "16700 loss: 1.9061526096866517\n",
            "16800 loss: 1.905141315222496\n",
            "16900 loss: 1.90349698654646\n",
            "17000 loss: 1.9049688304627643\n",
            "17100 loss: 1.9052033466449259\n",
            "17200 loss: 1.9036922596568284\n",
            "17300 loss: 1.9034930569243569\n",
            "17400 loss: 1.9051051773627599\n",
            "17500 loss: 1.9063694063322885\n",
            "17600 loss: 1.9068072563613003\n",
            "17700 loss: 1.906768054537854\n",
            "17800 loss: 1.9075012333875292\n",
            "17900 loss: 1.9078900653356947\n",
            "18000 loss: 1.908151961021953\n",
            "18100 loss: 1.9086957858082998\n",
            "18200 loss: 1.9087294109247543\n",
            "18300 loss: 1.9088642266799842\n",
            "18400 loss: 1.9091669745743274\n",
            "18500 loss: 1.9093636990560068\n",
            "18600 loss: 1.9094705811495423\n",
            "18700 loss: 1.9098314519744506\n",
            "18800 loss: 1.9102884830883209\n",
            "18900 loss: 1.9105564595341051\n",
            "19000 loss: 1.9107684596776962\n",
            "19100 loss: 1.91120363666125\n",
            "19200 loss: 1.9119246478440861\n",
            "19300 loss: 1.9120929606034966\n",
            "19400 loss: 1.9123419315053016\n",
            "19500 loss: 1.9123073129103734\n",
            "19600 loss: 1.9123281426819003\n",
            "19700 loss: 1.912487451122497\n",
            "19800 loss: 1.9128627673845098\n",
            "19900 loss: 1.913142087513478\n",
            "20000 loss: 1.9135619921445846\n",
            "20100 loss: 1.9139921456367814\n",
            "20200 loss: 1.914473623007831\n",
            "20300 loss: 1.9149176159339585\n",
            "20400 loss: 1.9156502887548186\n",
            "20500 loss: 1.9159788581685322\n",
            "20600 loss: 1.9163102913770862\n",
            "20700 loss: 1.9168355842604154\n",
            "20800 loss: 1.9170559940945644\n",
            "20900 loss: 1.9172424308706129\n",
            "21000 loss: 1.9171005729436874\n",
            "21100 loss: 1.9174801358170983\n",
            "21200 loss: 1.9180518759140428\n",
            "21300 loss: 1.9182144644450694\n",
            "21400 loss: 1.9187422820173692\n",
            "21500 loss: 1.9191142997686252\n",
            "21600 loss: 1.9196228857890323\n",
            "21700 loss: 1.9201516044139861\n",
            "21800 loss: 1.9206506854569145\n",
            "21900 loss: 1.9212065097730453\n",
            "22000 loss: 1.9214769872318616\n",
            "22100 loss: 1.9223789917559646\n",
            "22200 loss: 1.9224997119592118\n",
            "22300 loss: 1.9227079442958661\n",
            "22400 loss: 1.923327554336616\n",
            "22500 loss: 1.9237860367827946\n",
            "22600 loss: 1.9241786227553292\n",
            "22700 loss: 1.9245085347092625\n",
            "22800 loss: 1.925687295038972\n",
            "22900 loss: 1.9264334773574854\n",
            "23000 loss: 1.9265248431718869\n",
            "23100 loss: 1.9271949202493155\n",
            "23200 loss: 1.9279251049227755\n",
            "23300 loss: 1.9287183523612985\n",
            "23400 loss: 1.9290338637202213\n",
            "23500 loss: 1.9298077027670881\n",
            "23600 loss: 1.9302842297468146\n",
            "23700 loss: 1.9306207252982297\n",
            "23800 loss: 1.9312030199420551\n",
            "23900 loss: 1.9317837871293144\n",
            "24000 loss: 1.932490139293174\n",
            "24100 loss: 1.93290416021812\n",
            "24200 loss: 1.9334250323018751\n",
            "24300 loss: 1.9341327546923248\n",
            "24400 loss: 1.9347266477321992\n",
            "24500 loss: 1.9354209317796085\n",
            "24600 loss: 1.936234936929815\n",
            "24700 loss: 1.9371713710048422\n",
            "24800 loss: 1.9373910713412108\n",
            "24900 loss: 1.9378323080190214\n",
            "25000 loss: 1.9385753013443947\n",
            "25100 loss: 1.9393143808865452\n",
            "25200 loss: 1.9395779992592712\n",
            "25300 loss: 1.9399437079622812\n",
            "25400 loss: 1.9395952248455972\n",
            "25500 loss: 1.940675995333522\n",
            "25600 loss: 1.9416545684146695\n",
            "25700 loss: 1.942468072011313\n",
            "25800 loss: 1.943297576241253\n",
            "25900 loss: 1.9434016022788052\n",
            "26000 loss: 1.9442698089411625\n",
            "26100 loss: 1.9452595219872464\n",
            "26200 loss: 1.9461286160304345\n",
            "26300 loss: 1.945954006613434\n",
            "26400 loss: 1.9469591363474275\n",
            "26500 loss: 1.9468969743499216\n",
            "26600 loss: 1.947524717779536\n",
            "26700 loss: 1.9482016656081775\n",
            "26800 loss: 1.949219441113632\n",
            "26900 loss: 1.9500108265278508\n",
            "27000 loss: 1.9500825806083502\n",
            "27100 loss: 1.9507462488219307\n",
            "27200 loss: 1.9505015108931592\n",
            "27300 loss: 1.9506560476817492\n",
            "27400 loss: 1.9514119311340534\n",
            "27500 loss: 1.952100932175463\n",
            "27600 loss: 1.9528042318661143\n",
            "27700 loss: 1.953644247827547\n",
            "27800 loss: 1.9541791488819842\n",
            "27900 loss: 1.9541233138991085\n",
            "28000 loss: 1.9548181335606745\n",
            "28100 loss: 1.9555034601582326\n",
            "28200 loss: 1.955820522640191\n",
            "28300 loss: 1.9562910091434689\n",
            "28400 loss: 1.957014171760267\n",
            "28500 loss: 1.9580095532714274\n",
            "28600 loss: 1.9587960736255545\n",
            "28700 loss: 1.9591534624967841\n",
            "28800 loss: 1.9598027030399277\n",
            "28900 loss: 1.9603969480319006\n",
            "29000 loss: 1.960581890334343\n",
            "29100 loss: 1.9606585088730677\n",
            "29200 loss: 1.9617428919244302\n",
            "29300 loss: 1.96254639722178\n",
            "29400 loss: 1.9620841274070902\n",
            "29500 loss: 1.9619124078487946\n",
            "29600 loss: 1.962773070917339\n",
            "29700 loss: 1.9631080300017238\n",
            "29800 loss: 1.964238999671984\n",
            "29900 loss: 1.965047969983573\n",
            "30000 loss: 1.9655741318802038\n",
            "30100 loss: 1.9658404751810123\n",
            "30200 loss: 1.9669724387384409\n",
            "30300 loss: 1.967249571370213\n",
            "30400 loss: 1.9676178779825568\n",
            "30500 loss: 1.96779003245127\n",
            "30600 loss: 1.9676835737216707\n",
            "30700 loss: 1.968417521654977\n",
            "30800 loss: 1.9692331169119903\n",
            "30900 loss: 1.9703769641883164\n",
            "31000 loss: 1.97005605137925\n",
            "31100 loss: 1.9699208176768477\n",
            "31200 loss: 1.9694981363549446\n",
            "31300 loss: 1.9701372544891156\n",
            "31400 loss: 1.9708653592512866\n",
            "31500 loss: 1.9716278040390165\n",
            "31600 loss: 1.9725465360340437\n",
            "31700 loss: 1.9731286063686906\n",
            "31800 loss: 1.9737372847791739\n",
            "31900 loss: 1.973798899446909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [18:34<00:00, 222.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000 loss: 1.9744793342817575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "total_loss = 0\n",
        "total_ct = 0\n",
        "total_epochs = 5\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for _ in tqdm.tqdm(range(total_epochs)):\n",
        "    total_loss = 0\n",
        "    total_ct = 0\n",
        "    for name in open(names_path, 'r'):\n",
        "        name = name.lower().strip()\n",
        "        # Add the start and end padding token\n",
        "        name = '.' + name + '.'\n",
        "        # name[:-1]\n",
        "        x_data = torch.tensor([stoi[c] for c in name[:-1]])\n",
        "        # name[1:]\n",
        "        y_data = torch.tensor([stoi[c] for c in name[1:]])\n",
        "        logits = []\n",
        "        # Set the hidden state to random\n",
        "        h = torch.zeros(1, d_h)\n",
        "        # Zero the grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Loop through each token and get the new h and then pass it forward\n",
        "        # Accumulate all the logits\n",
        "        for x in x_data:\n",
        "            y, h = model(x.unsqueeze(0), h)\n",
        "\n",
        "            logits.append(y)\n",
        "\n",
        "        # Put all the logits into one tensor\n",
        "        logits = torch.cat(logits, dim=0)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(logits, y_data)\n",
        "\n",
        "        # Get the new gradient\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients at max norm 0.1\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "        # Do a gradient update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the loss for the batch and get the number of batches\n",
        "        total_loss += loss.item()\n",
        "        total_ct += 1\n",
        "\n",
        "        if total_ct and total_ct % 100 == 0:\n",
        "            print(f\"{total_ct} loss: {total_loss / total_ct}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "83f5f990",
      "metadata": {
        "id": "83f5f990",
        "outputId": "e50e5143-be1d-4ff5-c1c0-560b174e087a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preplexity:  13.036799430847168\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    # Get perplexity\n",
        "    sumneglogp = 0\n",
        "    T = 0\n",
        "    for name in open(names_path, 'r'):\n",
        "        name = name.lower().strip()\n",
        "        T += len(name)\n",
        "        name = '.' + name\n",
        "        # Get the name from index 0 to -1 exclusive end\n",
        "        x_data = torch.tensor([stoi[c] for c in name[:-1]])\n",
        "        # Get the y from index 1 to end inclusive end\n",
        "        y_data = torch.tensor([stoi[c] for c in name[1:]])\n",
        "        # logits per token prediction\n",
        "        logits = []\n",
        "        # Initialize the h vector to random\n",
        "        h = torch.zeros(1, d_h)\n",
        "        # Loop over each chracter in the name and pass h and this into the RNN\n",
        "        # Get the new logit\n",
        "        for x in x_data:\n",
        "            # Get the int for x\n",
        "            x = x.unsqueeze(0)\n",
        "            # Get z and h\n",
        "            z, h = model(x, h)\n",
        "            # Append to logit\n",
        "            logits.append(z)\n",
        "\n",
        "        # Get all the logits for each character\n",
        "        logits = torch.cat(logits, dim=0)\n",
        "\n",
        "        # Compute the loss across all characters\n",
        "        loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")(logits, y_data)\n",
        "\n",
        "        # Change to log base 2\n",
        "        # log2(x) = ln(x) / ln(2)\n",
        "        loss /= torch.log(torch.tensor(2))\n",
        "\n",
        "        sumneglogp += loss.item()\n",
        "\n",
        "    # sumneglogp is -log(p('.' + name1)) -log(p('.' + name2)) -log(p('.' + name3)) ...\n",
        "    # Divide by the appropriate term to get the answer we want\n",
        "    print('Preplexity: ', torch.pow(2, torch.tensor(sumneglogp / T)).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "id": "7608536d",
      "metadata": {
        "id": "7608536d",
        "outputId": "2d6d8bca-efdb-4fb2-db40-2e84f1741cf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated name:  zenyikn\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# Intialize the word with\n",
        "name = '.'\n",
        "# Initialize h to random\n",
        "h = torch.zeros(1, d_h)\n",
        "while True:\n",
        "    # Make c to an integer\n",
        "    c = torch.tensor([stoi[name[-1]]])\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    logits, h = model(c, h)\n",
        "    # Get p; use Softmax\n",
        "    p = torch.softmax(logits, dim=1)\n",
        "    # Sample from p\n",
        "    c = torch.multinomial(p, num_samples=1)\n",
        "    # If we generate '.', stop\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d790024",
      "metadata": {
        "id": "4d790024"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}