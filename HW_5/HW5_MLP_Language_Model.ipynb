{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "624b39df",
      "metadata": {
        "id": "624b39df"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import tqdm\n",
        "import math\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "939a704e",
      "metadata": {
        "id": "939a704e"
      },
      "outputs": [],
      "source": [
        "FILL_IN = \"FILL_IN\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "names = []\n",
        "names_path = '/content/drive/MyDrive/ADL_HW/names.txt'\n",
        "with open(names_path, 'r') as f:\n",
        "    names = f.read().splitlines()"
      ],
      "metadata": {
        "id": "ENGCaduhcDV4",
        "outputId": "2de51c24-1523-4260-835e-b870d7402aa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ENGCaduhcDV4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names"
      ],
      "metadata": {
        "id": "qJ79WW1Uc4Of",
        "outputId": "18601d7b-be9a-4089-e5b9-3058c21bffde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qJ79WW1Uc4Of",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn',\n",
              " 'abigail',\n",
              " 'emily',\n",
              " 'elizabeth',\n",
              " 'mila',\n",
              " 'ella',\n",
              " 'avery',\n",
              " 'sofia',\n",
              " 'camila',\n",
              " 'aria',\n",
              " 'scarlett',\n",
              " 'victoria',\n",
              " 'madison',\n",
              " 'luna',\n",
              " 'grace',\n",
              " 'chloe',\n",
              " 'penelope',\n",
              " 'layla',\n",
              " 'riley',\n",
              " 'zoey',\n",
              " 'nora',\n",
              " 'lily',\n",
              " 'eleanor',\n",
              " 'hannah',\n",
              " 'lillian',\n",
              " 'addison',\n",
              " 'aubrey',\n",
              " 'ellie',\n",
              " 'stella',\n",
              " 'natalie',\n",
              " 'zoe',\n",
              " 'leah',\n",
              " 'hazel',\n",
              " 'violet',\n",
              " 'aurora',\n",
              " 'savannah',\n",
              " 'audrey',\n",
              " 'brooklyn',\n",
              " 'bella',\n",
              " 'claire',\n",
              " 'skylar',\n",
              " 'lucy',\n",
              " 'paisley',\n",
              " 'everly',\n",
              " 'anna',\n",
              " 'caroline',\n",
              " 'nova',\n",
              " 'genesis',\n",
              " 'emilia',\n",
              " 'kennedy',\n",
              " 'samantha',\n",
              " 'maya',\n",
              " 'willow',\n",
              " 'kinsley',\n",
              " 'naomi',\n",
              " 'aaliyah',\n",
              " 'elena',\n",
              " 'sarah',\n",
              " 'ariana',\n",
              " 'allison',\n",
              " 'gabriella',\n",
              " 'alice',\n",
              " 'madelyn',\n",
              " 'cora',\n",
              " 'ruby',\n",
              " 'eva',\n",
              " 'serenity',\n",
              " 'autumn',\n",
              " 'adeline',\n",
              " 'hailey',\n",
              " 'gianna',\n",
              " 'valentina',\n",
              " 'isla',\n",
              " 'eliana',\n",
              " 'quinn',\n",
              " 'nevaeh',\n",
              " 'ivy',\n",
              " 'sadie',\n",
              " 'piper',\n",
              " 'lydia',\n",
              " 'alexa',\n",
              " 'josephine',\n",
              " 'emery',\n",
              " 'julia',\n",
              " 'delilah',\n",
              " 'arianna',\n",
              " 'vivian',\n",
              " 'kaylee',\n",
              " 'sophie',\n",
              " 'brielle',\n",
              " 'madeline',\n",
              " 'peyton',\n",
              " 'rylee',\n",
              " 'clara',\n",
              " 'hadley',\n",
              " 'melanie',\n",
              " 'mackenzie',\n",
              " 'reagan',\n",
              " 'adalynn',\n",
              " 'liliana',\n",
              " 'aubree',\n",
              " 'jade',\n",
              " 'katherine',\n",
              " 'isabelle',\n",
              " 'natalia',\n",
              " 'raelynn',\n",
              " 'maria',\n",
              " 'athena',\n",
              " 'ximena',\n",
              " 'arya',\n",
              " 'leilani',\n",
              " 'taylor',\n",
              " 'faith',\n",
              " 'rose',\n",
              " 'kylie',\n",
              " 'alexandra',\n",
              " 'mary',\n",
              " 'margaret',\n",
              " 'lyla',\n",
              " 'ashley',\n",
              " 'amaya',\n",
              " 'eliza',\n",
              " 'brianna',\n",
              " 'bailey',\n",
              " 'andrea',\n",
              " 'khloe',\n",
              " 'jasmine',\n",
              " 'melody',\n",
              " 'iris',\n",
              " 'isabel',\n",
              " 'norah',\n",
              " 'annabelle',\n",
              " 'valeria',\n",
              " 'emerson',\n",
              " 'adalyn',\n",
              " 'ryleigh',\n",
              " 'eden',\n",
              " 'emersyn',\n",
              " 'anastasia',\n",
              " 'kayla',\n",
              " 'alyssa',\n",
              " 'juliana',\n",
              " 'charlie',\n",
              " 'esther',\n",
              " 'ariel',\n",
              " 'cecilia',\n",
              " 'valerie',\n",
              " 'alina',\n",
              " 'molly',\n",
              " 'reese',\n",
              " 'aliyah',\n",
              " 'lilly',\n",
              " 'parker',\n",
              " 'finley',\n",
              " 'morgan',\n",
              " 'sydney',\n",
              " 'jordyn',\n",
              " 'eloise',\n",
              " 'trinity',\n",
              " 'daisy',\n",
              " 'kimberly',\n",
              " 'lauren',\n",
              " 'genevieve',\n",
              " 'sara',\n",
              " 'arabella',\n",
              " 'harmony',\n",
              " 'elise',\n",
              " 'remi',\n",
              " 'teagan',\n",
              " 'alexis',\n",
              " 'london',\n",
              " 'sloane',\n",
              " 'laila',\n",
              " 'lucia',\n",
              " 'diana',\n",
              " 'juliette',\n",
              " 'sienna',\n",
              " 'elliana',\n",
              " 'londyn',\n",
              " 'ayla',\n",
              " 'callie',\n",
              " 'gracie',\n",
              " 'josie',\n",
              " 'amara',\n",
              " 'jocelyn',\n",
              " 'daniela',\n",
              " 'everleigh',\n",
              " 'mya',\n",
              " 'rachel',\n",
              " 'summer',\n",
              " 'alana',\n",
              " 'brooke',\n",
              " 'alaina',\n",
              " 'mckenzie',\n",
              " 'catherine',\n",
              " 'amy',\n",
              " 'presley',\n",
              " 'journee',\n",
              " 'rosalie',\n",
              " 'ember',\n",
              " 'brynlee',\n",
              " 'rowan',\n",
              " 'joanna',\n",
              " 'paige',\n",
              " 'rebecca',\n",
              " 'ana',\n",
              " 'sawyer',\n",
              " 'mariah',\n",
              " 'nicole',\n",
              " 'brooklynn',\n",
              " 'payton',\n",
              " 'marley',\n",
              " 'fiona',\n",
              " 'georgia',\n",
              " 'lila',\n",
              " 'harley',\n",
              " 'adelyn',\n",
              " 'alivia',\n",
              " 'noelle',\n",
              " 'gemma',\n",
              " 'vanessa',\n",
              " 'journey',\n",
              " 'makayla',\n",
              " 'angelina',\n",
              " 'adaline',\n",
              " 'catalina',\n",
              " 'alayna',\n",
              " 'julianna',\n",
              " 'leila',\n",
              " 'lola',\n",
              " 'adriana',\n",
              " 'june',\n",
              " 'juliet',\n",
              " 'jayla',\n",
              " 'river',\n",
              " 'tessa',\n",
              " 'lia',\n",
              " 'dakota',\n",
              " 'delaney',\n",
              " 'selena',\n",
              " 'blakely',\n",
              " 'ada',\n",
              " 'camille',\n",
              " 'zara',\n",
              " 'malia',\n",
              " 'hope',\n",
              " 'samara',\n",
              " 'vera',\n",
              " 'mckenna',\n",
              " 'briella',\n",
              " 'izabella',\n",
              " 'hayden',\n",
              " 'raegan',\n",
              " 'michelle',\n",
              " 'angela',\n",
              " 'ruth',\n",
              " 'freya',\n",
              " 'kamila',\n",
              " 'vivienne',\n",
              " 'aspen',\n",
              " 'olive',\n",
              " 'kendall',\n",
              " 'elaina',\n",
              " 'thea',\n",
              " 'kali',\n",
              " 'destiny',\n",
              " 'amiyah',\n",
              " 'evangeline',\n",
              " 'cali',\n",
              " 'blake',\n",
              " 'elsie',\n",
              " 'juniper',\n",
              " 'alexandria',\n",
              " 'myla',\n",
              " 'ariella',\n",
              " 'kate',\n",
              " 'mariana',\n",
              " 'lilah',\n",
              " 'charlee',\n",
              " 'daleyza',\n",
              " 'nyla',\n",
              " 'jane',\n",
              " 'maggie',\n",
              " 'zuri',\n",
              " 'aniyah',\n",
              " 'lucille',\n",
              " 'leia',\n",
              " 'melissa',\n",
              " 'adelaide',\n",
              " 'amina',\n",
              " 'giselle',\n",
              " 'lena',\n",
              " 'camilla',\n",
              " 'miriam',\n",
              " 'millie',\n",
              " 'brynn',\n",
              " 'gabrielle',\n",
              " 'sage',\n",
              " 'annie',\n",
              " 'logan',\n",
              " 'lilliana',\n",
              " 'haven',\n",
              " 'jessica',\n",
              " 'kaia',\n",
              " 'magnolia',\n",
              " 'amira',\n",
              " 'adelynn',\n",
              " 'makenzie',\n",
              " 'stephanie',\n",
              " 'nina',\n",
              " 'phoebe',\n",
              " 'arielle',\n",
              " 'evie',\n",
              " 'lyric',\n",
              " 'alessandra',\n",
              " 'gabriela',\n",
              " 'paislee',\n",
              " 'raelyn',\n",
              " 'madilyn',\n",
              " 'paris',\n",
              " 'makenna',\n",
              " 'kinley',\n",
              " 'gracelyn',\n",
              " 'talia',\n",
              " 'maeve',\n",
              " 'rylie',\n",
              " 'kiara',\n",
              " 'evelynn',\n",
              " 'brinley',\n",
              " 'jacqueline',\n",
              " 'laura',\n",
              " 'gracelynn',\n",
              " 'lexi',\n",
              " 'ariah',\n",
              " 'fatima',\n",
              " 'jennifer',\n",
              " 'kehlani',\n",
              " 'alani',\n",
              " 'ariyah',\n",
              " 'luciana',\n",
              " 'allie',\n",
              " 'heidi',\n",
              " 'maci',\n",
              " 'phoenix',\n",
              " 'felicity',\n",
              " 'joy',\n",
              " 'kenzie',\n",
              " 'veronica',\n",
              " 'margot',\n",
              " 'addilyn',\n",
              " 'lana',\n",
              " 'cassidy',\n",
              " 'remington',\n",
              " 'saylor',\n",
              " 'ryan',\n",
              " 'keira',\n",
              " 'harlow',\n",
              " 'miranda',\n",
              " 'angel',\n",
              " 'amanda',\n",
              " 'daniella',\n",
              " 'royalty',\n",
              " 'gwendolyn',\n",
              " 'ophelia',\n",
              " 'heaven',\n",
              " 'jordan',\n",
              " 'madeleine',\n",
              " 'esmeralda',\n",
              " 'kira',\n",
              " 'miracle',\n",
              " 'elle',\n",
              " 'amari',\n",
              " 'danielle',\n",
              " 'daphne',\n",
              " 'willa',\n",
              " 'haley',\n",
              " 'gia',\n",
              " 'kaitlyn',\n",
              " 'oakley',\n",
              " 'kailani',\n",
              " 'winter',\n",
              " 'alicia',\n",
              " 'serena',\n",
              " 'nadia',\n",
              " 'aviana',\n",
              " 'demi',\n",
              " 'jada',\n",
              " 'braelynn',\n",
              " 'dylan',\n",
              " 'ainsley',\n",
              " 'alison',\n",
              " 'camryn',\n",
              " 'avianna',\n",
              " 'bianca',\n",
              " 'skyler',\n",
              " 'scarlet',\n",
              " 'maddison',\n",
              " 'nylah',\n",
              " 'sarai',\n",
              " 'regina',\n",
              " 'dahlia',\n",
              " 'nayeli',\n",
              " 'raven',\n",
              " 'helen',\n",
              " 'adrianna',\n",
              " 'averie',\n",
              " 'skye',\n",
              " 'kelsey',\n",
              " 'tatum',\n",
              " 'kensley',\n",
              " 'maliyah',\n",
              " 'erin',\n",
              " 'viviana',\n",
              " 'jenna',\n",
              " 'anaya',\n",
              " 'carolina',\n",
              " 'shelby',\n",
              " 'sabrina',\n",
              " 'mikayla',\n",
              " 'annalise',\n",
              " 'octavia',\n",
              " 'lennon',\n",
              " 'blair',\n",
              " 'carmen',\n",
              " 'yaretzi',\n",
              " 'kennedi',\n",
              " 'mabel',\n",
              " 'zariah',\n",
              " 'kyla',\n",
              " 'christina',\n",
              " 'selah',\n",
              " 'celeste',\n",
              " 'eve',\n",
              " 'mckinley',\n",
              " 'milani',\n",
              " 'frances',\n",
              " 'jimena',\n",
              " 'kylee',\n",
              " 'leighton',\n",
              " 'katie',\n",
              " 'aitana',\n",
              " 'kayleigh',\n",
              " 'sierra',\n",
              " 'kathryn',\n",
              " 'rosemary',\n",
              " 'jolene',\n",
              " 'alondra',\n",
              " 'elisa',\n",
              " 'helena',\n",
              " 'charleigh',\n",
              " 'hallie',\n",
              " 'lainey',\n",
              " 'avah',\n",
              " 'jazlyn',\n",
              " 'kamryn',\n",
              " 'mira',\n",
              " 'cheyenne',\n",
              " 'francesca',\n",
              " 'antonella',\n",
              " 'wren',\n",
              " 'chelsea',\n",
              " 'amber',\n",
              " 'emory',\n",
              " 'lorelei',\n",
              " 'nia',\n",
              " 'abby',\n",
              " 'april',\n",
              " 'emelia',\n",
              " 'carter',\n",
              " 'aylin',\n",
              " 'cataleya',\n",
              " 'bethany',\n",
              " 'marlee',\n",
              " 'carly',\n",
              " 'kaylani',\n",
              " 'emely',\n",
              " 'liana',\n",
              " 'madelynn',\n",
              " 'cadence',\n",
              " 'matilda',\n",
              " 'sylvia',\n",
              " 'myra',\n",
              " 'fernanda',\n",
              " 'oaklyn',\n",
              " 'elianna',\n",
              " 'hattie',\n",
              " 'dayana',\n",
              " 'kendra',\n",
              " 'maisie',\n",
              " 'malaysia',\n",
              " 'kara',\n",
              " 'katelyn',\n",
              " 'maia',\n",
              " 'celine',\n",
              " 'cameron',\n",
              " 'renata',\n",
              " 'jayleen',\n",
              " 'charli',\n",
              " 'emmalyn',\n",
              " 'holly',\n",
              " 'azalea',\n",
              " 'leona',\n",
              " 'alejandra',\n",
              " 'bristol',\n",
              " 'collins',\n",
              " 'imani',\n",
              " 'meadow',\n",
              " 'alexia',\n",
              " 'edith',\n",
              " 'kaydence',\n",
              " 'leslie',\n",
              " 'lilith',\n",
              " 'kora',\n",
              " 'aisha',\n",
              " 'meredith',\n",
              " 'danna',\n",
              " 'wynter',\n",
              " 'emberly',\n",
              " 'julieta',\n",
              " 'michaela',\n",
              " 'alayah',\n",
              " 'jemma',\n",
              " 'reign',\n",
              " 'colette',\n",
              " 'kaliyah',\n",
              " 'elliott',\n",
              " 'johanna',\n",
              " 'remy',\n",
              " 'sutton',\n",
              " 'emmy',\n",
              " 'virginia',\n",
              " 'briana',\n",
              " 'oaklynn',\n",
              " 'adelina',\n",
              " 'everlee',\n",
              " 'megan',\n",
              " 'angelica',\n",
              " 'justice',\n",
              " 'mariam',\n",
              " 'khaleesi',\n",
              " 'macie',\n",
              " 'karsyn',\n",
              " 'alanna',\n",
              " 'aleah',\n",
              " 'mae',\n",
              " 'mallory',\n",
              " 'esme',\n",
              " 'skyla',\n",
              " 'madilynn',\n",
              " 'charley',\n",
              " 'allyson',\n",
              " 'hanna',\n",
              " 'shiloh',\n",
              " 'henley',\n",
              " 'macy',\n",
              " 'maryam',\n",
              " 'ivanna',\n",
              " 'ashlynn',\n",
              " 'lorelai',\n",
              " 'amora',\n",
              " 'ashlyn',\n",
              " 'sasha',\n",
              " 'baylee',\n",
              " 'beatrice',\n",
              " 'itzel',\n",
              " 'priscilla',\n",
              " 'marie',\n",
              " 'jayda',\n",
              " 'liberty',\n",
              " 'rory',\n",
              " 'alessia',\n",
              " 'alaia',\n",
              " 'janelle',\n",
              " 'kalani',\n",
              " 'gloria',\n",
              " 'sloan',\n",
              " 'dorothy',\n",
              " 'greta',\n",
              " 'julie',\n",
              " 'zahra',\n",
              " 'savanna',\n",
              " 'annabella',\n",
              " 'poppy',\n",
              " 'amalia',\n",
              " 'zaylee',\n",
              " 'cecelia',\n",
              " 'coraline',\n",
              " 'kimber',\n",
              " 'emmie',\n",
              " 'anne',\n",
              " 'karina',\n",
              " 'kassidy',\n",
              " 'kynlee',\n",
              " 'monroe',\n",
              " 'anahi',\n",
              " 'jaliyah',\n",
              " 'jazmin',\n",
              " 'maren',\n",
              " 'monica',\n",
              " 'siena',\n",
              " 'marilyn',\n",
              " 'reyna',\n",
              " 'kyra',\n",
              " 'lilian',\n",
              " 'jamie',\n",
              " 'melany',\n",
              " 'alaya',\n",
              " 'ariya',\n",
              " 'kelly',\n",
              " 'rosie',\n",
              " 'adley',\n",
              " 'dream',\n",
              " 'jaylah',\n",
              " 'laurel',\n",
              " 'jazmine',\n",
              " 'mina',\n",
              " 'karla',\n",
              " 'bailee',\n",
              " 'aubrie',\n",
              " 'katalina',\n",
              " 'melina',\n",
              " 'harlee',\n",
              " 'elliot',\n",
              " 'hayley',\n",
              " 'elaine',\n",
              " 'karen',\n",
              " 'dallas',\n",
              " 'irene',\n",
              " 'lylah',\n",
              " 'ivory',\n",
              " 'chaya',\n",
              " 'rosa',\n",
              " 'aleena',\n",
              " 'braelyn',\n",
              " 'nola',\n",
              " 'alma',\n",
              " 'leyla',\n",
              " 'pearl',\n",
              " 'addyson',\n",
              " 'roselyn',\n",
              " 'lacey',\n",
              " 'lennox',\n",
              " 'reina',\n",
              " 'aurelia',\n",
              " 'noa',\n",
              " 'janiyah',\n",
              " 'jessie',\n",
              " 'madisyn',\n",
              " 'saige',\n",
              " 'alia',\n",
              " 'tiana',\n",
              " 'astrid',\n",
              " 'cassandra',\n",
              " 'kyleigh',\n",
              " 'romina',\n",
              " 'stevie',\n",
              " 'haylee',\n",
              " 'zelda',\n",
              " 'lillie',\n",
              " 'aileen',\n",
              " 'brylee',\n",
              " 'eileen',\n",
              " 'yara',\n",
              " 'ensley',\n",
              " 'lauryn',\n",
              " 'giuliana',\n",
              " 'livia',\n",
              " 'anya',\n",
              " 'mikaela',\n",
              " 'palmer',\n",
              " 'lyra',\n",
              " 'mara',\n",
              " 'marina',\n",
              " 'kailey',\n",
              " 'liv',\n",
              " 'clementine',\n",
              " 'kenna',\n",
              " 'briar',\n",
              " 'emerie',\n",
              " 'galilea',\n",
              " 'tiffany',\n",
              " 'bonnie',\n",
              " 'elyse',\n",
              " 'cynthia',\n",
              " 'frida',\n",
              " 'kinslee',\n",
              " 'tatiana',\n",
              " 'joelle',\n",
              " 'armani',\n",
              " 'jolie',\n",
              " 'nalani',\n",
              " 'rayna',\n",
              " 'yareli',\n",
              " 'meghan',\n",
              " 'rebekah',\n",
              " 'addilynn',\n",
              " 'faye',\n",
              " 'zariyah',\n",
              " 'lea',\n",
              " 'aliza',\n",
              " 'julissa',\n",
              " 'lilyana',\n",
              " 'anika',\n",
              " 'kairi',\n",
              " 'aniya',\n",
              " 'noemi',\n",
              " 'angie',\n",
              " 'crystal',\n",
              " 'bridget',\n",
              " 'ari',\n",
              " 'davina',\n",
              " 'amelie',\n",
              " 'amirah',\n",
              " 'annika',\n",
              " 'elora',\n",
              " 'xiomara',\n",
              " 'linda',\n",
              " 'hana',\n",
              " 'laney',\n",
              " 'mercy',\n",
              " 'hadassah',\n",
              " 'madalyn',\n",
              " 'louisa',\n",
              " 'simone',\n",
              " 'kori',\n",
              " 'jillian',\n",
              " 'alena',\n",
              " 'malaya',\n",
              " 'miley',\n",
              " 'milan',\n",
              " 'sariyah',\n",
              " 'malani',\n",
              " 'clarissa',\n",
              " 'nala',\n",
              " 'princess',\n",
              " 'amani',\n",
              " 'analia',\n",
              " 'estella',\n",
              " 'milana',\n",
              " 'aya',\n",
              " 'chana',\n",
              " 'jayde',\n",
              " 'tenley',\n",
              " 'zaria',\n",
              " 'itzayana',\n",
              " 'penny',\n",
              " 'ailani',\n",
              " 'lara',\n",
              " 'aubriella',\n",
              " 'clare',\n",
              " 'lina',\n",
              " 'rhea',\n",
              " 'bria',\n",
              " 'thalia',\n",
              " 'keyla',\n",
              " 'haisley',\n",
              " 'ryann',\n",
              " 'addisyn',\n",
              " 'amaia',\n",
              " 'chanel',\n",
              " 'ellen',\n",
              " 'harmoni',\n",
              " 'aliana',\n",
              " 'tinsley',\n",
              " 'landry',\n",
              " 'paisleigh',\n",
              " 'lexie',\n",
              " 'myah',\n",
              " 'rylan',\n",
              " 'deborah',\n",
              " 'emilee',\n",
              " 'laylah',\n",
              " 'novalee',\n",
              " 'ellis',\n",
              " 'emmeline',\n",
              " 'avalynn',\n",
              " 'hadlee',\n",
              " 'legacy',\n",
              " 'braylee',\n",
              " 'elisabeth',\n",
              " 'kaylie',\n",
              " 'ansley',\n",
              " 'dior',\n",
              " 'paula',\n",
              " 'belen',\n",
              " 'corinne',\n",
              " 'maleah',\n",
              " 'martha',\n",
              " 'teresa',\n",
              " 'salma',\n",
              " 'louise',\n",
              " 'averi',\n",
              " 'lilianna',\n",
              " 'amiya',\n",
              " 'milena',\n",
              " 'royal',\n",
              " 'aubrielle',\n",
              " 'calliope',\n",
              " 'frankie',\n",
              " 'natasha',\n",
              " 'kamilah',\n",
              " 'meilani',\n",
              " 'raina',\n",
              " 'amayah',\n",
              " 'lailah',\n",
              " 'rayne',\n",
              " 'zaniyah',\n",
              " 'isabela',\n",
              " 'nathalie',\n",
              " 'miah',\n",
              " 'opal',\n",
              " 'kenia',\n",
              " 'azariah',\n",
              " 'hunter',\n",
              " 'tori',\n",
              " 'andi',\n",
              " 'keily',\n",
              " 'leanna',\n",
              " 'scarlette',\n",
              " 'jaelyn',\n",
              " 'saoirse',\n",
              " 'selene',\n",
              " 'dalary',\n",
              " 'lindsey',\n",
              " 'marianna',\n",
              " 'ramona',\n",
              " 'estelle',\n",
              " 'giovanna',\n",
              " 'holland',\n",
              " 'nancy',\n",
              " 'emmalynn',\n",
              " 'mylah',\n",
              " 'rosalee',\n",
              " 'sariah',\n",
              " 'zoie',\n",
              " 'blaire',\n",
              " 'lyanna',\n",
              " 'maxine',\n",
              " 'anais',\n",
              " 'dana',\n",
              " 'judith',\n",
              " 'kiera',\n",
              " 'jaelynn',\n",
              " 'noor',\n",
              " 'kai',\n",
              " 'adalee',\n",
              " 'oaklee',\n",
              " 'amaris',\n",
              " 'jaycee',\n",
              " 'belle',\n",
              " 'carolyn',\n",
              " 'della',\n",
              " 'karter',\n",
              " 'sky',\n",
              " 'treasure',\n",
              " 'vienna',\n",
              " 'jewel',\n",
              " 'rivka',\n",
              " 'rosalyn',\n",
              " 'alannah',\n",
              " 'ellianna',\n",
              " 'sunny',\n",
              " 'claudia',\n",
              " 'cara',\n",
              " 'hailee',\n",
              " 'estrella',\n",
              " 'harleigh',\n",
              " 'zhavia',\n",
              " 'alianna',\n",
              " 'brittany',\n",
              " 'jaylene',\n",
              " 'journi',\n",
              " 'marissa',\n",
              " 'mavis',\n",
              " 'iliana',\n",
              " 'jurnee',\n",
              " 'aislinn',\n",
              " 'alyson',\n",
              " 'elsa',\n",
              " 'kamiyah',\n",
              " 'kiana',\n",
              " 'lisa',\n",
              " 'arlette',\n",
              " 'kadence',\n",
              " 'kathleen',\n",
              " 'halle',\n",
              " 'erika',\n",
              " 'sylvie',\n",
              " 'adele',\n",
              " 'erica',\n",
              " 'veda',\n",
              " 'whitney',\n",
              " 'bexley',\n",
              " 'emmaline',\n",
              " 'guadalupe',\n",
              " 'august',\n",
              " 'brynleigh',\n",
              " 'gwen',\n",
              " 'promise',\n",
              " 'alisson',\n",
              " 'india',\n",
              " 'madalynn',\n",
              " 'paloma',\n",
              " 'patricia',\n",
              " 'samira',\n",
              " 'aliya',\n",
              " 'casey',\n",
              " 'jazlynn',\n",
              " 'paulina',\n",
              " 'dulce',\n",
              " 'kallie',\n",
              " 'perla',\n",
              " 'adrienne',\n",
              " 'alora',\n",
              " 'nataly',\n",
              " 'ayleen',\n",
              " 'christine',\n",
              " 'kaiya',\n",
              " 'ariadne',\n",
              " 'karlee',\n",
              " 'barbara',\n",
              " 'lillianna',\n",
              " 'raquel',\n",
              " 'saniyah',\n",
              " 'yamileth',\n",
              " 'arely',\n",
              " 'celia',\n",
              " 'heavenly',\n",
              " 'kaylin',\n",
              " 'marisol',\n",
              " 'marleigh',\n",
              " 'avalyn',\n",
              " 'berkley',\n",
              " 'kataleya',\n",
              " 'zainab',\n",
              " 'dani',\n",
              " 'egypt',\n",
              " 'joyce',\n",
              " 'kenley',\n",
              " 'annabel',\n",
              " 'kaelyn',\n",
              " 'etta',\n",
              " 'hadleigh',\n",
              " 'joselyn',\n",
              " 'luella',\n",
              " 'jaylee',\n",
              " 'zola',\n",
              " 'alisha',\n",
              " 'ezra',\n",
              " 'queen',\n",
              " 'amia',\n",
              " 'annalee',\n",
              " 'bellamy',\n",
              " 'paola',\n",
              " 'tinley',\n",
              " 'violeta',\n",
              " 'jenesis',\n",
              " 'arden',\n",
              " 'giana',\n",
              " 'wendy',\n",
              " 'ellison',\n",
              " 'florence',\n",
              " 'margo',\n",
              " 'naya',\n",
              " 'robin',\n",
              " 'sandra',\n",
              " 'scout',\n",
              " 'waverly',\n",
              " 'janessa',\n",
              " 'jayden',\n",
              " 'micah',\n",
              " 'novah',\n",
              " 'zora',\n",
              " 'ann',\n",
              " 'jana',\n",
              " 'taliyah',\n",
              " 'vada',\n",
              " 'giavanna',\n",
              " 'ingrid',\n",
              " 'valery',\n",
              " 'azaria',\n",
              " 'emmarie',\n",
              " 'esperanza',\n",
              " 'kailyn',\n",
              " 'aiyana',\n",
              " 'keilani',\n",
              " 'austyn',\n",
              " 'whitley',\n",
              " 'elina',\n",
              " 'kimora',\n",
              " 'maliah',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fa935c21",
      "metadata": {
        "id": "fa935c21"
      },
      "outputs": [],
      "source": [
        "# Dictionaries, {idx -> ch} and {ch -> idx}\n",
        "itos = defaultdict(int)\n",
        "stoi = defaultdict(int)\n",
        "# Number of characters used to predict the target character in the MLP Language Model\n",
        "block_size = 3\n",
        "# Batch size used in MLP Language Model\n",
        "batch_size = 32\n",
        "# Embedding dimension, per character\n",
        "d_model = 10\n",
        "# Hidden dimension for RNN and also MLP Language Models\n",
        "d_h = 200\n",
        "\n",
        "# START = START token\n",
        "stoi['.'] = 0\n",
        "itos[0] = '.'\n",
        "\n",
        "count = 1\n",
        "# Loop over all names and create mappings itos and stoi mapping a unique character to a idx\n",
        "for name in names:\n",
        "  for ch in name:\n",
        "    if not ch in stoi:\n",
        "      stoi[ch] = count\n",
        "      itos[count] = ch\n",
        "      count = count + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d556793e",
      "metadata": {
        "id": "d556793e"
      },
      "outputs": [],
      "source": [
        "assert len(stoi) == len(itos)\n",
        "vocab_size = len(stoi)\n",
        "assert vocab_size == 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3765e0fe",
      "metadata": {
        "id": "3765e0fe",
        "outputId": "1ecdb620-507c-4756-8378-51b4fce2a402",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'.': 0,\n",
              "             'e': 1,\n",
              "             'm': 2,\n",
              "             'a': 3,\n",
              "             'o': 4,\n",
              "             'l': 5,\n",
              "             'i': 6,\n",
              "             'v': 7,\n",
              "             's': 8,\n",
              "             'b': 9,\n",
              "             'p': 10,\n",
              "             'h': 11,\n",
              "             'c': 12,\n",
              "             'r': 13,\n",
              "             't': 14,\n",
              "             'y': 15,\n",
              "             'n': 16,\n",
              "             'g': 17,\n",
              "             'z': 18,\n",
              "             'f': 19,\n",
              "             'd': 20,\n",
              "             'u': 21,\n",
              "             'k': 22,\n",
              "             'w': 23,\n",
              "             'q': 24,\n",
              "             'x': 25,\n",
              "             'j': 26})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "stoi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d706797e",
      "metadata": {
        "id": "d706797e"
      },
      "source": [
        "## BiGram Language Model\n",
        "- Implement the Bigram Language Model\n",
        "- Get all the relevent counts, then get the train dataset Perplexity\n",
        "- Use the class notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9cb554a5",
      "metadata": {
        "id": "9cb554a5"
      },
      "outputs": [],
      "source": [
        "# Using the formulas in class, loop over each name and get the parameters\n",
        "c1 = defaultdict(int)\n",
        "c2 = defaultdict(int)\n",
        "c2_sum = defaultdict(int)\n",
        "for name in open(names_path, 'r'):\n",
        "    # Lowercase and remove any whitespace at the end\n",
        "    name = name.lower().strip()\n",
        "    # Pad with START = '.' and STOP = '.'\n",
        "    name = '.'+name+'.'\n",
        "    # Transform to integer\n",
        "    name = [stoi[ch] for ch in name]\n",
        "    # Get the counts for Bigrams and Unigrams\n",
        "    for i in range(len(name)):\n",
        "      c1[name[i]] += 1\n",
        "\n",
        "      if i < len(name) - 1:\n",
        "        bigram = (name[i], name[i + 1])\n",
        "        c2[bigram] += 1\n",
        "        c2_sum[name[i]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "507e4525",
      "metadata": {
        "id": "507e4525",
        "outputId": "c31dbfe3-0576-4bf5-85ef-2230eef97c0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preplexity:  6.484745025634766\n"
          ]
        }
      ],
      "source": [
        "# Get perplexity\n",
        "\n",
        "sumneglogp = 0\n",
        "T = 0\n",
        "for name in open(names_path, 'r'):\n",
        "    # Get rid of white space and lowercase\n",
        "    name = name.lower().strip()\n",
        "    # Get the length of the word, without padding\n",
        "    T += len(name)\n",
        "    # Don't pad the STOP since we are not generating; pad with START\n",
        "    name = '.' + name\n",
        "    # Transform to integrs\n",
        "    name = [stoi[ch] for ch in name]\n",
        "    # Get the loss -log(p(name)); use that the log of the product is the sum of the logs\n",
        "    for i in range(len(name) - 1):\n",
        "      x, y = name[i], name[i + 1]\n",
        "      p = c2[(x, y)] / c1[x]\n",
        "\n",
        "      sumneglogp += -math.log(p)\n",
        "# Print the Perplexity\n",
        "print('Preplexity: ', torch.pow(2.0, torch.tensor(sumneglogp / T)).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b931b726",
      "metadata": {
        "id": "b931b726",
        "outputId": "1d7d668e-d58c-4830-ed44-4bf5f43c9e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated name:  mujueldeyn\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# When you generate STOP, terminate\n",
        "name = '.'\n",
        "while True:\n",
        "    c = stoi[name[-1]]\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    p = []\n",
        "    for d in range(vocab_size):\n",
        "        # Use the same indicies as the dictionary we set up\n",
        "        # Populate p\n",
        "        p.append(c2[(c,d)]/c1[c])\n",
        "    #print(p)\n",
        "    assert len(p) == vocab_size\n",
        "    # Sample randmly from the probability using torch.Categorical\n",
        "    c = torch.distributions.Categorical(torch.tensor(p)).sample()\n",
        "    # Offset by 1 since we want indices [1, 2, ..., vocab_size]\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[1:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561cdacb",
      "metadata": {
        "id": "561cdacb"
      },
      "source": [
        "## MLP Language Model\n",
        "\n",
        "- Implement the MLP language model from below\n",
        "- Look at page 7, Equation (1)\n",
        "- https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982566bf",
      "metadata": {
        "id": "982566bf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fb6328a5",
      "metadata": {
        "id": "fb6328a5"
      },
      "outputs": [],
      "source": [
        "x_data = []\n",
        "y_data = []\n",
        "for name in open(names_path, 'r'):\n",
        "    name = name.lower().strip()\n",
        "    # Pad with block_size START tokens and 1 STOP token\n",
        "    name = ''.join(block_size * ['.']) + name + '.'\n",
        "    # Loop through name and get the (x, y) pairs\n",
        "    # Add (x, y) to x_data and y_data and make sure you transform to characters\n",
        "    # Make sure x_data and y_data have integers, use stoi\n",
        "    for i in range(len(name) - block_size):\n",
        "      x = name[i: i + block_size]\n",
        "      y = name[i + block_size]\n",
        "      x_data.append([stoi[ch] for ch in x])\n",
        "      y_data.append(stoi[y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3142dd13",
      "metadata": {
        "id": "3142dd13"
      },
      "outputs": [],
      "source": [
        "class MLPLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # An embedding for each character; vocab_size of them\n",
        "        self.e = nn.Embedding(vocab_size, d_model)\n",
        "        # H; should take in block_size * d_model vector and output d_h\n",
        "        self.H = nn.Linear(block_size * d_model, d_h)\n",
        "        # U; should take in d_h vector and output vocab_size\n",
        "        self.U = nn.Linear(d_h, vocab_size)\n",
        "        # W; for the skip connection, should take in block_size * d_model and output vocab_size\n",
        "        self.W = nn.Linear(block_size * d_model, vocab_size)\n",
        "\n",
        "    # x should be (batch_size, block_size)\n",
        "    def forward(self, x):\n",
        "        x = self.e(x)\n",
        "        x_flat = x.view(x.shape[0], -1)\n",
        "        hid = torch.relu(self.H(x_flat))\n",
        "        logits = self.U(hid) + self.W(x_flat)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4a4f35f7",
      "metadata": {
        "id": "4a4f35f7",
        "outputId": "962311f7-aad8-449b-9968-e1a92b665f03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 0, 0], 1)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "x_data[0], y_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "_qfL4H_Bvaxh",
        "outputId": "5dc637a9-1ff8-4e4e-e30b-014f04082b24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_qfL4H_Bvaxh",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "17bf7a0a",
      "metadata": {
        "id": "17bf7a0a"
      },
      "outputs": [],
      "source": [
        "# Define a dataloader with x_data and y_data with batch_size\n",
        "dl = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(\n",
        "        torch.tensor(x_data, dtype=torch.long),\n",
        "        torch.tensor(y_data, dtype=torch.long)\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a56ce82f",
      "metadata": {
        "id": "a56ce82f"
      },
      "outputs": [],
      "source": [
        "for xb, yb in dl:\n",
        "    assert xb.shape == (batch_size, 3)\n",
        "    assert yb.shape == (batch_size,)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTnVktytvRX4"
      },
      "id": "nTnVktytvRX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1013903d",
      "metadata": {
        "id": "1013903d"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "# Define the MLP model and the Adam optimizer learning rate 0.001\n",
        "model = MLPLanguageModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0155215c",
      "metadata": {
        "id": "0155215c"
      },
      "outputs": [],
      "source": [
        "total_loss = 0\n",
        "total_ct = 0\n",
        "total_epochs = 20\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "for _ in range(total_epochs):\n",
        "    for xb, yb in dl:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the logits\n",
        "        logits = model(xb)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(logits, yb)\n",
        "\n",
        "        # Get the new gradient\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to max norm 0.1\n",
        "        # Use clid_grad_norm from torch\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "\n",
        "        # Do a gradient update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the loss for the batch and get the number of batches\n",
        "        total_loss += loss.item()\n",
        "        total_ct += 1\n",
        "\n",
        "        # Print the loss\n",
        "        if total_ct and total_ct % 500 == 0:\n",
        "            print(total_loss / total_ct)\n",
        "            total_loss = 0\n",
        "            total_ct = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ecf5fb3",
      "metadata": {
        "id": "4ecf5fb3"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Get perplexity\n",
        "    sumneglogp = 0\n",
        "    T = 0\n",
        "    for name in open(names_path, 'r'):\n",
        "        name = name.lower().strip()\n",
        "        T += len(name)\n",
        "        # Pad with block_size START tokens\n",
        "        name = ''.join(['.'] * block_size) + name\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        # Gather all the terms over the loss\n",
        "        # Notice that we compute -log p(...abc)\n",
        "        # Which is -log p(a | ...) - log p(b | a..) - log p(c | ba.)\n",
        "        for i in range(len(name) - block_size):\n",
        "            x = name[i:i+block_size]\n",
        "            y = name[i+block_size]\n",
        "            x_data.append([stoi[ch] for ch in x])\n",
        "            y_data.append(stoi[y])\n",
        "        # Gather the loss over the name, for each term\n",
        "        # You need to get the softmax loss for each term\n",
        "        # Can either use the CrossEntropyLoss or do this manually\n",
        "        # Compute the loss\n",
        "        xb = torch.tensor(x_data, dtype=torch.long).to(device)\n",
        "        yb = torch.tensor(y_data, dtype=torch.long).to(device)\n",
        "        logits = model(xb)\n",
        "\n",
        "        # Use reduction \"sum\" so you don't need to worry about N\n",
        "        loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")(logits, yb)\n",
        "\n",
        "        # Change to log base 2\n",
        "        loss *= (1.0 / torch.log(torch.tensor(2.0)))\n",
        "\n",
        "        sumneglogp += loss\n",
        "\n",
        "    print('Preplexity: ', torch.pow(2.0, sumneglogp.clone().detach() / T).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77131c8",
      "metadata": {
        "id": "e77131c8"
      },
      "outputs": [],
      "source": [
        "# Generate a random word using this distributon\n",
        "# When you generate STOP, terminate\n",
        "name = ''.join(block_size * ['.'])\n",
        "while True:\n",
        "    # Get the idx\n",
        "    c = torch.tensor([stoi[ch] for ch in name[-block_size:]], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    logits = model(c)\n",
        "    p = torch.softmax(logits, dim=1)\n",
        "    # Randomly sample from p a new character\n",
        "    c = torch.distributions.Categorical(p).sample()\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[block_size:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1b01ed",
      "metadata": {
        "id": "bb1b01ed"
      },
      "source": [
        "## RNN Language Model\n",
        "- For each name, run an RNN character by character\n",
        "- Use the recursion x = Tanh()(Wh @ h + Wx @ x + bh + bx) and y = Softmax()(Wy h + by)\n",
        "- Do not use the RNN Cell from PyTorch, do this manually as hinted below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c765e7fc",
      "metadata": {
        "id": "c765e7fc"
      },
      "outputs": [],
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Each token has an embedding of size vocab_size\n",
        "        self.e = nn.Embedding(vocab_size, d_model)\n",
        "        # Wh used to map hidden to hidden\n",
        "        self.Wh = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.Wx = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.Wy = nn.Linear(d_model, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Run through to get the embedding for the token\n",
        "        # The embedding per token is the feature vector x  we pass into the\n",
        "        # Represent x as an embedding\n",
        "        x = self.e(x)\n",
        "        # Get the hidden state\n",
        "        h = torch.tanh(self.Wx(x) + self.Wh(h))\n",
        "        # Get the logits we use to predict y\n",
        "        z = self.Wy(h)\n",
        "        # Return the z predicting y for the timestep we are at and the next hidden state\n",
        "        return z, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "cd798358",
      "metadata": {
        "id": "cd798358"
      },
      "outputs": [],
      "source": [
        "model = RNNLanguageModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7f3a49ff",
      "metadata": {
        "id": "7f3a49ff",
        "outputId": "ad77ef8d-5d2c-4544-ec7e-04408607e115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.403815312385559\n",
            "3.39144540309906\n",
            "3.370520541667938\n",
            "3.3381054639816283\n",
            "3.3522675371170045\n",
            "3.3422869634628296\n",
            "3.3254690861701963\n",
            "3.3027714705467224\n",
            "3.303741476535797\n",
            "3.3092859864234923\n",
            "3.281665642261505\n",
            "3.2837720251083375\n",
            "3.2938400173187254\n",
            "3.2847199559211733\n",
            "3.262160098552704\n",
            "3.252235553264618\n",
            "3.239172296524048\n",
            "3.226921045780182\n",
            "3.217320775985718\n",
            "3.211389660835266\n",
            "3.215096845626831\n",
            "3.183451974391937\n",
            "3.1514693546295165\n",
            "3.166835379600525\n",
            "3.1571210289001463\n",
            "3.1387170219421385\n",
            "3.088104884624481\n",
            "3.0608856511116027\n",
            "3.0885117864608764\n",
            "3.042241072654724\n",
            "2.9829265785217287\n",
            "2.994392685890198\n",
            "2.9893229746818544\n",
            "2.9392813444137573\n",
            "2.932671754360199\n",
            "2.854076840877533\n",
            "2.867024531364441\n",
            "2.8692841863632204\n",
            "2.8367901420593262\n",
            "2.7989142179489135\n",
            "2.8062514567375185\n",
            "2.823779683113098\n",
            "2.764306285381317\n",
            "2.7770681285858156\n",
            "2.7531023693084715\n",
            "2.7305845618247986\n",
            "2.7070208859443663\n",
            "2.7169364404678347\n",
            "2.6647800254821776\n",
            "2.696675441265106\n",
            "2.693651967048645\n",
            "2.6939841079711915\n",
            "2.672834002971649\n",
            "2.697805895805359\n",
            "2.697368657588959\n",
            "2.692535378932953\n",
            "2.5927897477149964\n",
            "2.6760131978988646\n",
            "2.602772259712219\n",
            "2.6535552954673767\n",
            "2.690196919441223\n",
            "2.678406698703766\n",
            "2.648210186958313\n",
            "2.601042734384537\n",
            "2.6890191054344177\n",
            "2.6682273745536804\n",
            "2.6404688024520873\n",
            "2.6254007911682127\n",
            "2.614452381134033\n",
            "2.6205083644390106\n",
            "2.6244179248809814\n",
            "2.611934468746185\n",
            "2.5701481103897095\n",
            "2.6078038907051084\n",
            "2.620060374736786\n",
            "2.527840747833252\n",
            "2.6662663340568544\n",
            "2.6560336995124816\n",
            "2.5564832353591918\n",
            "2.5761806416511535\n",
            "2.651250813007355\n",
            "2.5896716952323913\n",
            "2.608751018047333\n",
            "2.5308114421367645\n",
            "2.637490680217743\n",
            "2.5805101227760314\n",
            "2.60490599155426\n",
            "2.6310394740104677\n",
            "2.511711931228638\n",
            "2.604847106933594\n",
            "2.6298621892929077\n",
            "2.528318991661072\n",
            "2.602210087776184\n",
            "2.522248579263687\n",
            "2.533191514015198\n",
            "2.6290122199058534\n",
            "2.5841556549072267\n",
            "2.4969211053848266\n",
            "2.546329698562622\n",
            "2.628808240890503\n",
            "2.5832332086563112\n",
            "2.4863330817222593\n",
            "2.580228054523468\n",
            "2.630889811515808\n",
            "2.6463917088508606\n",
            "2.508676664829254\n",
            "2.578086249828339\n",
            "2.6473373413085937\n",
            "2.4891510653495788\n",
            "2.430585172176361\n",
            "2.517330815792084\n",
            "2.641001965999603\n",
            "2.5697783362865447\n",
            "2.5500492846965788\n",
            "2.5055909371376037\n",
            "2.5682608342170714\n",
            "2.676836037635803\n",
            "2.575241551399231\n",
            "2.4127094066143036\n",
            "2.476260251998901\n",
            "2.531882504224777\n",
            "2.687101590633392\n",
            "2.5806482899188996\n",
            "2.559020986557007\n",
            "2.418988355398178\n",
            "2.4952922999858855\n",
            "2.585644918680191\n",
            "2.556969964504242\n",
            "2.5870417189598083\n",
            "2.4453953528404235\n",
            "2.346174919605255\n",
            "2.499888058900833\n",
            "2.49030464887619\n",
            "2.642185877561569\n",
            "2.512101126909256\n",
            "2.640792920589447\n",
            "2.6155848693847656\n",
            "2.444171177148819\n",
            "2.455127065181732\n",
            "2.517288637161255\n",
            "2.5751642966270447\n",
            "2.540592660903931\n",
            "2.51502494096756\n",
            "2.721752238273621\n",
            "2.415664710998535\n",
            "2.4112199449539187\n",
            "2.3708180522918703\n",
            "2.477976975440979\n",
            "2.5013757443428037\n",
            "2.4869113039970396\n",
            "2.5929175806045532\n",
            "2.6136009740829467\n",
            "2.48185911655426\n",
            "2.6090312004089355\n",
            "2.626621308326721\n",
            "2.4569120633602144\n",
            "2.411646522283554\n",
            "2.3766033017635344\n",
            "2.5316012465953825\n",
            "2.512789841890335\n",
            "2.5293939995765684\n",
            "2.4684595894813537\n",
            "2.4805573415756226\n",
            "2.5627212738990783\n",
            "2.561600773334503\n",
            "2.4049204063415526\n",
            "2.478268529176712\n",
            "2.2776908707618713\n",
            "2.383526645898819\n",
            "2.3784752178192137\n",
            "2.4964507102966307\n",
            "2.2817024040222167\n",
            "2.4867745530605316\n",
            "2.641739597320557\n",
            "2.6115119791030885\n",
            "2.5495958805084227\n",
            "2.5563030540943146\n",
            "2.5342200350761415\n",
            "2.5957137656211855\n",
            "2.493457899093628\n",
            "2.7022891902923583\n",
            "2.788892650604248\n",
            "2.705696382522583\n",
            "2.690459374189377\n",
            "2.693772476911545\n",
            "2.6816028678417205\n",
            "2.671038365364075\n",
            "2.6307551097869872\n",
            "2.643156896829605\n",
            "2.6784583950042724\n",
            "2.660603816509247\n",
            "2.621908038854599\n",
            "2.5921537578105927\n",
            "2.647037228345871\n",
            "2.6246338725090026\n",
            "2.603013333082199\n",
            "2.577386119365692\n",
            "2.6544447898864747\n",
            "2.5921379244327545\n",
            "2.6868436658382415\n",
            "2.59752019405365\n",
            "2.6252215468883513\n",
            "2.579758669137955\n",
            "2.6126845490932467\n",
            "2.619985250234604\n",
            "2.5665724122524263\n",
            "2.594270442724228\n",
            "2.5817589497566225\n",
            "2.546164735555649\n",
            "2.5279488945007325\n",
            "2.5786098754405975\n",
            "2.57340540766716\n",
            "2.5985424768924714\n",
            "2.5812810075283052\n",
            "2.596369651556015\n",
            "2.597754374742508\n",
            "2.639881051778793\n",
            "2.6034363150596618\n",
            "2.6392637622356414\n",
            "2.600839252471924\n",
            "2.652903206348419\n",
            "2.561871392726898\n",
            "2.5756398844718933\n",
            "2.6169543635845183\n",
            "2.6231193459033966\n",
            "2.564966505765915\n",
            "2.577747370004654\n",
            "2.670720238685608\n",
            "2.614624094963074\n",
            "2.5563880729675295\n",
            "2.6231894850730897\n",
            "2.555805071592331\n",
            "2.541642813682556\n",
            "2.578802081346512\n",
            "2.560748084783554\n",
            "2.6454908788204192\n",
            "2.5162874484062194\n",
            "2.6464330804347993\n",
            "2.5946671605110168\n",
            "2.52609801530838\n",
            "2.5686122703552248\n",
            "2.5319402015209196\n",
            "2.60549533367157\n",
            "2.5732092690467834\n",
            "2.622592383623123\n",
            "2.585469400882721\n",
            "2.6198452162742614\n",
            "2.54304190158844\n",
            "2.5238969326019287\n",
            "2.5406707167625426\n",
            "2.561949607133865\n",
            "2.5227690541744234\n",
            "2.6257621812820435\n",
            "2.358814921379089\n",
            "2.6618968796730043\n",
            "2.61012597322464\n",
            "2.5291286182403563\n",
            "2.616264021396637\n",
            "2.3517074048519135\n",
            "2.6520680022239684\n",
            "2.676252213716507\n",
            "2.6077050805091857\n",
            "2.5293789744377135\n",
            "2.638588796854019\n",
            "2.325363800525665\n",
            "2.546716510057449\n",
            "2.5796319389343263\n",
            "2.649946309328079\n",
            "2.597332448959351\n",
            "2.5477474427223203\n",
            "2.583917974233627\n",
            "2.4120006227493285\n",
            "2.3754854011535644\n",
            "2.686352516412735\n",
            "2.5812599992752077\n",
            "2.641901156902313\n",
            "2.578536885976791\n",
            "2.5481265318393707\n",
            "2.4926963472366332\n",
            "2.6016153216362\n",
            "2.5338714230060577\n",
            "2.410174398422241\n",
            "2.5341750824451448\n",
            "2.6091132593154907\n",
            "2.635704987049103\n",
            "2.4669368481636047\n",
            "2.61706685423851\n",
            "2.5566878628730776\n",
            "2.501255900859833\n",
            "2.542141045331955\n",
            "2.4496700060367584\n",
            "2.5871679186820984\n",
            "2.5826070177555085\n",
            "2.3800620329380036\n",
            "2.2302477562427523\n",
            "2.5129532551765443\n",
            "2.5357774698734286\n",
            "2.678563679456711\n",
            "2.569138818979263\n",
            "2.6261756575107573\n",
            "2.6055519700050356\n",
            "2.6420595967769622\n",
            "2.502309730052948\n",
            "2.569422664642334\n",
            "2.484584342241287\n",
            "2.384847812652588\n",
            "2.60895379781723\n",
            "2.5890609824657442\n",
            "2.57292352437973\n",
            "2.372497195005417\n",
            "2.396440533399582\n",
            "2.3148006904125213\n",
            "2.4828330779075625\n",
            "2.547016831636429\n",
            "2.7014959597587587\n",
            "2.704080855846405\n",
            "2.5157312464714052\n",
            "2.544527530670166\n",
            "2.5247619163990023\n",
            "2.568356055021286\n",
            "2.56108965754509\n",
            "2.467236713171005\n",
            "2.4678282952308654\n",
            "2.4491700971126558\n",
            "2.387267352342606\n",
            "2.4282929873466492\n",
            "2.3476936054229736\n",
            "2.324766019582748\n",
            "2.360139001607895\n",
            "2.4051680529117583\n",
            "2.3895324730873106\n",
            "2.394452353715897\n",
            "2.3588909029960634\n",
            "2.4447524869441986\n",
            "2.387870764732361\n",
            "2.3488213515281675\n",
            "2.3738750982284547\n",
            "2.392845695018768\n",
            "2.384792904853821\n",
            "2.4219415807724\n",
            "2.3847977042198183\n",
            "2.4272511017322542\n",
            "2.344524869918823\n",
            "2.4110827231407166\n",
            "2.4202289462089537\n",
            "2.3982449281215668\n",
            "2.3714003276824953\n",
            "2.3525534152984617\n",
            "2.397076370716095\n",
            "2.3809588623046873\n",
            "2.357650636434555\n",
            "2.370700109004974\n",
            "2.350805078744888\n",
            "2.3715302097797393\n",
            "2.37750457406044\n",
            "2.313649433851242\n",
            "2.397671625614166\n",
            "2.4129658842086794\n",
            "2.3495370221138\n",
            "2.3555729460716246\n",
            "2.3341850209236146\n",
            "2.4313002145290374\n",
            "2.3565884184837342\n",
            "2.4305268692970277\n",
            "2.3776397562026976\n",
            "2.335151743888855\n",
            "2.30424510717392\n",
            "2.3436491656303406\n",
            "2.3272825157642365\n",
            "2.3814248704910277\n",
            "2.3393121445178986\n",
            "2.3876556825637816\n",
            "2.358108934164047\n",
            "2.3679839634895323\n",
            "2.374079271554947\n",
            "2.410293264389038\n",
            "2.316732248067856\n",
            "2.3015146493911742\n",
            "2.3352182829380035\n",
            "2.3786334097385406\n",
            "2.380293629169464\n",
            "2.3356369495391847\n",
            "2.3791683959960936\n",
            "2.2919161093235014\n",
            "2.4038069427013395\n",
            "2.3822495114803313\n",
            "2.3747137343883513\n",
            "2.392351185083389\n",
            "2.3678585052490235\n",
            "2.3155653417110442\n",
            "2.411670752763748\n",
            "2.3781216835975645\n",
            "2.224196746349335\n",
            "2.3539242970943453\n",
            "2.366812722682953\n",
            "2.221162797212601\n",
            "2.4355508244037627\n",
            "2.391464182138443\n",
            "2.369078766107559\n",
            "2.2070684480667113\n",
            "2.456756339073181\n",
            "2.295430910587311\n",
            "2.369027935266495\n",
            "2.295909888744354\n",
            "2.2908840870857237\n",
            "2.4235199666023255\n",
            "2.3392382502555846\n",
            "2.3986868393421172\n",
            "2.2802877163887025\n",
            "2.347668693065643\n",
            "2.432131234407425\n",
            "2.2883779990673063\n",
            "2.3564908790588377\n",
            "2.3618303430080414\n",
            "2.260014898777008\n",
            "2.3033716344833373\n",
            "2.4702261340618135\n",
            "2.3946099770069122\n",
            "2.2394845259189604\n",
            "2.3679748344421387\n",
            "2.4453439795970917\n",
            "2.2401650512218474\n",
            "2.350056092739105\n",
            "2.4474627792835237\n",
            "2.482974786758423\n",
            "2.3315729343891145\n",
            "2.3491092681884767\n",
            "2.383410966396332\n",
            "2.3728077614307406\n",
            "2.2253549337387084\n",
            "2.2951476263999937\n",
            "2.451264959573746\n",
            "2.3916020655632018\n",
            "2.460844472646713\n",
            "2.2776432144641876\n",
            "2.369984070062637\n",
            "2.357108689546585\n",
            "2.5594803035259246\n",
            "2.189477630853653\n",
            "2.2559876799583436\n",
            "2.3229104483127596\n",
            "2.5818679082393645\n",
            "2.383712337017059\n",
            "2.4329933989048005\n",
            "2.3012846302986145\n",
            "2.3250869739055635\n",
            "2.3578665113449095\n",
            "2.346584823131561\n",
            "2.4339818072319033\n",
            "2.2532162070274353\n",
            "2.1898176169395445\n",
            "2.3082914400100707\n",
            "2.343243238925934\n",
            "2.4537584352493287\n",
            "2.4132715463638306\n",
            "2.374480412006378\n",
            "2.4840740621089936\n",
            "2.4022388660907747\n",
            "2.2420145750045775\n",
            "2.290266560316086\n",
            "2.3644023823738096\n",
            "2.387426396608353\n",
            "2.33076047539711\n",
            "2.626567836999893\n",
            "2.3381926548480987\n",
            "2.258842569589615\n",
            "2.122395222187042\n",
            "2.391466532945633\n",
            "2.357721219062805\n",
            "2.26556113243103\n",
            "2.3870786106586457\n",
            "2.5722660660743712\n",
            "2.398033159971237\n",
            "2.4345834720134736\n",
            "2.402960813045502\n",
            "2.382189453840256\n",
            "2.296310976743698\n",
            "2.1664938950538635\n",
            "2.340627909898758\n",
            "2.3470292747020722\n",
            "2.3877450478076936\n",
            "2.3306001484394074\n",
            "2.2730251896381377\n",
            "2.3829822158813476\n",
            "2.493687012195587\n",
            "2.354040504693985\n",
            "2.249505567550659\n",
            "2.1630016374588013\n",
            "2.183726439476013\n",
            "2.30791064620018\n",
            "2.4105684435367585\n",
            "2.1201842832565307\n",
            "2.3060287153720855\n",
            "2.4423381793498993\n",
            "2.633863613605499\n",
            "2.48761194229126\n",
            "2.3223811209201815\n",
            "2.4841766715049745\n",
            "2.4579760074615478\n",
            "2.433227813243866\n",
            "2.4850812089443206\n",
            "2.561297758817673\n",
            "2.6437475442886353\n",
            "2.5212155890464785\n",
            "2.545370627641678\n",
            "2.545967502593994\n",
            "2.5114280092716217\n",
            "2.501089872121811\n",
            "2.4925260865688323\n",
            "2.5198775255680084\n",
            "2.5051495635509493\n",
            "2.5193371415138244\n",
            "2.521288970708847\n",
            "2.4803844916820528\n",
            "2.5061289012432098\n",
            "2.4953613102436067\n",
            "2.4039686024188995\n",
            "2.479202318191528\n",
            "2.4436307764053344\n",
            "2.5334291207790374\n",
            "2.451744120121002\n",
            "2.552726572751999\n",
            "2.540737307071686\n",
            "2.4491809105873106\n",
            "2.5147612965106965\n",
            "2.4502323734760285\n",
            "2.442053579092026\n",
            "2.470287991762161\n",
            "2.4174676740169527\n",
            "2.3796987199783324\n",
            "2.4272958886623384\n",
            "2.398330043554306\n",
            "2.478829107284546\n",
            "2.4499790608882903\n",
            "2.486334527730942\n",
            "2.439410854578018\n",
            "2.4745621275901795\n",
            "2.4995726335048674\n",
            "2.464827462434769\n",
            "2.4384865140914918\n",
            "2.5332923793792723\n",
            "2.399267749786377\n",
            "2.5231213867664337\n",
            "2.4205201482772827\n",
            "2.498699998855591\n",
            "2.4299301993846893\n",
            "2.4948974621295927\n",
            "2.5005575025081637\n",
            "2.4977342331409456\n",
            "2.5142725002765656\n",
            "2.501253927946091\n",
            "2.5202792036533355\n",
            "2.3954218113422394\n",
            "2.4778072988986968\n",
            "2.423052052259445\n",
            "2.497001442909241\n",
            "2.4583933997154235\n",
            "2.5989829301834106\n",
            "2.4754274642467498\n",
            "2.4051035296916963\n",
            "2.489697278738022\n",
            "2.4381650400161745\n",
            "2.456684322357178\n",
            "2.4847676169872286\n",
            "2.5244948506355285\n",
            "2.406838903427124\n",
            "2.535266613960266\n",
            "2.5208188939094542\n",
            "2.464963822364807\n",
            "2.381424242258072\n",
            "2.4689841270446777\n",
            "2.4804345071315765\n",
            "2.457724385261536\n",
            "2.3241499960422516\n",
            "2.538314303159714\n",
            "2.502090173959732\n",
            "2.520995268821716\n",
            "2.4583080053329467\n",
            "2.3266321325302126\n",
            "2.419662789106369\n",
            "2.620199022293091\n",
            "2.5636780357360838\n",
            "2.4546300411224364\n",
            "2.5233082222938537\n",
            "2.3820640790462493\n",
            "2.308336000442505\n",
            "2.5241403818130492\n",
            "2.5559636056423187\n",
            "2.5413356387615202\n",
            "2.4399974405765534\n",
            "2.4389643871784212\n",
            "2.404390400648117\n",
            "2.239696922302246\n",
            "2.537945708036423\n",
            "2.612411103248596\n",
            "2.5026691114902495\n",
            "2.60207359790802\n",
            "2.4609739172458647\n",
            "2.4129254639148714\n",
            "2.4605268013477324\n",
            "2.6053661108016968\n",
            "2.3371273851394654\n",
            "2.301992827653885\n",
            "2.510773857831955\n",
            "2.675427451133728\n",
            "2.4163102662563323\n",
            "2.470605524778366\n",
            "2.5214810228347777\n",
            "2.4917229914665224\n",
            "2.4142925679683684\n",
            "2.395989855527878\n",
            "2.463129765987396\n",
            "2.5304581582546235\n",
            "2.324827420711517\n",
            "2.2081277596950533\n",
            "2.411690471172333\n",
            "2.411009529829025\n",
            "2.6976254761219023\n",
            "2.414716718196869\n",
            "2.498188849687576\n",
            "2.6372664093971254\n",
            "2.617096176147461\n",
            "2.494346321821213\n",
            "2.4764357948303224\n",
            "2.3493163812160494\n",
            "2.3560723757743833\n",
            "2.4930687713623048\n",
            "2.59989647269249\n",
            "2.490389041900635\n",
            "2.3132895743846893\n",
            "2.434839473962784\n",
            "2.1753776037693022\n",
            "2.392065244913101\n",
            "2.467886266708374\n",
            "2.580618060827255\n",
            "2.697912014722824\n",
            "2.508754594326019\n",
            "2.496824221611023\n",
            "2.3819874858856203\n",
            "2.618469564914703\n",
            "2.491840513944626\n",
            "2.441004902124405\n",
            "2.409328438043594\n",
            "2.3819830763339995\n",
            "2.351522994041443\n",
            "2.3483622992038726\n",
            "2.3027886044979096\n",
            "2.3018552005290984\n",
            "2.252105847597122\n",
            "2.340498651266098\n",
            "2.3468922996520996\n",
            "2.2935952246189117\n",
            "2.30838304400444\n",
            "2.3641832411289214\n",
            "2.371375859975815\n",
            "2.2905705189704895\n",
            "2.2682009410858153\n",
            "2.352078832387924\n",
            "2.280191913843155\n",
            "2.358236092329025\n",
            "2.3064909744262696\n",
            "2.392053129673004\n",
            "2.2989928138256075\n",
            "2.3236201965808867\n",
            "2.366357752084732\n",
            "2.3822649657726287\n",
            "2.291737906932831\n",
            "2.3140917778015138\n",
            "2.297541263103485\n",
            "2.3059846103191375\n",
            "2.327326362133026\n",
            "2.2823188030719757\n",
            "2.2930021572113035\n",
            "2.306842192411423\n",
            "2.314542986154556\n",
            "2.2841323912143707\n",
            "2.284509451389313\n",
            "2.3668336451053618\n",
            "2.312589975595474\n",
            "2.294473806619644\n",
            "2.2862713706493376\n",
            "2.3634633409976957\n",
            "2.3227983283996583\n",
            "2.34875537276268\n",
            "2.320182726383209\n",
            "2.2944912207126618\n",
            "2.2472549426555632\n",
            "2.2768968975543977\n",
            "2.2952795255184175\n",
            "2.2554110264778138\n",
            "2.3518431222438814\n",
            "2.3013287341594695\n",
            "2.299864823818207\n",
            "2.341728491783142\n",
            "2.3146972715854646\n",
            "2.3529599702358244\n",
            "2.341472284793854\n",
            "2.228627049922943\n",
            "2.3322211730480196\n",
            "2.2350137078762056\n",
            "2.338771823644638\n",
            "2.2907150852680207\n",
            "2.327409653663635\n",
            "2.316817536354065\n",
            "2.3029289090633394\n",
            "2.2885401821136475\n",
            "2.3468288910388946\n",
            "2.3641499423980714\n",
            "2.2876219630241392\n",
            "2.2588569891452788\n",
            "2.39637003660202\n",
            "2.2892366886138915\n",
            "2.24773957490921\n",
            "2.297932496070862\n",
            "2.2273834335803984\n",
            "2.2386305177211763\n",
            "2.33854453086853\n",
            "2.350842361450195\n",
            "2.320511819124222\n",
            "2.228670073747635\n",
            "2.3279204034805296\n",
            "2.296496170759201\n",
            "2.278447312116623\n",
            "2.2814232409000397\n",
            "2.251874179840088\n",
            "2.3995456099510193\n",
            "2.238459537029266\n",
            "2.308203740119934\n",
            "2.2910139298439027\n",
            "2.1974875855445863\n",
            "2.449526335000992\n",
            "2.285813411474228\n",
            "2.3109792113304137\n",
            "2.3023134219646453\n",
            "2.2204828667640686\n",
            "2.214851578474045\n",
            "2.4486523258686064\n",
            "2.3434385752677915\n",
            "2.252222958803177\n",
            "2.2543251621723175\n",
            "2.483438000679016\n",
            "2.1384826624393463\n",
            "2.2892677867412567\n",
            "2.418945393562317\n",
            "2.4144417107105256\n",
            "2.335761215686798\n",
            "2.298276054859161\n",
            "2.297425628900528\n",
            "2.409659072160721\n",
            "2.1266949439048766\n",
            "2.2853190970420836\n",
            "2.2893513762950897\n",
            "2.3787105083465576\n",
            "2.4688474202156065\n",
            "2.2645559668540955\n",
            "2.284695242643356\n",
            "2.331479413509369\n",
            "2.473351044654846\n",
            "2.2768746399879456\n",
            "2.1464896357059478\n",
            "2.2838821840286254\n",
            "2.363387470245361\n",
            "2.506619173288345\n",
            "2.4272327363491057\n",
            "2.3026249587535856\n",
            "2.248507024049759\n",
            "2.3327648627758024\n",
            "2.2396723771095277\n",
            "2.3722654330730437\n",
            "2.335960431098938\n",
            "2.117258893251419\n",
            "2.2290340304374694\n",
            "2.2920979225635527\n",
            "2.3123325753211974\n",
            "2.4900835835933686\n",
            "2.3053074991703033\n",
            "2.4634953224658966\n",
            "2.349915097951889\n",
            "2.209565452337265\n",
            "2.3588601756095886\n",
            "2.292154837846756\n",
            "2.360689196586609\n",
            "2.2231155383586882\n",
            "2.510714757442474\n",
            "2.4383835887908933\n",
            "2.1360115122795107\n",
            "2.1463114714622495\n",
            "2.2957881832122804\n",
            "2.3165394353866575\n",
            "2.269074469804764\n",
            "2.3104011380672453\n",
            "2.5321965229511263\n",
            "2.4635509526729584\n",
            "2.3060421311855315\n",
            "2.3668061661720277\n",
            "2.458156495094299\n",
            "2.2218017160892485\n",
            "2.2185135114192964\n",
            "2.2657703626155854\n",
            "2.3467882871627808\n",
            "2.3613524413108826\n",
            "2.279486232995987\n",
            "2.1565981662273406\n",
            "2.3570163547992706\n",
            "2.4889960277080534\n",
            "2.364188438653946\n",
            "2.1628916442394255\n",
            "2.198197592496872\n",
            "2.1150783491134644\n",
            "2.273534115552902\n",
            "2.2781300163269043\n",
            "2.229039376974106\n",
            "2.1978728771209717\n",
            "2.3149953544139863\n",
            "2.5717002296447755\n",
            "2.536851739883423\n",
            "2.311900578737259\n",
            "2.5038530683517455\n",
            "2.3430887722969054\n",
            "2.4788078653812406\n",
            "2.3785450875759127\n",
            "2.5291630244255066\n",
            "2.569900223016739\n",
            "2.524601970911026\n",
            "2.521981620788574\n",
            "2.537082949876785\n",
            "2.4633801794052124\n",
            "2.496981432437897\n",
            "2.4466504418849944\n",
            "2.456451828479767\n",
            "2.503239240646362\n",
            "2.484964487552643\n",
            "2.4913161718845367\n",
            "2.4602128183841705\n",
            "2.41806566119194\n",
            "2.48238667845726\n",
            "2.4083582484722137\n",
            "2.398680717945099\n",
            "2.443529682159424\n",
            "2.49488614320755\n",
            "2.4300128984451295\n",
            "2.4806154680252077\n",
            "2.5184203815460204\n",
            "2.4305166566371916\n",
            "2.4872764694690703\n",
            "2.4587704491615296\n",
            "2.4125680243968963\n",
            "2.421671936511993\n",
            "2.3814247572422027\n",
            "2.3257080793380736\n",
            "2.367517328262329\n",
            "2.426896630525589\n",
            "2.4111387753486633\n",
            "2.397193747758865\n",
            "2.4601978945732115\n",
            "2.4345622420310975\n",
            "2.4479705286026\n",
            "2.4431970930099487\n",
            "2.4555800902843474\n",
            "2.424342873096466\n",
            "2.4605642104148866\n",
            "2.4461408138275145\n",
            "2.4133646202087404\n",
            "2.3648379015922547\n",
            "2.432308927774429\n",
            "2.457507300376892\n",
            "2.4059131848812103\n",
            "2.471097646951675\n",
            "2.525801795721054\n",
            "2.5094847202301027\n",
            "2.3610915076732635\n",
            "2.5197589099407196\n",
            "2.351585384607315\n",
            "2.488951461315155\n",
            "2.341222448348999\n",
            "2.5238107430934904\n",
            "2.4764020574092864\n",
            "2.4826819586753843\n",
            "2.448109976053238\n",
            "2.3973640716075897\n",
            "2.486954963207245\n",
            "2.385388096570969\n",
            "2.388396906852722\n",
            "2.546530758142471\n",
            "2.4246339702606203\n",
            "2.426882916688919\n",
            "2.5214873683452605\n",
            "2.498267765045166\n",
            "2.436388849020004\n",
            "2.343472925424576\n",
            "2.4861823499202726\n",
            "2.4419662725925444\n",
            "2.366827026605606\n",
            "2.4553337824344634\n",
            "2.296325687170029\n",
            "2.5376010644435882\n",
            "2.5424257028102875\n",
            "2.3380696642398835\n",
            "2.4349527657032013\n",
            "2.319964654445648\n",
            "2.599353702068329\n",
            "2.5665304219722747\n",
            "2.425701276063919\n",
            "2.423978750705719\n",
            "2.4203142642974855\n",
            "2.280445076227188\n",
            "2.4995002818107603\n",
            "2.4860801351070405\n",
            "2.5493191623687745\n",
            "2.440662444829941\n",
            "2.389715609550476\n",
            "2.4715532052516935\n",
            "2.1450710535049438\n",
            "2.411780104637146\n",
            "2.65875177025795\n",
            "2.4677027809619902\n",
            "2.5290550982952116\n",
            "2.4974907886981965\n",
            "2.417903782129288\n",
            "2.352973834276199\n",
            "2.6015288436412813\n",
            "2.3939545249938963\n",
            "2.2306224381923676\n",
            "2.4252861511707304\n",
            "2.6481613421440127\n",
            "2.48049533367157\n",
            "2.3444966113567354\n",
            "2.510040041208267\n",
            "2.552559423446655\n",
            "2.3910443592071533\n",
            "2.4017863881587984\n",
            "2.340180815458298\n",
            "2.565199393033981\n",
            "2.3400211834907534\n",
            "2.2337717640399934\n",
            "2.1990021216869353\n",
            "2.426342349052429\n",
            "2.5876610136032103\n",
            "2.5378502774238587\n",
            "2.4703476095199584\n",
            "2.5756910741329193\n",
            "2.608779535293579\n",
            "2.47213072180748\n",
            "2.440887693166733\n",
            "2.3942508089542387\n",
            "2.3772147619724273\n",
            "2.3379544126987457\n",
            "2.5912495923042296\n",
            "2.4050899386405944\n",
            "2.462134350538254\n",
            "2.3307709705829622\n",
            "2.2242392504215243\n",
            "2.2923930287361145\n",
            "2.4895962822437285\n",
            "2.4516216111183167\n",
            "2.693931608200073\n",
            "2.6006170773506163\n",
            "2.4159847843647\n",
            "2.4493531155586243\n",
            "2.4850406634807585\n",
            "2.5301442992687226\n",
            "2.459381242990494\n",
            "2.3519340205192565\n",
            "2.3948751890659334\n",
            "2.335754084587097\n",
            "2.297990520000458\n",
            "2.356709145307541\n",
            "2.213164886236191\n",
            "2.257930818796158\n",
            "2.270881563425064\n",
            "2.3440232956409455\n",
            "2.259419207572937\n",
            "2.301072028875351\n",
            "2.3119383525848387\n",
            "2.335541241168976\n",
            "2.2696522104740144\n",
            "2.2486005997657776\n",
            "2.3586180984973906\n",
            "2.272485702037811\n",
            "2.318726449012756\n",
            "2.2577590024471283\n",
            "2.354115533828735\n",
            "2.311974846124649\n",
            "2.279951479434967\n",
            "2.3452889108657837\n",
            "2.332360019683838\n",
            "2.2955553901195525\n",
            "2.2992968475818634\n",
            "2.282358201742172\n",
            "2.2747284674644472\n",
            "2.2931780874729157\n",
            "2.2419043231010436\n",
            "2.2913607537746428\n",
            "2.252023686170578\n",
            "2.2609851956367493\n",
            "2.3016473829746245\n",
            "2.2552258837223054\n",
            "2.2978127443790437\n",
            "2.3345213067531585\n",
            "2.2622837686538695\n",
            "2.2715776228904723\n",
            "2.28094869017601\n",
            "2.363545095920563\n",
            "2.27842777967453\n",
            "2.2934183502197265\n",
            "2.264180303812027\n",
            "2.2619071555137635\n",
            "2.2552843475341797\n",
            "2.268258571624756\n",
            "2.2592044258117676\n",
            "2.290320130586624\n",
            "2.288383810520172\n",
            "2.2874666237831116\n",
            "2.2823742985725404\n",
            "2.3268482422828676\n",
            "2.358689979314804\n",
            "2.2938728284835816\n",
            "2.199459842443466\n",
            "2.300800359249115\n",
            "2.2247046530246735\n",
            "2.324030112028122\n",
            "2.278360674381256\n",
            "2.3022425186634066\n",
            "2.2730613481998443\n",
            "2.2380457091331483\n",
            "2.3472336184978486\n",
            "2.334762922525406\n",
            "2.3047517919540406\n",
            "2.2871431982517243\n",
            "2.263092931509018\n",
            "2.3049593448638914\n",
            "2.2804063248634336\n",
            "2.2498976969718933\n",
            "2.258704249858856\n",
            "2.2624221777915956\n",
            "2.2308350229263305\n",
            "2.2049358010292055\n",
            "2.385236051082611\n",
            "2.327055666446686\n",
            "2.2157547545433043\n",
            "2.2326911699771883\n",
            "2.3481296372413634\n",
            "2.2462077474594118\n",
            "2.275547786951065\n",
            "2.1803695225715636\n",
            "2.298829486370087\n",
            "2.3132648241519926\n",
            "2.2388208639621734\n",
            "2.3535521245002746\n",
            "2.197648026943207\n",
            "2.2965333604812623\n",
            "2.355364580154419\n",
            "2.2306948709487915\n",
            "2.2916498982906344\n",
            "2.2212892520427703\n",
            "2.22578169465065\n",
            "2.376368998289108\n",
            "2.3295621824264527\n",
            "2.2456045627593992\n",
            "2.2423219013214113\n",
            "2.359040184020996\n",
            "2.316393291950226\n",
            "2.1931025528907777\n",
            "2.3184876120090485\n",
            "2.390299799442291\n",
            "2.406353042125702\n",
            "2.2519853568077086\n",
            "2.265800167322159\n",
            "2.3859274017810823\n",
            "2.199915952682495\n",
            "2.181134343147278\n",
            "2.242248386144638\n",
            "2.4138626778125762\n",
            "2.3449864101409914\n",
            "2.3558099269866943\n",
            "2.2439798748493196\n",
            "2.220561122894287\n",
            "2.4473810732364654\n",
            "2.3726710760593415\n",
            "2.1020525538921357\n",
            "2.2474761044979097\n",
            "2.2810327184200285\n",
            "2.5595418572425843\n",
            "2.3432443153858187\n",
            "2.3917207431793215\n",
            "2.1529559206962587\n",
            "2.2741856479644778\n",
            "2.3107667779922485\n",
            "2.2764602422714235\n",
            "2.3975746715068817\n",
            "2.1758984684944154\n",
            "2.0781610977649687\n",
            "2.3012352538108827\n",
            "2.2372106075286866\n",
            "2.465653131008148\n",
            "2.284536964893341\n",
            "2.40325426697731\n",
            "2.424450958967209\n",
            "2.261263165473938\n",
            "2.242497215270996\n",
            "2.2972710156440734\n",
            "2.3240821778774263\n",
            "2.204710112810135\n",
            "2.3319567680358886\n",
            "2.565263622999191\n",
            "2.1848624789714814\n",
            "2.1681209909915924\n",
            "2.136799483299255\n",
            "2.3513739585876463\n",
            "2.266818346977234\n",
            "2.2929448008537294\n",
            "2.398372347354889\n",
            "2.486602487564087\n",
            "2.3073466491699217\n",
            "2.4019903469085695\n",
            "2.4156253480911256\n",
            "2.2597491979599\n",
            "2.2111762404441833\n",
            "2.1659062194824217\n",
            "2.3636828482151033\n",
            "2.297740272283554\n",
            "2.2628627598285673\n",
            "2.1876574873924257\n",
            "2.2857891643047332\n",
            "2.433430893421173\n",
            "2.382731809616089\n",
            "2.221596566438675\n",
            "2.254722672700882\n",
            "2.0491344678401946\n",
            "2.198603949546814\n",
            "2.25695813536644\n",
            "2.2937731993198396\n",
            "2.0674263930320738\n",
            "2.3603487360477446\n",
            "2.4241831696033476\n",
            "2.5289720833301543\n",
            "2.4205915212631224\n",
            "2.382781444787979\n",
            "2.364362941980362\n",
            "2.4576464223861696\n",
            "2.3765906620025636\n",
            "2.4868412947654726\n",
            "2.5498314332962035\n",
            "2.4976272344589234\n",
            "2.4713127863407136\n",
            "2.5142188692092895\n",
            "2.4666865277290344\n",
            "2.4818202769756317\n",
            "2.4427887272834776\n",
            "2.4386468529701233\n",
            "2.4881875681877137\n",
            "2.4356170678138733\n",
            "2.4841079246997833\n",
            "2.432703779935837\n",
            "2.4794883489608766\n",
            "2.4045504772663118\n",
            "2.411629502773285\n",
            "2.3897869431972505\n",
            "2.4407936251163482\n",
            "2.4044737792015076\n",
            "2.480110274553299\n",
            "2.457745497226715\n",
            "2.4672239804267884\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2264826731.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Clip the gradients at max norm 0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Do a gradient update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mis_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# prevent generators from being exhausted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_generator\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "total_loss = 0\n",
        "total_ct = 0\n",
        "total_epochs = 5\n",
        "\n",
        "for _ in range(total_epochs):\n",
        "    for name in open(names_path, 'r'):\n",
        "        name = name.lower().strip()\n",
        "        # Add the start and end padding token\n",
        "        name = '.' + name + '.'\n",
        "        # name[:-1]\n",
        "        x_data = torch.tensor([stoi[c] for c in name[:-1]], dtype=torch.long)\n",
        "        # name[1:]\n",
        "        y_data = torch.tensor([stoi[c] for c in name[1:]], dtype=torch.long)\n",
        "        logits = []\n",
        "        # Set the hidden state to random\n",
        "        h = torch.zeros(1, d_model)\n",
        "        # Zero the grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Loop through each token and get the new h and then pass it forward\n",
        "        # Accumulate all the logits\n",
        "        for x in x_data:\n",
        "            z, h = model(x.unsqueeze(0), h)\n",
        "\n",
        "            logits.append(z)\n",
        "\n",
        "        # Put all the logits into one tensor\n",
        "        logits = torch.cat(logits, dim=0)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = F.cross_entropy(logits, y_data)\n",
        "\n",
        "        # Get the new gradient\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients at max norm 0.1\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "        # Do a gradient update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the loss for the batch and get the number of batches\n",
        "        total_loss += loss.item()\n",
        "        total_ct += 1\n",
        "\n",
        "        if total_ct and total_ct % 100 == 0:\n",
        "            print(total_loss / total_ct)\n",
        "            total_loss = 0\n",
        "            total_ct = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "83f5f990",
      "metadata": {
        "id": "83f5f990",
        "outputId": "4ee82d69-4226-468c-890a-e994a725b98c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preplexity:  14.54911994934082\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    # Get perplexity\n",
        "    sumneglogp = 0\n",
        "    T = 0\n",
        "    for name in open(names_path, 'r'):\n",
        "        name = name.lower().strip()\n",
        "        T += len(name)\n",
        "        name = '.' + name\n",
        "        # Get the name from index 0 to -1 exclusive end\n",
        "        x_data = torch.tensor([stoi[c] for c in name[:-1]], dtype=torch.long)\n",
        "        # Get the y from index 1 to end inclusive end\n",
        "        y_data = torch.tensor([stoi[c] for c in name[1:]], dtype=torch.long)\n",
        "        # logits per token prediction\n",
        "        logits = []\n",
        "        # Initialize the h vector to random\n",
        "        h = torch.zeros(1, d_model)\n",
        "        # Loop over each chracter in the name and pass h and this into the RNN\n",
        "        # Get the new logit\n",
        "        for x in x_data:\n",
        "            # Get the int for x\n",
        "            x = x.unsqueeze(0)\n",
        "            # Get z and h\n",
        "            z, h = model(x, h)\n",
        "            # Append to logit\n",
        "            logits.append(z)\n",
        "\n",
        "        # Get all the logits for each character\n",
        "        logits = torch.cat(logits, dim=0)\n",
        "\n",
        "        # Compute the loss across all characters\n",
        "        loss = F.cross_entropy(logits, y_data, reduction='sum')\n",
        "\n",
        "        # Change to log base 2\n",
        "        # log2(x) = ln(x) / ln(2)\n",
        "        loss /= math.log(2)\n",
        "\n",
        "        sumneglogp += loss.item()\n",
        "\n",
        "    # sumneglogp is -log(p('.' + name1)) -log(p('.' + name2)) -log(p('.' + name3)) ...\n",
        "    # Divide by the appropriate term to get the answer we want\n",
        "    print('Preplexity: ', torch.pow(2, torch.tensor(sumneglogp / T)).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "7608536d",
      "metadata": {
        "id": "7608536d",
        "outputId": "399cb3ff-256e-4978-8d25-30add4aca60a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated name:  ceyro\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# Intialize the word with\n",
        "name = '.'\n",
        "# Initialize h to random\n",
        "h = torch.zeros(1, d_model)\n",
        "while True:\n",
        "    # Make c to an integer\n",
        "    c = torch.tensor([stoi[name[-1]]], dtype=torch.long)\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    logits, h = model(c, h)\n",
        "    # Get p; use Softmax\n",
        "    p = torch.softmax(logits, dim=1)\n",
        "    # Sample from p\n",
        "    c = torch.multinomial(p, num_samples=1)\n",
        "    # If we generate '.', stop\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d790024",
      "metadata": {
        "id": "4d790024"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}