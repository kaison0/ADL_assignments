{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "624b39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "939a704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_IN = \"FILL_IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fa935c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries, {idx -> ch} and {ch -> idx}\n",
    "itos = defaultdict(int)\n",
    "stoi = defaultdict(int)\n",
    "# Number of characters used to predict the target character in the MLP Language Model \n",
    "block_size = 3\n",
    "# Batch size used in MLP Language Model\n",
    "batch_size = 32\n",
    "# Embedding dimension, per character\n",
    "d_model = 10\n",
    "# Hidden dimension for RNN and also MLP Language Models \n",
    "d_h = 200\n",
    "\n",
    "# START = START token\n",
    "stoi['.'] = 0\n",
    "itos[0] = '.'\n",
    "\n",
    "# Loop over all names and create mappings itos and stoi mapping a unique character to a idx\n",
    "FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "d556793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(stoi) == len(itos)\n",
    "vocab_size = len(stoi)\n",
    "assert vocab_size == 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "3765e0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'.': 0,\n",
       "             'e': 1,\n",
       "             'm': 2,\n",
       "             'a': 3,\n",
       "             'o': 4,\n",
       "             'l': 5,\n",
       "             'i': 6,\n",
       "             'v': 7,\n",
       "             's': 8,\n",
       "             'b': 9,\n",
       "             'p': 10,\n",
       "             'h': 11,\n",
       "             'c': 12,\n",
       "             'r': 13,\n",
       "             't': 14,\n",
       "             'y': 15,\n",
       "             'n': 16,\n",
       "             'g': 17,\n",
       "             'z': 18,\n",
       "             'f': 19,\n",
       "             'd': 20,\n",
       "             'u': 21,\n",
       "             'k': 22,\n",
       "             'w': 23,\n",
       "             'q': 24,\n",
       "             'x': 25,\n",
       "             'j': 26})"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706797e",
   "metadata": {},
   "source": [
    "## BiGram Language Model\n",
    "- Implement the Bigram Language Model\n",
    "- Get all the relevent counts, then get the train dataset Perplexity\n",
    "- Use the class notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9cb554a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the formulas in class, loop over each name and get the parameters\n",
    "c1 = defaultdict(int)\n",
    "c2 = defaultdict(int)\n",
    "for name in open('names.txt', 'r'):\n",
    "    # Lowercase and remove any whitespace at the end\n",
    "    name = name.lower().strip()\n",
    "    # Pad with START = '.' and STOP = '.'\n",
    "    name = FILL_IN\n",
    "    # Transform to integer\n",
    "    name = FILL_IN\n",
    "    # Get the counts for Bigrams and Unigrams\n",
    "    FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "507e4525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preplexity:  13.895605087280273\n"
     ]
    }
   ],
   "source": [
    "# Get perplexity\n",
    "sumneglogp = 0\n",
    "T = 0\n",
    "for name in open('names.txt', 'r'):\n",
    "    # Get rid of white space and lowercase\n",
    "    name = name.lower().strip()\n",
    "    # Get the length of the word, without padding\n",
    "    T += FILL_IN\n",
    "    # Don't pad the STOP since we are not generating; pad with START\n",
    "    name = FILL_IN\n",
    "    # Transform to integrs\n",
    "    name = FILL_IN\n",
    "    # Get the loss -log(p(name)); use that the log of the product is the sum of the logs\n",
    "    FILL_IN\n",
    "# Print the Perplexity\n",
    "print('Preplexity: ', torch.pow(torch.tensor(sumneglogp / T ), 2).item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b931b726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated name:  moryamayashion\n"
     ]
    }
   ],
   "source": [
    "# Generate a random word using this distributon\n",
    "# When you generate STOP, terminate\n",
    "name = '.'\n",
    "while True:\n",
    "    c = stoi[name[-1]]\n",
    "    # Make the distribution from c to any other word other than START\n",
    "    p = []\n",
    "    for d in range(vocab_size):\n",
    "        # Use the same indicies as the dictionary we set up\n",
    "        # Populate p\n",
    "        FILL_IN\n",
    "    #print(p)\n",
    "    assert len(p) == vocab_size\n",
    "    # Sample randmly from the probability using torch.Categorical\n",
    "    c = FILL_IN\n",
    "    # Offset by 1 since we want indices [1, 2, ..., vocab_size]\n",
    "    if c.item() == 0:\n",
    "        break\n",
    "    else:\n",
    "        name += itos[c.item()]\n",
    "print('Generated name: ' , name[1:])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561cdacb",
   "metadata": {},
   "source": [
    "## MLP Language Model\n",
    "\n",
    "- Implement the MLP language model from below\n",
    "- Look at page 7, Equation (1)\n",
    "- https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982566bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "fb6328a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "for name in open('names.txt', 'r'):\n",
    "    name = name.lower().strip()\n",
    "    # Pad with block_size START tokens and 1 STOP token\n",
    "    name = ''.join(block_size * ['.']) + name + '.'\n",
    "    # Loop through name and get the (x, y) pairs\n",
    "    # Add (x, y) to x_data and y_data and make sure you transform to characters\n",
    "    # Make sure x_data and y_data have integers, use stoi\n",
    "    for i in range(len(name) - block_size):\n",
    "        FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "3142dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # An embedding for each character; vocab_size of them\n",
    "        self.e = FILL_IN\n",
    "        # H; should take in block_size * d_model vector and output d_h\n",
    "        self.H = FILL_IN\n",
    "        # U; should take in d_h vector and output vocab_size\n",
    "        self.U = FILL_IN\n",
    "        # W; for the skip connection, should take in block_size * d_model and output vocab_size\n",
    "        self.W = FILL_IN\n",
    "\n",
    "    # x should be (batch_size, block_size)\n",
    "    def forward(self, x):\n",
    "        FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "4a4f35f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0], 18)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[0], y_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "17bf7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataloader with x_data and y_data with batch_size\n",
    "dl = FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a56ce82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb, yb in dl:\n",
    "    assert xb.shape == (batch_size, 3)\n",
    "    assert yb.shape == (batch_size,)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1013903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model and the Adam optimizer learning rate 0.001\n",
    "model = MLPLanguageModel()\n",
    "optimizer = FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "0155215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5894072308540346\n",
      "2.457281219482422\n",
      "2.407582850694656\n",
      "2.3947769327163697\n",
      "2.386825907945633\n",
      "2.34425501203537\n",
      "2.3441826746463774\n",
      "2.3230127596855166\n",
      "2.314083107471466\n",
      "2.3187712411880494\n",
      "2.3163943738937376\n",
      "2.299647463083267\n",
      "2.3065791826248168\n",
      "2.2824714307785032\n",
      "2.264568898677826\n",
      "2.2898624875545504\n",
      "2.284603733062744\n",
      "2.256791707277298\n",
      "2.2530757277011872\n",
      "2.259875492334366\n",
      "2.2480121746063233\n",
      "2.2454004590511323\n",
      "2.237783147096634\n",
      "2.2407697353363036\n",
      "2.248863885164261\n",
      "2.2322758870124817\n",
      "2.2193622727394104\n",
      "2.223179480791092\n",
      "2.2356123917102813\n",
      "2.216988515138626\n",
      "2.22616584610939\n",
      "2.2130448422431948\n",
      "2.203174202203751\n",
      "2.2143791213035584\n",
      "2.2166521270275115\n",
      "2.2154732391834258\n",
      "2.1883920991420744\n",
      "2.2045855729579924\n",
      "2.2106930553913116\n",
      "2.211230798959732\n",
      "2.191891407251358\n",
      "2.1892505435943606\n",
      "2.196866712808609\n",
      "2.181224030017853\n",
      "2.1818389432430267\n",
      "2.186741435050964\n",
      "2.1679062938690183\n",
      "2.1833346343040465\n",
      "2.1822933118343353\n",
      "2.1826359000205993\n",
      "2.205969117641449\n",
      "2.1794409832954407\n",
      "2.1966153411865235\n",
      "2.195118425369263\n",
      "2.170777860403061\n",
      "2.168219382286072\n",
      "2.1821779885292054\n",
      "2.1704253492355345\n",
      "2.1621498544216156\n",
      "2.181074720859528\n",
      "2.1661094183921814\n",
      "2.172186158657074\n",
      "2.1562428092956543\n",
      "2.162130882978439\n",
      "2.1608908133506777\n",
      "2.1688261275291443\n",
      "2.1582104210853577\n",
      "2.1680945496559145\n",
      "2.1691222383975983\n",
      "2.1633048994541166\n",
      "2.152134317398071\n",
      "2.1508175642490386\n",
      "2.1616415870189667\n",
      "2.1374976155757905\n",
      "2.152686320066452\n",
      "2.1477243535518644\n",
      "2.1640802018642424\n",
      "2.1518292405605317\n",
      "2.152129183292389\n",
      "2.143674113750458\n",
      "2.1547027745246887\n",
      "2.159407968044281\n",
      "2.1475126917362215\n",
      "2.162065080881119\n",
      "2.1467708139419557\n",
      "2.1540623824596405\n",
      "2.1464312090873716\n",
      "2.128968066453934\n",
      "2.1382183706760407\n",
      "2.1496013498306272\n",
      "2.148638415813446\n",
      "2.133571047067642\n",
      "2.1516552584171293\n",
      "2.148024996519089\n",
      "2.145294527053833\n",
      "2.145066568374634\n",
      "2.145038235425949\n",
      "2.1378802795410157\n",
      "2.150877092123032\n",
      "2.124778664827347\n",
      "2.128606203556061\n",
      "2.1380219435691834\n",
      "2.140316452264786\n",
      "2.1450090050697326\n",
      "2.1483073034286497\n",
      "2.125716846227646\n",
      "2.1236170737743376\n",
      "2.1377969801425936\n",
      "2.1388748972415925\n",
      "2.140829493045807\n",
      "2.153381428003311\n",
      "2.1061591856479644\n",
      "2.1307544856071474\n",
      "2.1372199158668517\n",
      "2.1361645109653473\n",
      "2.119778084278107\n",
      "2.0978619689941405\n",
      "2.1331179332733154\n",
      "2.1190319130420683\n",
      "2.1416146104335785\n",
      "2.1346324706077575\n",
      "2.1361966743469236\n",
      "2.130432144165039\n",
      "2.11774720454216\n",
      "2.124930157661438\n",
      "2.132985883951187\n",
      "2.13123312997818\n",
      "2.1451848568916323\n",
      "2.1324277975559234\n",
      "2.121409861326218\n",
      "2.1230474712848664\n",
      "2.098636980772018\n",
      "2.118087923526764\n",
      "2.127527127981186\n",
      "2.1339828217029573\n",
      "2.1201842164993288\n",
      "2.1123575570583344\n",
      "2.128602982759476\n",
      "2.129157070398331\n",
      "2.1304832916259766\n",
      "2.1301893215179444\n",
      "2.1315822732448577\n",
      "2.114510152101517\n",
      "2.1158635640144348\n",
      "2.104869607925415\n",
      "2.1217241673469545\n",
      "2.116632284402847\n",
      "2.116808346271515\n",
      "2.1218403482437136\n",
      "2.108985232830048\n",
      "2.1268839735984804\n",
      "2.1111370854377745\n",
      "2.122393314361572\n",
      "2.1295010697841645\n",
      "2.1421598343849184\n",
      "2.1151951611042024\n",
      "2.1062611954212187\n",
      "2.1020620856285097\n",
      "2.1158076219558715\n",
      "2.123169098377228\n",
      "2.1105967876911165\n",
      "2.1142330892086028\n",
      "2.121842725992203\n",
      "2.1075729887485504\n",
      "2.116517170190811\n",
      "2.1107917795181272\n",
      "2.10650629734993\n",
      "2.139111280441284\n",
      "2.096643429040909\n",
      "2.1260074272155762\n",
      "2.120508100748062\n",
      "2.098237284183502\n",
      "2.112832992315292\n",
      "2.103355184793472\n",
      "2.097235008239746\n",
      "2.1176110644340516\n",
      "2.098088711977005\n",
      "2.127496450424194\n",
      "2.109674681901932\n",
      "2.1193545401096343\n",
      "2.1289110038280485\n",
      "2.103670207977295\n",
      "2.1201903245449065\n",
      "2.0963507046699523\n",
      "2.1147133910655977\n",
      "2.1300642762184143\n",
      "2.111483735084534\n",
      "2.100964566707611\n",
      "2.1067271945476533\n",
      "2.1055310201644897\n",
      "2.116045585155487\n",
      "2.112235931158066\n",
      "2.1058252339363097\n",
      "2.100637074232101\n",
      "2.1128933982849123\n",
      "2.0849624288082125\n",
      "2.0972255890369413\n",
      "2.1085698652267455\n",
      "2.1266038670539857\n",
      "2.100752772569656\n",
      "2.104310161113739\n",
      "2.0887409884929657\n",
      "2.1175877034664152\n",
      "2.1083028984069823\n",
      "2.102303539276123\n",
      "2.118862781763077\n",
      "2.1049597079753877\n",
      "2.118791632652283\n",
      "2.100370110988617\n",
      "2.117748591899872\n",
      "2.1125023913383485\n",
      "2.1087042751312257\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[296], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Get the logits\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlogits, target\u001b[38;5;241m=\u001b[39myb)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[291], line 27\u001b[0m, in \u001b[0;36mMLPLanguageModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTanh()(x)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# (batch_size, vocab_size)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mU\u001b[49m(x)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Add the skip connection\u001b[39;00m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x_skip\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1256\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_full_backward_hook\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_full_backward_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1258\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "total_ct = 0\n",
    "total_epochs = 20\n",
    "\n",
    "for _ in range(total_epochs):\n",
    "    for xb, yb in dl:\n",
    "        # Zero the gradients\n",
    "        FILL_IN\n",
    "\n",
    "        # Get the logits\n",
    "        logits = FILL_IN\n",
    "                \n",
    "        # Compute the loss\n",
    "        loss = FILL_IN\n",
    "\n",
    "        # Get the new gradient\n",
    "        FILL_IN\n",
    "\n",
    "        # Clip the gradients to max norm 0.1\n",
    "        # Use clid_grad_norm from torch\n",
    "        FILL_IN\n",
    "        \n",
    "        # Do a gradient update\n",
    "        FILL_IN\n",
    "\n",
    "        # Get the loss for the batch and get the number of batches\n",
    "        total_loss += loss.item()\n",
    "        total_ct += 1\n",
    "\n",
    "        # Print the loss\n",
    "        if total_ct and total_ct % 500 == 0:\n",
    "            print(total_loss / total_ct)\n",
    "            total_loss = 0\n",
    "            total_ct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "4ecf5fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preplexity:  10.598803520202637\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Get perplexity\n",
    "    sumneglogp = 0\n",
    "    T = 0\n",
    "    for name in open('names.txt', 'r'):\n",
    "        name = name.lower().strip()\n",
    "        T += len(name)\n",
    "        # Pad with block_size START tokens\n",
    "        name = FILL_IN\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        # Gather all the terms over the loss\n",
    "        # Notice that we compute -log p(...abc)\n",
    "        # Which is -log p(a | ...) - log p(b | a..) - log p(c | ba.)\n",
    "        FILL_IN\n",
    "        # Gather the loss over the name, for each term\n",
    "        # You need to get the softmax loss for each term\n",
    "        # Can either use the CrossEntropyLoss or do this manually\n",
    "        # Compute the loss\n",
    "        logits = FILL_IN\n",
    "\n",
    "        # Use reduction \"sum\" so you don't need to worry about N\n",
    "        loss = FILL_IN\n",
    "\n",
    "        # Change to log base 2\n",
    "        loss *= FILL_IN\n",
    "\n",
    "        sumneglogp += loss\n",
    "\n",
    "    print('Preplexity: ', torch.pow(sumneglogp.clone().detach() / T , 2).item())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e77131c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated name:  manue\n"
     ]
    }
   ],
   "source": [
    "# Generate a random word using this distributon\n",
    "# When you generate STOP, terminate\n",
    "name = ''.join(block_size * ['.'])\n",
    "while True:\n",
    "    # Get the idx\n",
    "    c = FILL_IN\n",
    "    # Make the distribution from c to any other word other than START\n",
    "    p = FILL_IN\n",
    "    # Randomly sample from p a new character\n",
    "    c = FILL_IN\n",
    "    if c.item() == 0:\n",
    "        break\n",
    "    else:\n",
    "        name += itos[c.item()]\n",
    "print('Generated name: ' , name[block_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b01ed",
   "metadata": {},
   "source": [
    "## RNN Language Model\n",
    "- For each name, run an RNN character by character\n",
    "- Use the recursion x = Tanh()(Wh @ h + Wx @ x + bh + bx) and y = Softmax()(Wy h + by)\n",
    "- Do not use the RNN Cell from PyTorch, do this manually as hinted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c765e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each token has an embedding of size vocab_size\n",
    "        self.e = FILL_IN\n",
    "        # Wh used to map hidden to hidden\n",
    "        self.Wh = FILL_IN\n",
    "        self.Wx = FILL_IN\n",
    "        self.Wy = FILL_IN\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Run through to get the embedding for the token\n",
    "        # The embedding per token is the feature vector x  we pass into the\n",
    "        # Represent x as an embedding\n",
    "        x = FILL_IN\n",
    "        # Get the hidden state\n",
    "        h = FILL_IN\n",
    "        # Get the logits we use to predict y\n",
    "        z = FILL_IN\n",
    "        # Return the z predicting y for the timestep we are at and the next hidden state\n",
    "        return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "cd798358",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLanguageModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3a49ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FILL_IN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# name[:-1]\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m x_data \u001b[38;5;241m=\u001b[39m \u001b[43mFILL_IN\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# name[1:]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m y_data \u001b[38;5;241m=\u001b[39m FILL_IN\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FILL_IN' is not defined"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "total_ct = 0\n",
    "total_epochs = 5\n",
    "\n",
    "for _ in range(total_epochs):\n",
    "    for name in open('names.txt', 'r'):\n",
    "        name = name.lower().strip()\n",
    "        # Add the start and end padding token\n",
    "        name = '.' + name + '.'\n",
    "        # name[:-1]\n",
    "        x_data = FILL_IN\n",
    "        # name[1:]\n",
    "        y_data = FILL_IN\n",
    "        logits = []\n",
    "        # Set the hidden state to random\n",
    "        h = FILL_IN\n",
    "        # Zero the grad\n",
    "        FILL_IN\n",
    "        \n",
    "        # Loop through each token and get the new h and then pass it forward\n",
    "        # Accumulate all the logits\n",
    "        for x in x_data:\n",
    "            FILL_IN\n",
    "            FILL_IN\n",
    "        \n",
    "        # Put all the logits into one tensor\n",
    "        logits = FILL_IN\n",
    "                \n",
    "        # Compute the loss\n",
    "        loss = FILL_IN\n",
    "\n",
    "        # Get the new gradient\n",
    "        FILL_IN\n",
    "\n",
    "        # Clip the gradients at max norm 0.1\n",
    "        FILL_IN\n",
    "        \n",
    "        # Do a gradient update\n",
    "        FILL_IN\n",
    "\n",
    "        # Get the loss for the batch and get the number of batches\n",
    "        total_loss += loss.item()\n",
    "        total_ct += 1\n",
    "\n",
    "        if total_ct and total_ct % 100 == 0:\n",
    "            print(total_loss / total_ct)\n",
    "            total_loss = 0\n",
    "            total_ct = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "83f5f990",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[312], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_data:\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m stoi[x]\n\u001b[0;32m---> 21\u001b[0m     z, h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     logits\u001b[38;5;241m.\u001b[39mappend(z)\n\u001b[1;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Get perplexity\n",
    "    sumneglogp = 0\n",
    "    T = 0\n",
    "    for name in open('names.txt', 'r'):\n",
    "        name = name.lower().strip()\n",
    "        T += len(name)\n",
    "        name = '.' + name\n",
    "        # Get the name from index 0 to -1 exclusive end\n",
    "        x_data = FILL_IN\n",
    "        # Get the y from index 1 to end inclusive end\n",
    "        y_data = FILL_IN\n",
    "        # logits per token prediction\n",
    "        logits = []\n",
    "        # Initialize the h vector to random\n",
    "        h = FILL_IN\n",
    "        # Loop over each chracter in the name and pass h and this into the RNN\n",
    "        # Get the new logit\n",
    "        for x in x_data:\n",
    "            # Get the int for x\n",
    "            x = FILL_IN\n",
    "            # Get z and h\n",
    "            z, h = FILL_IN\n",
    "            # Append to logit\n",
    "            FILL_IN\n",
    "\n",
    "        # Get all the logits for each character\n",
    "        logits = FILL_IN\n",
    "\n",
    "        # Compute the loss across all characters\n",
    "        loss = FILL_IN\n",
    "                \n",
    "        # Change to log base 2\n",
    "        # log2(x) = ln(x) / ln(2)\n",
    "        loss *= FILL_IN\n",
    "\n",
    "        sumneglogp += FILL_IN\n",
    "        \n",
    "    # sumneglogp is -log(p('.' + name1)) -log(p('.' + name2)) -log(p('.' + name3)) ...  \n",
    "    # Divide by the appropriate term to get the answer we want \n",
    "    print('Preplexity: ', torch.pow(sumneglogp.clone().detach() / T , 2).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7608536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated name:  zacarlan\n"
     ]
    }
   ],
   "source": [
    "# Generate a random word using this distributon\n",
    "# Intialize the word with \n",
    "name = '.'\n",
    "# Initialize h to random\n",
    "h = FILL_IN\n",
    "while True:\n",
    "    # Make c to an integer\n",
    "    c = FILL_IN\n",
    "    # Make the distribution from c to any other word other than START\n",
    "    logits, h = FILL_IN\n",
    "    # Get p; use Softmax\n",
    "    p = FILL_IN\n",
    "    # Sample from p\n",
    "    c = FILL_IN\n",
    "    # If we generate '.', stop\n",
    "    if c.item() == 0:\n",
    "        break\n",
    "    else:\n",
    "        name += itos[c.item()]\n",
    "print('Generated name: ' , name[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d790024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
