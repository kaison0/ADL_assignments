{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "624b39df",
      "metadata": {
        "id": "624b39df"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import tqdm\n",
        "import math\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "939a704e",
      "metadata": {
        "id": "939a704e"
      },
      "outputs": [],
      "source": [
        "FILL_IN = \"FILL_IN\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "names = []\n",
        "names_path = '/content/drive/MyDrive/ADL_HW/names.txt'\n",
        "with open(names_path, 'r') as f:\n",
        "    names = f.read().splitlines()"
      ],
      "metadata": {
        "id": "ENGCaduhcDV4",
        "outputId": "624c2dfa-37c2-452b-ee3f-f61a7f12ae96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ENGCaduhcDV4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names"
      ],
      "metadata": {
        "id": "qJ79WW1Uc4Of",
        "outputId": "ca50727b-e936-49a0-b8d0-da2cdce54552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qJ79WW1Uc4Of",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn',\n",
              " 'abigail',\n",
              " 'emily',\n",
              " 'elizabeth',\n",
              " 'mila',\n",
              " 'ella',\n",
              " 'avery',\n",
              " 'sofia',\n",
              " 'camila',\n",
              " 'aria',\n",
              " 'scarlett',\n",
              " 'victoria',\n",
              " 'madison',\n",
              " 'luna',\n",
              " 'grace',\n",
              " 'chloe',\n",
              " 'penelope',\n",
              " 'layla',\n",
              " 'riley',\n",
              " 'zoey',\n",
              " 'nora',\n",
              " 'lily',\n",
              " 'eleanor',\n",
              " 'hannah',\n",
              " 'lillian',\n",
              " 'addison',\n",
              " 'aubrey',\n",
              " 'ellie',\n",
              " 'stella',\n",
              " 'natalie',\n",
              " 'zoe',\n",
              " 'leah',\n",
              " 'hazel',\n",
              " 'violet',\n",
              " 'aurora',\n",
              " 'savannah',\n",
              " 'audrey',\n",
              " 'brooklyn',\n",
              " 'bella',\n",
              " 'claire',\n",
              " 'skylar',\n",
              " 'lucy',\n",
              " 'paisley',\n",
              " 'everly',\n",
              " 'anna',\n",
              " 'caroline',\n",
              " 'nova',\n",
              " 'genesis',\n",
              " 'emilia',\n",
              " 'kennedy',\n",
              " 'samantha',\n",
              " 'maya',\n",
              " 'willow',\n",
              " 'kinsley',\n",
              " 'naomi',\n",
              " 'aaliyah',\n",
              " 'elena',\n",
              " 'sarah',\n",
              " 'ariana',\n",
              " 'allison',\n",
              " 'gabriella',\n",
              " 'alice',\n",
              " 'madelyn',\n",
              " 'cora',\n",
              " 'ruby',\n",
              " 'eva',\n",
              " 'serenity',\n",
              " 'autumn',\n",
              " 'adeline',\n",
              " 'hailey',\n",
              " 'gianna',\n",
              " 'valentina',\n",
              " 'isla',\n",
              " 'eliana',\n",
              " 'quinn',\n",
              " 'nevaeh',\n",
              " 'ivy',\n",
              " 'sadie',\n",
              " 'piper',\n",
              " 'lydia',\n",
              " 'alexa',\n",
              " 'josephine',\n",
              " 'emery',\n",
              " 'julia',\n",
              " 'delilah',\n",
              " 'arianna',\n",
              " 'vivian',\n",
              " 'kaylee',\n",
              " 'sophie',\n",
              " 'brielle',\n",
              " 'madeline',\n",
              " 'peyton',\n",
              " 'rylee',\n",
              " 'clara',\n",
              " 'hadley',\n",
              " 'melanie',\n",
              " 'mackenzie',\n",
              " 'reagan',\n",
              " 'adalynn',\n",
              " 'liliana',\n",
              " 'aubree',\n",
              " 'jade',\n",
              " 'katherine',\n",
              " 'isabelle',\n",
              " 'natalia',\n",
              " 'raelynn',\n",
              " 'maria',\n",
              " 'athena',\n",
              " 'ximena',\n",
              " 'arya',\n",
              " 'leilani',\n",
              " 'taylor',\n",
              " 'faith',\n",
              " 'rose',\n",
              " 'kylie',\n",
              " 'alexandra',\n",
              " 'mary',\n",
              " 'margaret',\n",
              " 'lyla',\n",
              " 'ashley',\n",
              " 'amaya',\n",
              " 'eliza',\n",
              " 'brianna',\n",
              " 'bailey',\n",
              " 'andrea',\n",
              " 'khloe',\n",
              " 'jasmine',\n",
              " 'melody',\n",
              " 'iris',\n",
              " 'isabel',\n",
              " 'norah',\n",
              " 'annabelle',\n",
              " 'valeria',\n",
              " 'emerson',\n",
              " 'adalyn',\n",
              " 'ryleigh',\n",
              " 'eden',\n",
              " 'emersyn',\n",
              " 'anastasia',\n",
              " 'kayla',\n",
              " 'alyssa',\n",
              " 'juliana',\n",
              " 'charlie',\n",
              " 'esther',\n",
              " 'ariel',\n",
              " 'cecilia',\n",
              " 'valerie',\n",
              " 'alina',\n",
              " 'molly',\n",
              " 'reese',\n",
              " 'aliyah',\n",
              " 'lilly',\n",
              " 'parker',\n",
              " 'finley',\n",
              " 'morgan',\n",
              " 'sydney',\n",
              " 'jordyn',\n",
              " 'eloise',\n",
              " 'trinity',\n",
              " 'daisy',\n",
              " 'kimberly',\n",
              " 'lauren',\n",
              " 'genevieve',\n",
              " 'sara',\n",
              " 'arabella',\n",
              " 'harmony',\n",
              " 'elise',\n",
              " 'remi',\n",
              " 'teagan',\n",
              " 'alexis',\n",
              " 'london',\n",
              " 'sloane',\n",
              " 'laila',\n",
              " 'lucia',\n",
              " 'diana',\n",
              " 'juliette',\n",
              " 'sienna',\n",
              " 'elliana',\n",
              " 'londyn',\n",
              " 'ayla',\n",
              " 'callie',\n",
              " 'gracie',\n",
              " 'josie',\n",
              " 'amara',\n",
              " 'jocelyn',\n",
              " 'daniela',\n",
              " 'everleigh',\n",
              " 'mya',\n",
              " 'rachel',\n",
              " 'summer',\n",
              " 'alana',\n",
              " 'brooke',\n",
              " 'alaina',\n",
              " 'mckenzie',\n",
              " 'catherine',\n",
              " 'amy',\n",
              " 'presley',\n",
              " 'journee',\n",
              " 'rosalie',\n",
              " 'ember',\n",
              " 'brynlee',\n",
              " 'rowan',\n",
              " 'joanna',\n",
              " 'paige',\n",
              " 'rebecca',\n",
              " 'ana',\n",
              " 'sawyer',\n",
              " 'mariah',\n",
              " 'nicole',\n",
              " 'brooklynn',\n",
              " 'payton',\n",
              " 'marley',\n",
              " 'fiona',\n",
              " 'georgia',\n",
              " 'lila',\n",
              " 'harley',\n",
              " 'adelyn',\n",
              " 'alivia',\n",
              " 'noelle',\n",
              " 'gemma',\n",
              " 'vanessa',\n",
              " 'journey',\n",
              " 'makayla',\n",
              " 'angelina',\n",
              " 'adaline',\n",
              " 'catalina',\n",
              " 'alayna',\n",
              " 'julianna',\n",
              " 'leila',\n",
              " 'lola',\n",
              " 'adriana',\n",
              " 'june',\n",
              " 'juliet',\n",
              " 'jayla',\n",
              " 'river',\n",
              " 'tessa',\n",
              " 'lia',\n",
              " 'dakota',\n",
              " 'delaney',\n",
              " 'selena',\n",
              " 'blakely',\n",
              " 'ada',\n",
              " 'camille',\n",
              " 'zara',\n",
              " 'malia',\n",
              " 'hope',\n",
              " 'samara',\n",
              " 'vera',\n",
              " 'mckenna',\n",
              " 'briella',\n",
              " 'izabella',\n",
              " 'hayden',\n",
              " 'raegan',\n",
              " 'michelle',\n",
              " 'angela',\n",
              " 'ruth',\n",
              " 'freya',\n",
              " 'kamila',\n",
              " 'vivienne',\n",
              " 'aspen',\n",
              " 'olive',\n",
              " 'kendall',\n",
              " 'elaina',\n",
              " 'thea',\n",
              " 'kali',\n",
              " 'destiny',\n",
              " 'amiyah',\n",
              " 'evangeline',\n",
              " 'cali',\n",
              " 'blake',\n",
              " 'elsie',\n",
              " 'juniper',\n",
              " 'alexandria',\n",
              " 'myla',\n",
              " 'ariella',\n",
              " 'kate',\n",
              " 'mariana',\n",
              " 'lilah',\n",
              " 'charlee',\n",
              " 'daleyza',\n",
              " 'nyla',\n",
              " 'jane',\n",
              " 'maggie',\n",
              " 'zuri',\n",
              " 'aniyah',\n",
              " 'lucille',\n",
              " 'leia',\n",
              " 'melissa',\n",
              " 'adelaide',\n",
              " 'amina',\n",
              " 'giselle',\n",
              " 'lena',\n",
              " 'camilla',\n",
              " 'miriam',\n",
              " 'millie',\n",
              " 'brynn',\n",
              " 'gabrielle',\n",
              " 'sage',\n",
              " 'annie',\n",
              " 'logan',\n",
              " 'lilliana',\n",
              " 'haven',\n",
              " 'jessica',\n",
              " 'kaia',\n",
              " 'magnolia',\n",
              " 'amira',\n",
              " 'adelynn',\n",
              " 'makenzie',\n",
              " 'stephanie',\n",
              " 'nina',\n",
              " 'phoebe',\n",
              " 'arielle',\n",
              " 'evie',\n",
              " 'lyric',\n",
              " 'alessandra',\n",
              " 'gabriela',\n",
              " 'paislee',\n",
              " 'raelyn',\n",
              " 'madilyn',\n",
              " 'paris',\n",
              " 'makenna',\n",
              " 'kinley',\n",
              " 'gracelyn',\n",
              " 'talia',\n",
              " 'maeve',\n",
              " 'rylie',\n",
              " 'kiara',\n",
              " 'evelynn',\n",
              " 'brinley',\n",
              " 'jacqueline',\n",
              " 'laura',\n",
              " 'gracelynn',\n",
              " 'lexi',\n",
              " 'ariah',\n",
              " 'fatima',\n",
              " 'jennifer',\n",
              " 'kehlani',\n",
              " 'alani',\n",
              " 'ariyah',\n",
              " 'luciana',\n",
              " 'allie',\n",
              " 'heidi',\n",
              " 'maci',\n",
              " 'phoenix',\n",
              " 'felicity',\n",
              " 'joy',\n",
              " 'kenzie',\n",
              " 'veronica',\n",
              " 'margot',\n",
              " 'addilyn',\n",
              " 'lana',\n",
              " 'cassidy',\n",
              " 'remington',\n",
              " 'saylor',\n",
              " 'ryan',\n",
              " 'keira',\n",
              " 'harlow',\n",
              " 'miranda',\n",
              " 'angel',\n",
              " 'amanda',\n",
              " 'daniella',\n",
              " 'royalty',\n",
              " 'gwendolyn',\n",
              " 'ophelia',\n",
              " 'heaven',\n",
              " 'jordan',\n",
              " 'madeleine',\n",
              " 'esmeralda',\n",
              " 'kira',\n",
              " 'miracle',\n",
              " 'elle',\n",
              " 'amari',\n",
              " 'danielle',\n",
              " 'daphne',\n",
              " 'willa',\n",
              " 'haley',\n",
              " 'gia',\n",
              " 'kaitlyn',\n",
              " 'oakley',\n",
              " 'kailani',\n",
              " 'winter',\n",
              " 'alicia',\n",
              " 'serena',\n",
              " 'nadia',\n",
              " 'aviana',\n",
              " 'demi',\n",
              " 'jada',\n",
              " 'braelynn',\n",
              " 'dylan',\n",
              " 'ainsley',\n",
              " 'alison',\n",
              " 'camryn',\n",
              " 'avianna',\n",
              " 'bianca',\n",
              " 'skyler',\n",
              " 'scarlet',\n",
              " 'maddison',\n",
              " 'nylah',\n",
              " 'sarai',\n",
              " 'regina',\n",
              " 'dahlia',\n",
              " 'nayeli',\n",
              " 'raven',\n",
              " 'helen',\n",
              " 'adrianna',\n",
              " 'averie',\n",
              " 'skye',\n",
              " 'kelsey',\n",
              " 'tatum',\n",
              " 'kensley',\n",
              " 'maliyah',\n",
              " 'erin',\n",
              " 'viviana',\n",
              " 'jenna',\n",
              " 'anaya',\n",
              " 'carolina',\n",
              " 'shelby',\n",
              " 'sabrina',\n",
              " 'mikayla',\n",
              " 'annalise',\n",
              " 'octavia',\n",
              " 'lennon',\n",
              " 'blair',\n",
              " 'carmen',\n",
              " 'yaretzi',\n",
              " 'kennedi',\n",
              " 'mabel',\n",
              " 'zariah',\n",
              " 'kyla',\n",
              " 'christina',\n",
              " 'selah',\n",
              " 'celeste',\n",
              " 'eve',\n",
              " 'mckinley',\n",
              " 'milani',\n",
              " 'frances',\n",
              " 'jimena',\n",
              " 'kylee',\n",
              " 'leighton',\n",
              " 'katie',\n",
              " 'aitana',\n",
              " 'kayleigh',\n",
              " 'sierra',\n",
              " 'kathryn',\n",
              " 'rosemary',\n",
              " 'jolene',\n",
              " 'alondra',\n",
              " 'elisa',\n",
              " 'helena',\n",
              " 'charleigh',\n",
              " 'hallie',\n",
              " 'lainey',\n",
              " 'avah',\n",
              " 'jazlyn',\n",
              " 'kamryn',\n",
              " 'mira',\n",
              " 'cheyenne',\n",
              " 'francesca',\n",
              " 'antonella',\n",
              " 'wren',\n",
              " 'chelsea',\n",
              " 'amber',\n",
              " 'emory',\n",
              " 'lorelei',\n",
              " 'nia',\n",
              " 'abby',\n",
              " 'april',\n",
              " 'emelia',\n",
              " 'carter',\n",
              " 'aylin',\n",
              " 'cataleya',\n",
              " 'bethany',\n",
              " 'marlee',\n",
              " 'carly',\n",
              " 'kaylani',\n",
              " 'emely',\n",
              " 'liana',\n",
              " 'madelynn',\n",
              " 'cadence',\n",
              " 'matilda',\n",
              " 'sylvia',\n",
              " 'myra',\n",
              " 'fernanda',\n",
              " 'oaklyn',\n",
              " 'elianna',\n",
              " 'hattie',\n",
              " 'dayana',\n",
              " 'kendra',\n",
              " 'maisie',\n",
              " 'malaysia',\n",
              " 'kara',\n",
              " 'katelyn',\n",
              " 'maia',\n",
              " 'celine',\n",
              " 'cameron',\n",
              " 'renata',\n",
              " 'jayleen',\n",
              " 'charli',\n",
              " 'emmalyn',\n",
              " 'holly',\n",
              " 'azalea',\n",
              " 'leona',\n",
              " 'alejandra',\n",
              " 'bristol',\n",
              " 'collins',\n",
              " 'imani',\n",
              " 'meadow',\n",
              " 'alexia',\n",
              " 'edith',\n",
              " 'kaydence',\n",
              " 'leslie',\n",
              " 'lilith',\n",
              " 'kora',\n",
              " 'aisha',\n",
              " 'meredith',\n",
              " 'danna',\n",
              " 'wynter',\n",
              " 'emberly',\n",
              " 'julieta',\n",
              " 'michaela',\n",
              " 'alayah',\n",
              " 'jemma',\n",
              " 'reign',\n",
              " 'colette',\n",
              " 'kaliyah',\n",
              " 'elliott',\n",
              " 'johanna',\n",
              " 'remy',\n",
              " 'sutton',\n",
              " 'emmy',\n",
              " 'virginia',\n",
              " 'briana',\n",
              " 'oaklynn',\n",
              " 'adelina',\n",
              " 'everlee',\n",
              " 'megan',\n",
              " 'angelica',\n",
              " 'justice',\n",
              " 'mariam',\n",
              " 'khaleesi',\n",
              " 'macie',\n",
              " 'karsyn',\n",
              " 'alanna',\n",
              " 'aleah',\n",
              " 'mae',\n",
              " 'mallory',\n",
              " 'esme',\n",
              " 'skyla',\n",
              " 'madilynn',\n",
              " 'charley',\n",
              " 'allyson',\n",
              " 'hanna',\n",
              " 'shiloh',\n",
              " 'henley',\n",
              " 'macy',\n",
              " 'maryam',\n",
              " 'ivanna',\n",
              " 'ashlynn',\n",
              " 'lorelai',\n",
              " 'amora',\n",
              " 'ashlyn',\n",
              " 'sasha',\n",
              " 'baylee',\n",
              " 'beatrice',\n",
              " 'itzel',\n",
              " 'priscilla',\n",
              " 'marie',\n",
              " 'jayda',\n",
              " 'liberty',\n",
              " 'rory',\n",
              " 'alessia',\n",
              " 'alaia',\n",
              " 'janelle',\n",
              " 'kalani',\n",
              " 'gloria',\n",
              " 'sloan',\n",
              " 'dorothy',\n",
              " 'greta',\n",
              " 'julie',\n",
              " 'zahra',\n",
              " 'savanna',\n",
              " 'annabella',\n",
              " 'poppy',\n",
              " 'amalia',\n",
              " 'zaylee',\n",
              " 'cecelia',\n",
              " 'coraline',\n",
              " 'kimber',\n",
              " 'emmie',\n",
              " 'anne',\n",
              " 'karina',\n",
              " 'kassidy',\n",
              " 'kynlee',\n",
              " 'monroe',\n",
              " 'anahi',\n",
              " 'jaliyah',\n",
              " 'jazmin',\n",
              " 'maren',\n",
              " 'monica',\n",
              " 'siena',\n",
              " 'marilyn',\n",
              " 'reyna',\n",
              " 'kyra',\n",
              " 'lilian',\n",
              " 'jamie',\n",
              " 'melany',\n",
              " 'alaya',\n",
              " 'ariya',\n",
              " 'kelly',\n",
              " 'rosie',\n",
              " 'adley',\n",
              " 'dream',\n",
              " 'jaylah',\n",
              " 'laurel',\n",
              " 'jazmine',\n",
              " 'mina',\n",
              " 'karla',\n",
              " 'bailee',\n",
              " 'aubrie',\n",
              " 'katalina',\n",
              " 'melina',\n",
              " 'harlee',\n",
              " 'elliot',\n",
              " 'hayley',\n",
              " 'elaine',\n",
              " 'karen',\n",
              " 'dallas',\n",
              " 'irene',\n",
              " 'lylah',\n",
              " 'ivory',\n",
              " 'chaya',\n",
              " 'rosa',\n",
              " 'aleena',\n",
              " 'braelyn',\n",
              " 'nola',\n",
              " 'alma',\n",
              " 'leyla',\n",
              " 'pearl',\n",
              " 'addyson',\n",
              " 'roselyn',\n",
              " 'lacey',\n",
              " 'lennox',\n",
              " 'reina',\n",
              " 'aurelia',\n",
              " 'noa',\n",
              " 'janiyah',\n",
              " 'jessie',\n",
              " 'madisyn',\n",
              " 'saige',\n",
              " 'alia',\n",
              " 'tiana',\n",
              " 'astrid',\n",
              " 'cassandra',\n",
              " 'kyleigh',\n",
              " 'romina',\n",
              " 'stevie',\n",
              " 'haylee',\n",
              " 'zelda',\n",
              " 'lillie',\n",
              " 'aileen',\n",
              " 'brylee',\n",
              " 'eileen',\n",
              " 'yara',\n",
              " 'ensley',\n",
              " 'lauryn',\n",
              " 'giuliana',\n",
              " 'livia',\n",
              " 'anya',\n",
              " 'mikaela',\n",
              " 'palmer',\n",
              " 'lyra',\n",
              " 'mara',\n",
              " 'marina',\n",
              " 'kailey',\n",
              " 'liv',\n",
              " 'clementine',\n",
              " 'kenna',\n",
              " 'briar',\n",
              " 'emerie',\n",
              " 'galilea',\n",
              " 'tiffany',\n",
              " 'bonnie',\n",
              " 'elyse',\n",
              " 'cynthia',\n",
              " 'frida',\n",
              " 'kinslee',\n",
              " 'tatiana',\n",
              " 'joelle',\n",
              " 'armani',\n",
              " 'jolie',\n",
              " 'nalani',\n",
              " 'rayna',\n",
              " 'yareli',\n",
              " 'meghan',\n",
              " 'rebekah',\n",
              " 'addilynn',\n",
              " 'faye',\n",
              " 'zariyah',\n",
              " 'lea',\n",
              " 'aliza',\n",
              " 'julissa',\n",
              " 'lilyana',\n",
              " 'anika',\n",
              " 'kairi',\n",
              " 'aniya',\n",
              " 'noemi',\n",
              " 'angie',\n",
              " 'crystal',\n",
              " 'bridget',\n",
              " 'ari',\n",
              " 'davina',\n",
              " 'amelie',\n",
              " 'amirah',\n",
              " 'annika',\n",
              " 'elora',\n",
              " 'xiomara',\n",
              " 'linda',\n",
              " 'hana',\n",
              " 'laney',\n",
              " 'mercy',\n",
              " 'hadassah',\n",
              " 'madalyn',\n",
              " 'louisa',\n",
              " 'simone',\n",
              " 'kori',\n",
              " 'jillian',\n",
              " 'alena',\n",
              " 'malaya',\n",
              " 'miley',\n",
              " 'milan',\n",
              " 'sariyah',\n",
              " 'malani',\n",
              " 'clarissa',\n",
              " 'nala',\n",
              " 'princess',\n",
              " 'amani',\n",
              " 'analia',\n",
              " 'estella',\n",
              " 'milana',\n",
              " 'aya',\n",
              " 'chana',\n",
              " 'jayde',\n",
              " 'tenley',\n",
              " 'zaria',\n",
              " 'itzayana',\n",
              " 'penny',\n",
              " 'ailani',\n",
              " 'lara',\n",
              " 'aubriella',\n",
              " 'clare',\n",
              " 'lina',\n",
              " 'rhea',\n",
              " 'bria',\n",
              " 'thalia',\n",
              " 'keyla',\n",
              " 'haisley',\n",
              " 'ryann',\n",
              " 'addisyn',\n",
              " 'amaia',\n",
              " 'chanel',\n",
              " 'ellen',\n",
              " 'harmoni',\n",
              " 'aliana',\n",
              " 'tinsley',\n",
              " 'landry',\n",
              " 'paisleigh',\n",
              " 'lexie',\n",
              " 'myah',\n",
              " 'rylan',\n",
              " 'deborah',\n",
              " 'emilee',\n",
              " 'laylah',\n",
              " 'novalee',\n",
              " 'ellis',\n",
              " 'emmeline',\n",
              " 'avalynn',\n",
              " 'hadlee',\n",
              " 'legacy',\n",
              " 'braylee',\n",
              " 'elisabeth',\n",
              " 'kaylie',\n",
              " 'ansley',\n",
              " 'dior',\n",
              " 'paula',\n",
              " 'belen',\n",
              " 'corinne',\n",
              " 'maleah',\n",
              " 'martha',\n",
              " 'teresa',\n",
              " 'salma',\n",
              " 'louise',\n",
              " 'averi',\n",
              " 'lilianna',\n",
              " 'amiya',\n",
              " 'milena',\n",
              " 'royal',\n",
              " 'aubrielle',\n",
              " 'calliope',\n",
              " 'frankie',\n",
              " 'natasha',\n",
              " 'kamilah',\n",
              " 'meilani',\n",
              " 'raina',\n",
              " 'amayah',\n",
              " 'lailah',\n",
              " 'rayne',\n",
              " 'zaniyah',\n",
              " 'isabela',\n",
              " 'nathalie',\n",
              " 'miah',\n",
              " 'opal',\n",
              " 'kenia',\n",
              " 'azariah',\n",
              " 'hunter',\n",
              " 'tori',\n",
              " 'andi',\n",
              " 'keily',\n",
              " 'leanna',\n",
              " 'scarlette',\n",
              " 'jaelyn',\n",
              " 'saoirse',\n",
              " 'selene',\n",
              " 'dalary',\n",
              " 'lindsey',\n",
              " 'marianna',\n",
              " 'ramona',\n",
              " 'estelle',\n",
              " 'giovanna',\n",
              " 'holland',\n",
              " 'nancy',\n",
              " 'emmalynn',\n",
              " 'mylah',\n",
              " 'rosalee',\n",
              " 'sariah',\n",
              " 'zoie',\n",
              " 'blaire',\n",
              " 'lyanna',\n",
              " 'maxine',\n",
              " 'anais',\n",
              " 'dana',\n",
              " 'judith',\n",
              " 'kiera',\n",
              " 'jaelynn',\n",
              " 'noor',\n",
              " 'kai',\n",
              " 'adalee',\n",
              " 'oaklee',\n",
              " 'amaris',\n",
              " 'jaycee',\n",
              " 'belle',\n",
              " 'carolyn',\n",
              " 'della',\n",
              " 'karter',\n",
              " 'sky',\n",
              " 'treasure',\n",
              " 'vienna',\n",
              " 'jewel',\n",
              " 'rivka',\n",
              " 'rosalyn',\n",
              " 'alannah',\n",
              " 'ellianna',\n",
              " 'sunny',\n",
              " 'claudia',\n",
              " 'cara',\n",
              " 'hailee',\n",
              " 'estrella',\n",
              " 'harleigh',\n",
              " 'zhavia',\n",
              " 'alianna',\n",
              " 'brittany',\n",
              " 'jaylene',\n",
              " 'journi',\n",
              " 'marissa',\n",
              " 'mavis',\n",
              " 'iliana',\n",
              " 'jurnee',\n",
              " 'aislinn',\n",
              " 'alyson',\n",
              " 'elsa',\n",
              " 'kamiyah',\n",
              " 'kiana',\n",
              " 'lisa',\n",
              " 'arlette',\n",
              " 'kadence',\n",
              " 'kathleen',\n",
              " 'halle',\n",
              " 'erika',\n",
              " 'sylvie',\n",
              " 'adele',\n",
              " 'erica',\n",
              " 'veda',\n",
              " 'whitney',\n",
              " 'bexley',\n",
              " 'emmaline',\n",
              " 'guadalupe',\n",
              " 'august',\n",
              " 'brynleigh',\n",
              " 'gwen',\n",
              " 'promise',\n",
              " 'alisson',\n",
              " 'india',\n",
              " 'madalynn',\n",
              " 'paloma',\n",
              " 'patricia',\n",
              " 'samira',\n",
              " 'aliya',\n",
              " 'casey',\n",
              " 'jazlynn',\n",
              " 'paulina',\n",
              " 'dulce',\n",
              " 'kallie',\n",
              " 'perla',\n",
              " 'adrienne',\n",
              " 'alora',\n",
              " 'nataly',\n",
              " 'ayleen',\n",
              " 'christine',\n",
              " 'kaiya',\n",
              " 'ariadne',\n",
              " 'karlee',\n",
              " 'barbara',\n",
              " 'lillianna',\n",
              " 'raquel',\n",
              " 'saniyah',\n",
              " 'yamileth',\n",
              " 'arely',\n",
              " 'celia',\n",
              " 'heavenly',\n",
              " 'kaylin',\n",
              " 'marisol',\n",
              " 'marleigh',\n",
              " 'avalyn',\n",
              " 'berkley',\n",
              " 'kataleya',\n",
              " 'zainab',\n",
              " 'dani',\n",
              " 'egypt',\n",
              " 'joyce',\n",
              " 'kenley',\n",
              " 'annabel',\n",
              " 'kaelyn',\n",
              " 'etta',\n",
              " 'hadleigh',\n",
              " 'joselyn',\n",
              " 'luella',\n",
              " 'jaylee',\n",
              " 'zola',\n",
              " 'alisha',\n",
              " 'ezra',\n",
              " 'queen',\n",
              " 'amia',\n",
              " 'annalee',\n",
              " 'bellamy',\n",
              " 'paola',\n",
              " 'tinley',\n",
              " 'violeta',\n",
              " 'jenesis',\n",
              " 'arden',\n",
              " 'giana',\n",
              " 'wendy',\n",
              " 'ellison',\n",
              " 'florence',\n",
              " 'margo',\n",
              " 'naya',\n",
              " 'robin',\n",
              " 'sandra',\n",
              " 'scout',\n",
              " 'waverly',\n",
              " 'janessa',\n",
              " 'jayden',\n",
              " 'micah',\n",
              " 'novah',\n",
              " 'zora',\n",
              " 'ann',\n",
              " 'jana',\n",
              " 'taliyah',\n",
              " 'vada',\n",
              " 'giavanna',\n",
              " 'ingrid',\n",
              " 'valery',\n",
              " 'azaria',\n",
              " 'emmarie',\n",
              " 'esperanza',\n",
              " 'kailyn',\n",
              " 'aiyana',\n",
              " 'keilani',\n",
              " 'austyn',\n",
              " 'whitley',\n",
              " 'elina',\n",
              " 'kimora',\n",
              " 'maliah',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "fa935c21",
      "metadata": {
        "id": "fa935c21"
      },
      "outputs": [],
      "source": [
        "# Dictionaries, {idx -> ch} and {ch -> idx}\n",
        "itos = defaultdict(int)\n",
        "stoi = defaultdict(int)\n",
        "# Number of characters used to predict the target character in the MLP Language Model\n",
        "block_size = 3\n",
        "# Batch size used in MLP Language Model\n",
        "batch_size = 32\n",
        "# Embedding dimension, per character\n",
        "d_model = 10\n",
        "# Hidden dimension for RNN and also MLP Language Models\n",
        "d_h = 200\n",
        "\n",
        "# START = START token\n",
        "stoi['.'] = 0\n",
        "itos[0] = '.'\n",
        "\n",
        "count = 1\n",
        "# Loop over all names and create mappings itos and stoi mapping a unique character to a idx\n",
        "for name in names:\n",
        "  for ch in name:\n",
        "    if not ch in stoi:\n",
        "      stoi[ch] = count\n",
        "      itos[count] = ch\n",
        "      count = count + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "d556793e",
      "metadata": {
        "id": "d556793e"
      },
      "outputs": [],
      "source": [
        "assert len(stoi) == len(itos)\n",
        "vocab_size = len(stoi)\n",
        "assert vocab_size == 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3765e0fe",
      "metadata": {
        "id": "3765e0fe",
        "outputId": "f5c0d169-8595-4d19-bda8-92879f49240f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'.': 0,\n",
              "             'e': 1,\n",
              "             'm': 2,\n",
              "             'a': 3,\n",
              "             'o': 4,\n",
              "             'l': 5,\n",
              "             'i': 6,\n",
              "             'v': 7,\n",
              "             's': 8,\n",
              "             'b': 9,\n",
              "             'p': 10,\n",
              "             'h': 11,\n",
              "             'c': 12,\n",
              "             'r': 13,\n",
              "             't': 14,\n",
              "             'y': 15,\n",
              "             'n': 16,\n",
              "             'g': 17,\n",
              "             'z': 18,\n",
              "             'f': 19,\n",
              "             'd': 20,\n",
              "             'u': 21,\n",
              "             'k': 22,\n",
              "             'w': 23,\n",
              "             'q': 24,\n",
              "             'x': 25,\n",
              "             'j': 26})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "stoi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d706797e",
      "metadata": {
        "id": "d706797e"
      },
      "source": [
        "## BiGram Language Model\n",
        "- Implement the Bigram Language Model\n",
        "- Get all the relevent counts, then get the train dataset Perplexity\n",
        "- Use the class notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9cb554a5",
      "metadata": {
        "id": "9cb554a5"
      },
      "outputs": [],
      "source": [
        "# Using the formulas in class, loop over each name and get the parameters\n",
        "c1 = defaultdict(int)\n",
        "c2 = defaultdict(int)\n",
        "for name in open(names_path, 'r'):\n",
        "    # Lowercase and remove any whitespace at the end\n",
        "    name = name.lower().strip()\n",
        "    # Pad with START = '.' and STOP = '.'\n",
        "    name = '.'+name+'.'\n",
        "    # Transform to integer\n",
        "    name = [stoi[ch] for ch in name]\n",
        "    # Get the counts for Bigrams and Unigrams\n",
        "    for i in range(len(name)):\n",
        "      c1[name[i]] += 1\n",
        "\n",
        "      if i < len(name) - 1:\n",
        "        bigram = (name[i], name[i + 1])\n",
        "        c2[bigram] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "507e4525",
      "metadata": {
        "id": "507e4525",
        "outputId": "72ac648c-26eb-4738-a2f2-99c28206a9d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preplexity:  6.484745025634766\n"
          ]
        }
      ],
      "source": [
        "# Get perplexity\n",
        "\n",
        "sumneglogp = 0\n",
        "T = 0\n",
        "for name in open(names_path, 'r'):\n",
        "    # Get rid of white space and lowercase\n",
        "    name = name.lower().strip()\n",
        "    # Get the length of the word, without padding\n",
        "    T += len(name)\n",
        "    # Don't pad the STOP since we are not generating; pad with START\n",
        "    name = '.' + name\n",
        "    # Transform to integrs\n",
        "    name = [stoi[ch] for ch in name]\n",
        "    # Get the loss -log(p(name)); use that the log of the product is the sum of the logs\n",
        "    for i in range(len(name) - 1):\n",
        "      x, y = name[i], name[i + 1]\n",
        "      num = c2[(x, y)]\n",
        "      den = c1[x]\n",
        "      p = num/den\n",
        "\n",
        "      sumneglogp += -math.log(p)\n",
        "# Print the Perplexity\n",
        "print('Preplexity: ', torch.pow(2.0, torch.tensor(sumneglogp / T)).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b931b726",
      "metadata": {
        "id": "b931b726",
        "outputId": "8bdab0bd-acc4-4942-c589-665f53a79a32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated name:  donyvillukan\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# When you generate STOP, terminate\n",
        "name = '.'\n",
        "while True:\n",
        "    c = stoi[name[-1]]\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    p = []\n",
        "    for d in range(vocab_size):\n",
        "        # Use the same indicies as the dictionary we set up\n",
        "        # Populate p\n",
        "        p.append(c2[(c,d)]/c1[c])\n",
        "    #print(p)\n",
        "    assert len(p) == vocab_size\n",
        "    # Sample randmly from the probability using torch.Categorical\n",
        "    c = torch.distributions.Categorical(torch.tensor(p)).sample()\n",
        "    # Offset by 1 since we want indices [1, 2, ..., vocab_size]\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[1:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561cdacb",
      "metadata": {
        "id": "561cdacb"
      },
      "source": [
        "## MLP Language Model\n",
        "\n",
        "- Implement the MLP language model from below\n",
        "- Look at page 7, Equation (1)\n",
        "- https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982566bf",
      "metadata": {
        "id": "982566bf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fb6328a5",
      "metadata": {
        "id": "fb6328a5"
      },
      "outputs": [],
      "source": [
        "x_data = []\n",
        "y_data = []\n",
        "for name in open(names_path, 'r'):\n",
        "    name = name.lower().strip()\n",
        "    # Pad with block_size START tokens and 1 STOP token\n",
        "    name = ''.join(block_size * ['.']) + name + '.'\n",
        "    # Loop through name and get the (x, y) pairs\n",
        "    # Add (x, y) to x_data and y_data and make sure you transform to characters\n",
        "    # Make sure x_data and y_data have integers, use stoi\n",
        "    for i in range(len(name) - block_size):\n",
        "      x = name[i: i + block_size]\n",
        "      y = name[i + block_size]\n",
        "      x_data.append([stoi[ch] for ch in x])\n",
        "      y_data.append(stoi[y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3142dd13",
      "metadata": {
        "id": "3142dd13"
      },
      "outputs": [],
      "source": [
        "class MLPLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # An embedding for each character; vocab_size of them\n",
        "        self.e = nn.Embedding(vocab_size, d_model)\n",
        "        # H; should take in block_size * d_model vector and output d_h\n",
        "        self.H = nn.Linear(block_size * d_model, d_h)\n",
        "        # U; should take in d_h vector and output vocab_size\n",
        "        self.U = nn.Linear(d_h, vocab_size)\n",
        "        # W; for the skip connection, should take in block_size * d_model and output vocab_size\n",
        "        self.W = nn.Linear(block_size * d_model, vocab_size)\n",
        "\n",
        "    # x should be (batch_size, block_size)\n",
        "    def forward(self, x):\n",
        "        x = self.e(x)\n",
        "        x_flat = x.view(x.shape[0], -1)\n",
        "        hid = torch.relu(self.H(x_flat))\n",
        "        logits = self.U(hid) + self.W(x_flat)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4a4f35f7",
      "metadata": {
        "id": "4a4f35f7",
        "outputId": "d4f167a0-75e0-4e9d-d74e-427d8f0498ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 0, 0], 1)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "x_data[0], y_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "_qfL4H_Bvaxh",
        "outputId": "b0810cfa-92a6-48be-e032-46a1d600bc26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_qfL4H_Bvaxh",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "17bf7a0a",
      "metadata": {
        "id": "17bf7a0a"
      },
      "outputs": [],
      "source": [
        "# Define a dataloader with x_data and y_data with batch_size\n",
        "dl = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(\n",
        "        torch.tensor(x_data, dtype=torch.long),\n",
        "        torch.tensor(y_data, dtype=torch.long)\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a56ce82f",
      "metadata": {
        "id": "a56ce82f"
      },
      "outputs": [],
      "source": [
        "for xb, yb in dl:\n",
        "    assert xb.shape == (batch_size, 3)\n",
        "    assert yb.shape == (batch_size,)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTnVktytvRX4"
      },
      "id": "nTnVktytvRX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1013903d",
      "metadata": {
        "id": "1013903d"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "# Define the MLP model and the Adam optimizer learning rate 0.001\n",
        "model = MLPLanguageModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "0155215c",
      "metadata": {
        "id": "0155215c",
        "outputId": "31dbe7e7-e2ff-46db-fc1a-dc546190e011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.572445277929306\n",
            "2.3763574120998383\n",
            "2.3373064618110657\n",
            "2.299782624721527\n",
            "2.2885865232944487\n",
            "2.280738336801529\n",
            "2.256634145259857\n",
            "2.25579089140892\n",
            "2.242019241809845\n",
            "2.2405084068775176\n",
            "2.229731682062149\n",
            "2.226990898370743\n",
            "2.22165487742424\n",
            "2.216302121400833\n",
            "2.1857772216796874\n",
            "2.1903146004676817\n",
            "2.212893445968628\n",
            "2.1852008602619173\n",
            "2.193347140789032\n",
            "2.2103211643695833\n",
            "2.187148030757904\n",
            "2.1835943450927733\n",
            "2.205803402900696\n",
            "2.1961781134605407\n",
            "2.185334327697754\n",
            "2.1725257363319397\n",
            "2.1958053493499756\n",
            "2.1691962022781373\n",
            "2.171479551076889\n",
            "2.1658376986980437\n",
            "2.159438243150711\n",
            "2.163826595544815\n",
            "2.1650139422416688\n",
            "2.1599254631996154\n",
            "2.170836991071701\n",
            "2.1661839098930358\n",
            "2.1486204710006716\n",
            "2.1583956165313722\n",
            "2.1663399810791018\n",
            "2.157143893957138\n",
            "2.1652252457141876\n",
            "2.1700349991321564\n",
            "2.152646734237671\n",
            "2.1479042150974275\n",
            "2.1510036301612856\n",
            "2.1532086579799654\n",
            "2.1368192510604858\n",
            "2.159572172164917\n",
            "2.1601994173526764\n",
            "2.138728188991547\n",
            "2.1506101582050325\n",
            "2.131245499610901\n",
            "2.1378354580402372\n",
            "2.16137398815155\n",
            "2.142180074453354\n",
            "2.1597889442443847\n",
            "2.131930021762848\n",
            "2.144292979955673\n",
            "2.139581570625305\n",
            "2.147213311910629\n",
            "2.1325104951858522\n",
            "2.130448938369751\n",
            "2.13179612660408\n",
            "2.129883442878723\n",
            "2.144619479417801\n",
            "2.1240359869003296\n",
            "2.117329499006271\n",
            "2.1312334542274476\n",
            "2.1421110570430755\n",
            "2.134566026687622\n",
            "2.146785603761673\n",
            "2.1295109906196594\n",
            "2.1380051388740537\n",
            "2.1272030386924743\n",
            "2.1366646909713745\n",
            "2.1241371688842774\n",
            "2.109065418958664\n",
            "2.112278600931168\n",
            "2.12335927772522\n",
            "2.1272661447525025\n",
            "2.1437082540988923\n",
            "2.120581389904022\n",
            "2.1342370953559877\n",
            "2.1233732631206514\n",
            "2.139884323358536\n",
            "2.142452523946762\n",
            "2.1054422800540924\n",
            "2.1260103285312653\n",
            "2.0968564405441286\n",
            "2.118800588130951\n",
            "2.1270137057304384\n",
            "2.12980410027504\n",
            "2.139109841823578\n",
            "2.144783676147461\n",
            "2.1335051457881926\n",
            "2.1172436866760256\n",
            "2.109889006137848\n",
            "2.118740942955017\n",
            "2.1136559867858886\n",
            "2.117763039588928\n",
            "2.1064219851493835\n",
            "2.1103630335330963\n",
            "2.1147004923820494\n",
            "2.106804850578308\n",
            "2.1274009141922\n",
            "2.105584553003311\n",
            "2.122293435096741\n",
            "2.12504066324234\n",
            "2.1184379019737243\n",
            "2.118970674276352\n",
            "2.112063317298889\n",
            "2.123891642332077\n",
            "2.1242851014137267\n",
            "2.1195916402339936\n",
            "2.117923929691315\n",
            "2.12354745388031\n",
            "2.115876527070999\n",
            "2.122831448554993\n",
            "2.099418961524963\n",
            "2.1064610431194306\n",
            "2.1260001311302186\n",
            "2.103545019626617\n",
            "2.115409077644348\n",
            "2.1182097465991974\n",
            "2.0959762647151945\n",
            "2.1152970888614653\n",
            "2.116378306150436\n",
            "2.105866453886032\n",
            "2.1040424966812132\n",
            "2.0913712491989136\n",
            "2.0969697871208193\n",
            "2.1142067012786865\n",
            "2.1078147060871126\n",
            "2.119303212404251\n",
            "2.103754528045654\n",
            "2.0936116967201235\n",
            "2.1049360601902007\n",
            "2.110017761230469\n",
            "2.108984339952469\n",
            "2.1138594613075257\n",
            "2.116300145149231\n",
            "2.119395366668701\n",
            "2.096491411924362\n",
            "2.1059890105724333\n",
            "2.097071665763855\n",
            "2.1061083941459655\n",
            "2.1138450701236726\n",
            "2.108883305311203\n",
            "2.1043885867595673\n",
            "2.1064285979270934\n",
            "2.1042123186588286\n",
            "2.1008434205055235\n",
            "2.1150978717803954\n",
            "2.106221546649933\n",
            "2.103900429010391\n",
            "2.11245827460289\n",
            "2.0974675760269164\n",
            "2.09403884601593\n",
            "2.103557590961456\n",
            "2.090751068353653\n",
            "2.109498584270477\n",
            "2.1046315248012544\n",
            "2.098410944223404\n",
            "2.085318161725998\n",
            "2.0884524896144865\n",
            "2.1076017866134644\n",
            "2.0812011122703553\n",
            "2.105868562221527\n",
            "2.1102649760246277\n",
            "2.113632753133774\n",
            "2.1249533643722534\n",
            "2.0772998089790344\n",
            "2.0885904176235197\n",
            "2.109830159664154\n",
            "2.093755258321762\n",
            "2.093504023551941\n",
            "2.1039442429542543\n",
            "2.107540448188782\n",
            "2.0967990846633913\n",
            "2.1084434111118315\n",
            "2.1041541669368744\n",
            "2.099707280397415\n",
            "2.1115660762786863\n",
            "2.0952719631195067\n",
            "2.104806165218353\n",
            "2.0890975174903867\n",
            "2.095561184167862\n",
            "2.070721781253815\n",
            "2.1007592928409577\n",
            "2.099220299243927\n",
            "2.0839026691913607\n",
            "2.0934357957839964\n",
            "2.094194710969925\n",
            "2.104024914741516\n",
            "2.086691574573517\n",
            "2.1082521426677703\n",
            "2.0985239481925966\n",
            "2.108476687192917\n",
            "2.095303127527237\n",
            "2.110971991300583\n",
            "2.08625580906868\n",
            "2.0997319915294645\n",
            "2.096781383037567\n",
            "2.0855103530883787\n",
            "2.0946217570304873\n",
            "2.0895687384605406\n",
            "2.1017249932289124\n",
            "2.0850649237632752\n",
            "2.0918355724811555\n",
            "2.0960875642299652\n",
            "2.0897337007522583\n",
            "2.1098901300430297\n",
            "2.0968226833343504\n",
            "2.094849896907806\n",
            "2.0968199622631074\n",
            "2.100505564451218\n",
            "2.0802384996414185\n",
            "2.09882528591156\n",
            "2.111515172958374\n",
            "2.0906991686820984\n",
            "2.0914736306667328\n",
            "2.083085122346878\n",
            "2.1030365719795228\n",
            "2.0894908511638643\n",
            "2.0986852896213533\n",
            "2.0919478375911713\n",
            "2.0887836980819703\n",
            "2.081837010383606\n",
            "2.062836972475052\n",
            "2.0950126278400423\n",
            "2.091001850605011\n",
            "2.087184546470642\n",
            "2.078919241428375\n",
            "2.0874315252304076\n",
            "2.1004462954998018\n",
            "2.1042522585391996\n",
            "2.0923245124816896\n",
            "2.088723332166672\n",
            "2.0956390163898466\n",
            "2.0955237164497373\n",
            "2.0888931345939636\n",
            "2.0931219379901886\n",
            "2.0742691309452055\n",
            "2.0708885340690615\n",
            "2.105822070837021\n",
            "2.089138389348984\n",
            "2.0916495184898376\n",
            "2.0936740684509276\n",
            "2.0911162641048433\n",
            "2.0796719489097595\n",
            "2.0901946880817412\n",
            "2.0865223858356474\n",
            "2.084357660293579\n",
            "2.1054209802150727\n",
            "2.093790743112564\n",
            "2.095459867954254\n",
            "2.0897495300769804\n",
            "2.0774680454730987\n",
            "2.0992676668167114\n",
            "2.072530827522278\n",
            "2.0838621525764465\n",
            "2.0677184948921203\n",
            "2.088153699398041\n",
            "2.080529732942581\n",
            "2.1020698859691618\n",
            "2.0659750273227693\n",
            "2.089526858091354\n",
            "2.104783232688904\n",
            "2.093716230869293\n",
            "2.0889449121952057\n",
            "2.103089495897293\n",
            "2.0760784537792207\n",
            "2.0852146611213684\n",
            "2.0859324083328246\n",
            "2.08612345290184\n",
            "2.0872309103012086\n",
            "2.0925906059741974\n",
            "2.084485711812973\n",
            "2.0871272296905516\n",
            "2.0768211991786956\n",
            "2.1047674734592436\n",
            "2.08381064248085\n",
            "2.09225212264061\n",
            "2.0846089284420013\n",
            "2.078574028491974\n"
          ]
        }
      ],
      "source": [
        "total_loss = 0\n",
        "total_ct = 0\n",
        "total_epochs = 20\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "for _ in range(total_epochs):\n",
        "    for xb, yb in dl:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the logits\n",
        "        logits = model(xb)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(logits, yb)\n",
        "\n",
        "        # Get the new gradient\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to max norm 0.1\n",
        "        # Use clid_grad_norm from torch\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "\n",
        "        # Do a gradient update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the loss for the batch and get the number of batches\n",
        "        total_loss += loss.item()\n",
        "        total_ct += 1\n",
        "\n",
        "        # Print the loss\n",
        "        if total_ct and total_ct % 500 == 0:\n",
        "            print(total_loss / total_ct)\n",
        "            total_loss = 0\n",
        "            total_ct = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "4ecf5fb3",
      "metadata": {
        "id": "4ecf5fb3",
        "outputId": "3cab52f8-b0c4-4919-bd42-a3df312c0b0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preplexity:  9.308974266052246\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    # Get perplexity\n",
        "    sumneglogp = 0\n",
        "    T = 0\n",
        "    for name in open(names_path, 'r'):\n",
        "        name = name.lower().strip()\n",
        "        T += len(name)\n",
        "        # Pad with block_size START tokens\n",
        "        name = ''.join(['.'] * block_size) + name\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        # Gather all the terms over the loss\n",
        "        # Notice that we compute -log p(...abc)\n",
        "        # Which is -log p(a | ...) - log p(b | a..) - log p(c | ba.)\n",
        "        for i in range(len(name) - block_size):\n",
        "            x = name[i:i+block_size]\n",
        "            y = name[i+block_size]\n",
        "            x_data.append([stoi[ch] for ch in x])\n",
        "            y_data.append(stoi[y])\n",
        "        # Gather the loss over the name, for each term\n",
        "        # You need to get the softmax loss for each term\n",
        "        # Can either use the CrossEntropyLoss or do this manually\n",
        "        # Compute the loss\n",
        "        xb = torch.tensor(x_data, dtype=torch.long).to(device)\n",
        "        yb = torch.tensor(y_data, dtype=torch.long).to(device)\n",
        "        logits = model(xb)\n",
        "\n",
        "        # Use reduction \"sum\" so you don't need to worry about N\n",
        "        loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")(logits, yb)\n",
        "\n",
        "        # Change to log base 2\n",
        "        loss *= (1.0 / torch.log(torch.tensor(2.0)))\n",
        "\n",
        "        sumneglogp += loss\n",
        "\n",
        "    print('Preplexity: ', torch.pow(2.0, sumneglogp.clone().detach() / T).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e77131c8",
      "metadata": {
        "id": "e77131c8",
        "outputId": "8d319c7f-0b20-40e9-db1e-b01ebdea1a37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated name:  keymaelyn\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# When you generate STOP, terminate\n",
        "name = ''.join(block_size * ['.'])\n",
        "while True:\n",
        "    # Get the idx\n",
        "    c = torch.tensor([stoi[ch] for ch in name[-block_size:]], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    logits = model(c)\n",
        "    p = torch.softmax(logits, dim=1)\n",
        "    # Randomly sample from p a new character\n",
        "    c = torch.distributions.Categorical(p).sample()\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[block_size:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1b01ed",
      "metadata": {
        "id": "bb1b01ed"
      },
      "source": [
        "## RNN Language Model\n",
        "- For each name, run an RNN character by character\n",
        "- Use the recursion x = Tanh()(Wh @ h + Wx @ x + bh + bx) and y = Softmax()(Wy h + by)\n",
        "- Do not use the RNN Cell from PyTorch, do this manually as hinted below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c765e7fc",
      "metadata": {
        "id": "c765e7fc"
      },
      "outputs": [],
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Each token has an embedding of size vocab_size\n",
        "        self.e = FILL_IN\n",
        "        # Wh used to map hidden to hidden\n",
        "        self.Wh = FILL_IN\n",
        "        self.Wx = FILL_IN\n",
        "        self.Wy = FILL_IN\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Run through to get the embedding for the token\n",
        "        # The embedding per token is the feature vector x  we pass into the\n",
        "        # Represent x as an embedding\n",
        "        x = FILL_IN\n",
        "        # Get the hidden state\n",
        "        h = FILL_IN\n",
        "        # Get the logits we use to predict y\n",
        "        z = FILL_IN\n",
        "        # Return the z predicting y for the timestep we are at and the next hidden state\n",
        "        return z, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd798358",
      "metadata": {
        "id": "cd798358"
      },
      "outputs": [],
      "source": [
        "model = RNNLanguageModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f3a49ff",
      "metadata": {
        "id": "7f3a49ff"
      },
      "outputs": [],
      "source": [
        "total_loss = 0\n",
        "total_ct = 0\n",
        "total_epochs = 5\n",
        "\n",
        "for _ in range(total_epochs):\n",
        "    for name in open('names.txt', 'r'):\n",
        "        name = name.lower().strip()\n",
        "        # Add the start and end padding token\n",
        "        name = '.' + name + '.'\n",
        "        # name[:-1]\n",
        "        x_data = FILL_IN\n",
        "        # name[1:]\n",
        "        y_data = FILL_IN\n",
        "        logits = []\n",
        "        # Set the hidden state to random\n",
        "        h = FILL_IN\n",
        "        # Zero the grad\n",
        "        FILL_IN\n",
        "\n",
        "        # Loop through each token and get the new h and then pass it forward\n",
        "        # Accumulate all the logits\n",
        "        for x in x_data:\n",
        "            FILL_IN\n",
        "            FILL_IN\n",
        "\n",
        "        # Put all the logits into one tensor\n",
        "        logits = FILL_IN\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = FILL_IN\n",
        "\n",
        "        # Get the new gradient\n",
        "        FILL_IN\n",
        "\n",
        "        # Clip the gradients at max norm 0.1\n",
        "        FILL_IN\n",
        "\n",
        "        # Do a gradient update\n",
        "        FILL_IN\n",
        "\n",
        "        # Get the loss for the batch and get the number of batches\n",
        "        total_loss += loss.item()\n",
        "        total_ct += 1\n",
        "\n",
        "        if total_ct and total_ct % 100 == 0:\n",
        "            print(total_loss / total_ct)\n",
        "            total_loss = 0\n",
        "            total_ct = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f5f990",
      "metadata": {
        "id": "83f5f990"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Get perplexity\n",
        "    sumneglogp = 0\n",
        "    T = 0\n",
        "    for name in open('names.txt', 'r'):\n",
        "        name = name.lower().strip()\n",
        "        T += len(name)\n",
        "        name = '.' + name\n",
        "        # Get the name from index 0 to -1 exclusive end\n",
        "        x_data = FILL_IN\n",
        "        # Get the y from index 1 to end inclusive end\n",
        "        y_data = FILL_IN\n",
        "        # logits per token prediction\n",
        "        logits = []\n",
        "        # Initialize the h vector to random\n",
        "        h = FILL_IN\n",
        "        # Loop over each chracter in the name and pass h and this into the RNN\n",
        "        # Get the new logit\n",
        "        for x in x_data:\n",
        "            # Get the int for x\n",
        "            x = FILL_IN\n",
        "            # Get z and h\n",
        "            z, h = FILL_IN\n",
        "            # Append to logit\n",
        "            FILL_IN\n",
        "\n",
        "        # Get all the logits for each character\n",
        "        logits = FILL_IN\n",
        "\n",
        "        # Compute the loss across all characters\n",
        "        loss = FILL_IN\n",
        "\n",
        "        # Change to log base 2\n",
        "        # log2(x) = ln(x) / ln(2)\n",
        "        loss *= FILL_IN\n",
        "\n",
        "        sumneglogp += FILL_IN\n",
        "\n",
        "    # sumneglogp is -log(p('.' + name1)) -log(p('.' + name2)) -log(p('.' + name3)) ...\n",
        "    # Divide by the appropriate term to get the answer we want\n",
        "    print('Preplexity: ', torch.pow(sumneglogp.clone().detach() / T , 2).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7608536d",
      "metadata": {
        "id": "7608536d"
      },
      "outputs": [],
      "source": [
        "# Generate a random word using this distributon\n",
        "# Intialize the word with\n",
        "name = '.'\n",
        "# Initialize h to random\n",
        "h = FILL_IN\n",
        "while True:\n",
        "    # Make c to an integer\n",
        "    c = FILL_IN\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    logits, h = FILL_IN\n",
        "    # Get p; use Softmax\n",
        "    p = FILL_IN\n",
        "    # Sample from p\n",
        "    c = FILL_IN\n",
        "    # If we generate '.', stop\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d790024",
      "metadata": {
        "id": "4d790024"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}