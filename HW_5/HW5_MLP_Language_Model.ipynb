{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "624b39df",
      "metadata": {
        "id": "624b39df"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import tqdm\n",
        "import math\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "939a704e",
      "metadata": {
        "id": "939a704e"
      },
      "outputs": [],
      "source": [
        "FILL_IN = \"FILL_IN\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "names = []\n",
        "names_path = '/content/drive/MyDrive/ADL_HW/names.txt'\n",
        "with open(names_path, 'r') as f:\n",
        "    names = f.read().splitlines()"
      ],
      "metadata": {
        "id": "ENGCaduhcDV4",
        "outputId": "efff105b-8e96-4efa-e3fb-b5b0d8d2a57b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ENGCaduhcDV4",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names"
      ],
      "metadata": {
        "id": "qJ79WW1Uc4Of",
        "outputId": "24cf950d-2747-425b-af49-e6b387ade509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qJ79WW1Uc4Of",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn',\n",
              " 'abigail',\n",
              " 'emily',\n",
              " 'elizabeth',\n",
              " 'mila',\n",
              " 'ella',\n",
              " 'avery',\n",
              " 'sofia',\n",
              " 'camila',\n",
              " 'aria',\n",
              " 'scarlett',\n",
              " 'victoria',\n",
              " 'madison',\n",
              " 'luna',\n",
              " 'grace',\n",
              " 'chloe',\n",
              " 'penelope',\n",
              " 'layla',\n",
              " 'riley',\n",
              " 'zoey',\n",
              " 'nora',\n",
              " 'lily',\n",
              " 'eleanor',\n",
              " 'hannah',\n",
              " 'lillian',\n",
              " 'addison',\n",
              " 'aubrey',\n",
              " 'ellie',\n",
              " 'stella',\n",
              " 'natalie',\n",
              " 'zoe',\n",
              " 'leah',\n",
              " 'hazel',\n",
              " 'violet',\n",
              " 'aurora',\n",
              " 'savannah',\n",
              " 'audrey',\n",
              " 'brooklyn',\n",
              " 'bella',\n",
              " 'claire',\n",
              " 'skylar',\n",
              " 'lucy',\n",
              " 'paisley',\n",
              " 'everly',\n",
              " 'anna',\n",
              " 'caroline',\n",
              " 'nova',\n",
              " 'genesis',\n",
              " 'emilia',\n",
              " 'kennedy',\n",
              " 'samantha',\n",
              " 'maya',\n",
              " 'willow',\n",
              " 'kinsley',\n",
              " 'naomi',\n",
              " 'aaliyah',\n",
              " 'elena',\n",
              " 'sarah',\n",
              " 'ariana',\n",
              " 'allison',\n",
              " 'gabriella',\n",
              " 'alice',\n",
              " 'madelyn',\n",
              " 'cora',\n",
              " 'ruby',\n",
              " 'eva',\n",
              " 'serenity',\n",
              " 'autumn',\n",
              " 'adeline',\n",
              " 'hailey',\n",
              " 'gianna',\n",
              " 'valentina',\n",
              " 'isla',\n",
              " 'eliana',\n",
              " 'quinn',\n",
              " 'nevaeh',\n",
              " 'ivy',\n",
              " 'sadie',\n",
              " 'piper',\n",
              " 'lydia',\n",
              " 'alexa',\n",
              " 'josephine',\n",
              " 'emery',\n",
              " 'julia',\n",
              " 'delilah',\n",
              " 'arianna',\n",
              " 'vivian',\n",
              " 'kaylee',\n",
              " 'sophie',\n",
              " 'brielle',\n",
              " 'madeline',\n",
              " 'peyton',\n",
              " 'rylee',\n",
              " 'clara',\n",
              " 'hadley',\n",
              " 'melanie',\n",
              " 'mackenzie',\n",
              " 'reagan',\n",
              " 'adalynn',\n",
              " 'liliana',\n",
              " 'aubree',\n",
              " 'jade',\n",
              " 'katherine',\n",
              " 'isabelle',\n",
              " 'natalia',\n",
              " 'raelynn',\n",
              " 'maria',\n",
              " 'athena',\n",
              " 'ximena',\n",
              " 'arya',\n",
              " 'leilani',\n",
              " 'taylor',\n",
              " 'faith',\n",
              " 'rose',\n",
              " 'kylie',\n",
              " 'alexandra',\n",
              " 'mary',\n",
              " 'margaret',\n",
              " 'lyla',\n",
              " 'ashley',\n",
              " 'amaya',\n",
              " 'eliza',\n",
              " 'brianna',\n",
              " 'bailey',\n",
              " 'andrea',\n",
              " 'khloe',\n",
              " 'jasmine',\n",
              " 'melody',\n",
              " 'iris',\n",
              " 'isabel',\n",
              " 'norah',\n",
              " 'annabelle',\n",
              " 'valeria',\n",
              " 'emerson',\n",
              " 'adalyn',\n",
              " 'ryleigh',\n",
              " 'eden',\n",
              " 'emersyn',\n",
              " 'anastasia',\n",
              " 'kayla',\n",
              " 'alyssa',\n",
              " 'juliana',\n",
              " 'charlie',\n",
              " 'esther',\n",
              " 'ariel',\n",
              " 'cecilia',\n",
              " 'valerie',\n",
              " 'alina',\n",
              " 'molly',\n",
              " 'reese',\n",
              " 'aliyah',\n",
              " 'lilly',\n",
              " 'parker',\n",
              " 'finley',\n",
              " 'morgan',\n",
              " 'sydney',\n",
              " 'jordyn',\n",
              " 'eloise',\n",
              " 'trinity',\n",
              " 'daisy',\n",
              " 'kimberly',\n",
              " 'lauren',\n",
              " 'genevieve',\n",
              " 'sara',\n",
              " 'arabella',\n",
              " 'harmony',\n",
              " 'elise',\n",
              " 'remi',\n",
              " 'teagan',\n",
              " 'alexis',\n",
              " 'london',\n",
              " 'sloane',\n",
              " 'laila',\n",
              " 'lucia',\n",
              " 'diana',\n",
              " 'juliette',\n",
              " 'sienna',\n",
              " 'elliana',\n",
              " 'londyn',\n",
              " 'ayla',\n",
              " 'callie',\n",
              " 'gracie',\n",
              " 'josie',\n",
              " 'amara',\n",
              " 'jocelyn',\n",
              " 'daniela',\n",
              " 'everleigh',\n",
              " 'mya',\n",
              " 'rachel',\n",
              " 'summer',\n",
              " 'alana',\n",
              " 'brooke',\n",
              " 'alaina',\n",
              " 'mckenzie',\n",
              " 'catherine',\n",
              " 'amy',\n",
              " 'presley',\n",
              " 'journee',\n",
              " 'rosalie',\n",
              " 'ember',\n",
              " 'brynlee',\n",
              " 'rowan',\n",
              " 'joanna',\n",
              " 'paige',\n",
              " 'rebecca',\n",
              " 'ana',\n",
              " 'sawyer',\n",
              " 'mariah',\n",
              " 'nicole',\n",
              " 'brooklynn',\n",
              " 'payton',\n",
              " 'marley',\n",
              " 'fiona',\n",
              " 'georgia',\n",
              " 'lila',\n",
              " 'harley',\n",
              " 'adelyn',\n",
              " 'alivia',\n",
              " 'noelle',\n",
              " 'gemma',\n",
              " 'vanessa',\n",
              " 'journey',\n",
              " 'makayla',\n",
              " 'angelina',\n",
              " 'adaline',\n",
              " 'catalina',\n",
              " 'alayna',\n",
              " 'julianna',\n",
              " 'leila',\n",
              " 'lola',\n",
              " 'adriana',\n",
              " 'june',\n",
              " 'juliet',\n",
              " 'jayla',\n",
              " 'river',\n",
              " 'tessa',\n",
              " 'lia',\n",
              " 'dakota',\n",
              " 'delaney',\n",
              " 'selena',\n",
              " 'blakely',\n",
              " 'ada',\n",
              " 'camille',\n",
              " 'zara',\n",
              " 'malia',\n",
              " 'hope',\n",
              " 'samara',\n",
              " 'vera',\n",
              " 'mckenna',\n",
              " 'briella',\n",
              " 'izabella',\n",
              " 'hayden',\n",
              " 'raegan',\n",
              " 'michelle',\n",
              " 'angela',\n",
              " 'ruth',\n",
              " 'freya',\n",
              " 'kamila',\n",
              " 'vivienne',\n",
              " 'aspen',\n",
              " 'olive',\n",
              " 'kendall',\n",
              " 'elaina',\n",
              " 'thea',\n",
              " 'kali',\n",
              " 'destiny',\n",
              " 'amiyah',\n",
              " 'evangeline',\n",
              " 'cali',\n",
              " 'blake',\n",
              " 'elsie',\n",
              " 'juniper',\n",
              " 'alexandria',\n",
              " 'myla',\n",
              " 'ariella',\n",
              " 'kate',\n",
              " 'mariana',\n",
              " 'lilah',\n",
              " 'charlee',\n",
              " 'daleyza',\n",
              " 'nyla',\n",
              " 'jane',\n",
              " 'maggie',\n",
              " 'zuri',\n",
              " 'aniyah',\n",
              " 'lucille',\n",
              " 'leia',\n",
              " 'melissa',\n",
              " 'adelaide',\n",
              " 'amina',\n",
              " 'giselle',\n",
              " 'lena',\n",
              " 'camilla',\n",
              " 'miriam',\n",
              " 'millie',\n",
              " 'brynn',\n",
              " 'gabrielle',\n",
              " 'sage',\n",
              " 'annie',\n",
              " 'logan',\n",
              " 'lilliana',\n",
              " 'haven',\n",
              " 'jessica',\n",
              " 'kaia',\n",
              " 'magnolia',\n",
              " 'amira',\n",
              " 'adelynn',\n",
              " 'makenzie',\n",
              " 'stephanie',\n",
              " 'nina',\n",
              " 'phoebe',\n",
              " 'arielle',\n",
              " 'evie',\n",
              " 'lyric',\n",
              " 'alessandra',\n",
              " 'gabriela',\n",
              " 'paislee',\n",
              " 'raelyn',\n",
              " 'madilyn',\n",
              " 'paris',\n",
              " 'makenna',\n",
              " 'kinley',\n",
              " 'gracelyn',\n",
              " 'talia',\n",
              " 'maeve',\n",
              " 'rylie',\n",
              " 'kiara',\n",
              " 'evelynn',\n",
              " 'brinley',\n",
              " 'jacqueline',\n",
              " 'laura',\n",
              " 'gracelynn',\n",
              " 'lexi',\n",
              " 'ariah',\n",
              " 'fatima',\n",
              " 'jennifer',\n",
              " 'kehlani',\n",
              " 'alani',\n",
              " 'ariyah',\n",
              " 'luciana',\n",
              " 'allie',\n",
              " 'heidi',\n",
              " 'maci',\n",
              " 'phoenix',\n",
              " 'felicity',\n",
              " 'joy',\n",
              " 'kenzie',\n",
              " 'veronica',\n",
              " 'margot',\n",
              " 'addilyn',\n",
              " 'lana',\n",
              " 'cassidy',\n",
              " 'remington',\n",
              " 'saylor',\n",
              " 'ryan',\n",
              " 'keira',\n",
              " 'harlow',\n",
              " 'miranda',\n",
              " 'angel',\n",
              " 'amanda',\n",
              " 'daniella',\n",
              " 'royalty',\n",
              " 'gwendolyn',\n",
              " 'ophelia',\n",
              " 'heaven',\n",
              " 'jordan',\n",
              " 'madeleine',\n",
              " 'esmeralda',\n",
              " 'kira',\n",
              " 'miracle',\n",
              " 'elle',\n",
              " 'amari',\n",
              " 'danielle',\n",
              " 'daphne',\n",
              " 'willa',\n",
              " 'haley',\n",
              " 'gia',\n",
              " 'kaitlyn',\n",
              " 'oakley',\n",
              " 'kailani',\n",
              " 'winter',\n",
              " 'alicia',\n",
              " 'serena',\n",
              " 'nadia',\n",
              " 'aviana',\n",
              " 'demi',\n",
              " 'jada',\n",
              " 'braelynn',\n",
              " 'dylan',\n",
              " 'ainsley',\n",
              " 'alison',\n",
              " 'camryn',\n",
              " 'avianna',\n",
              " 'bianca',\n",
              " 'skyler',\n",
              " 'scarlet',\n",
              " 'maddison',\n",
              " 'nylah',\n",
              " 'sarai',\n",
              " 'regina',\n",
              " 'dahlia',\n",
              " 'nayeli',\n",
              " 'raven',\n",
              " 'helen',\n",
              " 'adrianna',\n",
              " 'averie',\n",
              " 'skye',\n",
              " 'kelsey',\n",
              " 'tatum',\n",
              " 'kensley',\n",
              " 'maliyah',\n",
              " 'erin',\n",
              " 'viviana',\n",
              " 'jenna',\n",
              " 'anaya',\n",
              " 'carolina',\n",
              " 'shelby',\n",
              " 'sabrina',\n",
              " 'mikayla',\n",
              " 'annalise',\n",
              " 'octavia',\n",
              " 'lennon',\n",
              " 'blair',\n",
              " 'carmen',\n",
              " 'yaretzi',\n",
              " 'kennedi',\n",
              " 'mabel',\n",
              " 'zariah',\n",
              " 'kyla',\n",
              " 'christina',\n",
              " 'selah',\n",
              " 'celeste',\n",
              " 'eve',\n",
              " 'mckinley',\n",
              " 'milani',\n",
              " 'frances',\n",
              " 'jimena',\n",
              " 'kylee',\n",
              " 'leighton',\n",
              " 'katie',\n",
              " 'aitana',\n",
              " 'kayleigh',\n",
              " 'sierra',\n",
              " 'kathryn',\n",
              " 'rosemary',\n",
              " 'jolene',\n",
              " 'alondra',\n",
              " 'elisa',\n",
              " 'helena',\n",
              " 'charleigh',\n",
              " 'hallie',\n",
              " 'lainey',\n",
              " 'avah',\n",
              " 'jazlyn',\n",
              " 'kamryn',\n",
              " 'mira',\n",
              " 'cheyenne',\n",
              " 'francesca',\n",
              " 'antonella',\n",
              " 'wren',\n",
              " 'chelsea',\n",
              " 'amber',\n",
              " 'emory',\n",
              " 'lorelei',\n",
              " 'nia',\n",
              " 'abby',\n",
              " 'april',\n",
              " 'emelia',\n",
              " 'carter',\n",
              " 'aylin',\n",
              " 'cataleya',\n",
              " 'bethany',\n",
              " 'marlee',\n",
              " 'carly',\n",
              " 'kaylani',\n",
              " 'emely',\n",
              " 'liana',\n",
              " 'madelynn',\n",
              " 'cadence',\n",
              " 'matilda',\n",
              " 'sylvia',\n",
              " 'myra',\n",
              " 'fernanda',\n",
              " 'oaklyn',\n",
              " 'elianna',\n",
              " 'hattie',\n",
              " 'dayana',\n",
              " 'kendra',\n",
              " 'maisie',\n",
              " 'malaysia',\n",
              " 'kara',\n",
              " 'katelyn',\n",
              " 'maia',\n",
              " 'celine',\n",
              " 'cameron',\n",
              " 'renata',\n",
              " 'jayleen',\n",
              " 'charli',\n",
              " 'emmalyn',\n",
              " 'holly',\n",
              " 'azalea',\n",
              " 'leona',\n",
              " 'alejandra',\n",
              " 'bristol',\n",
              " 'collins',\n",
              " 'imani',\n",
              " 'meadow',\n",
              " 'alexia',\n",
              " 'edith',\n",
              " 'kaydence',\n",
              " 'leslie',\n",
              " 'lilith',\n",
              " 'kora',\n",
              " 'aisha',\n",
              " 'meredith',\n",
              " 'danna',\n",
              " 'wynter',\n",
              " 'emberly',\n",
              " 'julieta',\n",
              " 'michaela',\n",
              " 'alayah',\n",
              " 'jemma',\n",
              " 'reign',\n",
              " 'colette',\n",
              " 'kaliyah',\n",
              " 'elliott',\n",
              " 'johanna',\n",
              " 'remy',\n",
              " 'sutton',\n",
              " 'emmy',\n",
              " 'virginia',\n",
              " 'briana',\n",
              " 'oaklynn',\n",
              " 'adelina',\n",
              " 'everlee',\n",
              " 'megan',\n",
              " 'angelica',\n",
              " 'justice',\n",
              " 'mariam',\n",
              " 'khaleesi',\n",
              " 'macie',\n",
              " 'karsyn',\n",
              " 'alanna',\n",
              " 'aleah',\n",
              " 'mae',\n",
              " 'mallory',\n",
              " 'esme',\n",
              " 'skyla',\n",
              " 'madilynn',\n",
              " 'charley',\n",
              " 'allyson',\n",
              " 'hanna',\n",
              " 'shiloh',\n",
              " 'henley',\n",
              " 'macy',\n",
              " 'maryam',\n",
              " 'ivanna',\n",
              " 'ashlynn',\n",
              " 'lorelai',\n",
              " 'amora',\n",
              " 'ashlyn',\n",
              " 'sasha',\n",
              " 'baylee',\n",
              " 'beatrice',\n",
              " 'itzel',\n",
              " 'priscilla',\n",
              " 'marie',\n",
              " 'jayda',\n",
              " 'liberty',\n",
              " 'rory',\n",
              " 'alessia',\n",
              " 'alaia',\n",
              " 'janelle',\n",
              " 'kalani',\n",
              " 'gloria',\n",
              " 'sloan',\n",
              " 'dorothy',\n",
              " 'greta',\n",
              " 'julie',\n",
              " 'zahra',\n",
              " 'savanna',\n",
              " 'annabella',\n",
              " 'poppy',\n",
              " 'amalia',\n",
              " 'zaylee',\n",
              " 'cecelia',\n",
              " 'coraline',\n",
              " 'kimber',\n",
              " 'emmie',\n",
              " 'anne',\n",
              " 'karina',\n",
              " 'kassidy',\n",
              " 'kynlee',\n",
              " 'monroe',\n",
              " 'anahi',\n",
              " 'jaliyah',\n",
              " 'jazmin',\n",
              " 'maren',\n",
              " 'monica',\n",
              " 'siena',\n",
              " 'marilyn',\n",
              " 'reyna',\n",
              " 'kyra',\n",
              " 'lilian',\n",
              " 'jamie',\n",
              " 'melany',\n",
              " 'alaya',\n",
              " 'ariya',\n",
              " 'kelly',\n",
              " 'rosie',\n",
              " 'adley',\n",
              " 'dream',\n",
              " 'jaylah',\n",
              " 'laurel',\n",
              " 'jazmine',\n",
              " 'mina',\n",
              " 'karla',\n",
              " 'bailee',\n",
              " 'aubrie',\n",
              " 'katalina',\n",
              " 'melina',\n",
              " 'harlee',\n",
              " 'elliot',\n",
              " 'hayley',\n",
              " 'elaine',\n",
              " 'karen',\n",
              " 'dallas',\n",
              " 'irene',\n",
              " 'lylah',\n",
              " 'ivory',\n",
              " 'chaya',\n",
              " 'rosa',\n",
              " 'aleena',\n",
              " 'braelyn',\n",
              " 'nola',\n",
              " 'alma',\n",
              " 'leyla',\n",
              " 'pearl',\n",
              " 'addyson',\n",
              " 'roselyn',\n",
              " 'lacey',\n",
              " 'lennox',\n",
              " 'reina',\n",
              " 'aurelia',\n",
              " 'noa',\n",
              " 'janiyah',\n",
              " 'jessie',\n",
              " 'madisyn',\n",
              " 'saige',\n",
              " 'alia',\n",
              " 'tiana',\n",
              " 'astrid',\n",
              " 'cassandra',\n",
              " 'kyleigh',\n",
              " 'romina',\n",
              " 'stevie',\n",
              " 'haylee',\n",
              " 'zelda',\n",
              " 'lillie',\n",
              " 'aileen',\n",
              " 'brylee',\n",
              " 'eileen',\n",
              " 'yara',\n",
              " 'ensley',\n",
              " 'lauryn',\n",
              " 'giuliana',\n",
              " 'livia',\n",
              " 'anya',\n",
              " 'mikaela',\n",
              " 'palmer',\n",
              " 'lyra',\n",
              " 'mara',\n",
              " 'marina',\n",
              " 'kailey',\n",
              " 'liv',\n",
              " 'clementine',\n",
              " 'kenna',\n",
              " 'briar',\n",
              " 'emerie',\n",
              " 'galilea',\n",
              " 'tiffany',\n",
              " 'bonnie',\n",
              " 'elyse',\n",
              " 'cynthia',\n",
              " 'frida',\n",
              " 'kinslee',\n",
              " 'tatiana',\n",
              " 'joelle',\n",
              " 'armani',\n",
              " 'jolie',\n",
              " 'nalani',\n",
              " 'rayna',\n",
              " 'yareli',\n",
              " 'meghan',\n",
              " 'rebekah',\n",
              " 'addilynn',\n",
              " 'faye',\n",
              " 'zariyah',\n",
              " 'lea',\n",
              " 'aliza',\n",
              " 'julissa',\n",
              " 'lilyana',\n",
              " 'anika',\n",
              " 'kairi',\n",
              " 'aniya',\n",
              " 'noemi',\n",
              " 'angie',\n",
              " 'crystal',\n",
              " 'bridget',\n",
              " 'ari',\n",
              " 'davina',\n",
              " 'amelie',\n",
              " 'amirah',\n",
              " 'annika',\n",
              " 'elora',\n",
              " 'xiomara',\n",
              " 'linda',\n",
              " 'hana',\n",
              " 'laney',\n",
              " 'mercy',\n",
              " 'hadassah',\n",
              " 'madalyn',\n",
              " 'louisa',\n",
              " 'simone',\n",
              " 'kori',\n",
              " 'jillian',\n",
              " 'alena',\n",
              " 'malaya',\n",
              " 'miley',\n",
              " 'milan',\n",
              " 'sariyah',\n",
              " 'malani',\n",
              " 'clarissa',\n",
              " 'nala',\n",
              " 'princess',\n",
              " 'amani',\n",
              " 'analia',\n",
              " 'estella',\n",
              " 'milana',\n",
              " 'aya',\n",
              " 'chana',\n",
              " 'jayde',\n",
              " 'tenley',\n",
              " 'zaria',\n",
              " 'itzayana',\n",
              " 'penny',\n",
              " 'ailani',\n",
              " 'lara',\n",
              " 'aubriella',\n",
              " 'clare',\n",
              " 'lina',\n",
              " 'rhea',\n",
              " 'bria',\n",
              " 'thalia',\n",
              " 'keyla',\n",
              " 'haisley',\n",
              " 'ryann',\n",
              " 'addisyn',\n",
              " 'amaia',\n",
              " 'chanel',\n",
              " 'ellen',\n",
              " 'harmoni',\n",
              " 'aliana',\n",
              " 'tinsley',\n",
              " 'landry',\n",
              " 'paisleigh',\n",
              " 'lexie',\n",
              " 'myah',\n",
              " 'rylan',\n",
              " 'deborah',\n",
              " 'emilee',\n",
              " 'laylah',\n",
              " 'novalee',\n",
              " 'ellis',\n",
              " 'emmeline',\n",
              " 'avalynn',\n",
              " 'hadlee',\n",
              " 'legacy',\n",
              " 'braylee',\n",
              " 'elisabeth',\n",
              " 'kaylie',\n",
              " 'ansley',\n",
              " 'dior',\n",
              " 'paula',\n",
              " 'belen',\n",
              " 'corinne',\n",
              " 'maleah',\n",
              " 'martha',\n",
              " 'teresa',\n",
              " 'salma',\n",
              " 'louise',\n",
              " 'averi',\n",
              " 'lilianna',\n",
              " 'amiya',\n",
              " 'milena',\n",
              " 'royal',\n",
              " 'aubrielle',\n",
              " 'calliope',\n",
              " 'frankie',\n",
              " 'natasha',\n",
              " 'kamilah',\n",
              " 'meilani',\n",
              " 'raina',\n",
              " 'amayah',\n",
              " 'lailah',\n",
              " 'rayne',\n",
              " 'zaniyah',\n",
              " 'isabela',\n",
              " 'nathalie',\n",
              " 'miah',\n",
              " 'opal',\n",
              " 'kenia',\n",
              " 'azariah',\n",
              " 'hunter',\n",
              " 'tori',\n",
              " 'andi',\n",
              " 'keily',\n",
              " 'leanna',\n",
              " 'scarlette',\n",
              " 'jaelyn',\n",
              " 'saoirse',\n",
              " 'selene',\n",
              " 'dalary',\n",
              " 'lindsey',\n",
              " 'marianna',\n",
              " 'ramona',\n",
              " 'estelle',\n",
              " 'giovanna',\n",
              " 'holland',\n",
              " 'nancy',\n",
              " 'emmalynn',\n",
              " 'mylah',\n",
              " 'rosalee',\n",
              " 'sariah',\n",
              " 'zoie',\n",
              " 'blaire',\n",
              " 'lyanna',\n",
              " 'maxine',\n",
              " 'anais',\n",
              " 'dana',\n",
              " 'judith',\n",
              " 'kiera',\n",
              " 'jaelynn',\n",
              " 'noor',\n",
              " 'kai',\n",
              " 'adalee',\n",
              " 'oaklee',\n",
              " 'amaris',\n",
              " 'jaycee',\n",
              " 'belle',\n",
              " 'carolyn',\n",
              " 'della',\n",
              " 'karter',\n",
              " 'sky',\n",
              " 'treasure',\n",
              " 'vienna',\n",
              " 'jewel',\n",
              " 'rivka',\n",
              " 'rosalyn',\n",
              " 'alannah',\n",
              " 'ellianna',\n",
              " 'sunny',\n",
              " 'claudia',\n",
              " 'cara',\n",
              " 'hailee',\n",
              " 'estrella',\n",
              " 'harleigh',\n",
              " 'zhavia',\n",
              " 'alianna',\n",
              " 'brittany',\n",
              " 'jaylene',\n",
              " 'journi',\n",
              " 'marissa',\n",
              " 'mavis',\n",
              " 'iliana',\n",
              " 'jurnee',\n",
              " 'aislinn',\n",
              " 'alyson',\n",
              " 'elsa',\n",
              " 'kamiyah',\n",
              " 'kiana',\n",
              " 'lisa',\n",
              " 'arlette',\n",
              " 'kadence',\n",
              " 'kathleen',\n",
              " 'halle',\n",
              " 'erika',\n",
              " 'sylvie',\n",
              " 'adele',\n",
              " 'erica',\n",
              " 'veda',\n",
              " 'whitney',\n",
              " 'bexley',\n",
              " 'emmaline',\n",
              " 'guadalupe',\n",
              " 'august',\n",
              " 'brynleigh',\n",
              " 'gwen',\n",
              " 'promise',\n",
              " 'alisson',\n",
              " 'india',\n",
              " 'madalynn',\n",
              " 'paloma',\n",
              " 'patricia',\n",
              " 'samira',\n",
              " 'aliya',\n",
              " 'casey',\n",
              " 'jazlynn',\n",
              " 'paulina',\n",
              " 'dulce',\n",
              " 'kallie',\n",
              " 'perla',\n",
              " 'adrienne',\n",
              " 'alora',\n",
              " 'nataly',\n",
              " 'ayleen',\n",
              " 'christine',\n",
              " 'kaiya',\n",
              " 'ariadne',\n",
              " 'karlee',\n",
              " 'barbara',\n",
              " 'lillianna',\n",
              " 'raquel',\n",
              " 'saniyah',\n",
              " 'yamileth',\n",
              " 'arely',\n",
              " 'celia',\n",
              " 'heavenly',\n",
              " 'kaylin',\n",
              " 'marisol',\n",
              " 'marleigh',\n",
              " 'avalyn',\n",
              " 'berkley',\n",
              " 'kataleya',\n",
              " 'zainab',\n",
              " 'dani',\n",
              " 'egypt',\n",
              " 'joyce',\n",
              " 'kenley',\n",
              " 'annabel',\n",
              " 'kaelyn',\n",
              " 'etta',\n",
              " 'hadleigh',\n",
              " 'joselyn',\n",
              " 'luella',\n",
              " 'jaylee',\n",
              " 'zola',\n",
              " 'alisha',\n",
              " 'ezra',\n",
              " 'queen',\n",
              " 'amia',\n",
              " 'annalee',\n",
              " 'bellamy',\n",
              " 'paola',\n",
              " 'tinley',\n",
              " 'violeta',\n",
              " 'jenesis',\n",
              " 'arden',\n",
              " 'giana',\n",
              " 'wendy',\n",
              " 'ellison',\n",
              " 'florence',\n",
              " 'margo',\n",
              " 'naya',\n",
              " 'robin',\n",
              " 'sandra',\n",
              " 'scout',\n",
              " 'waverly',\n",
              " 'janessa',\n",
              " 'jayden',\n",
              " 'micah',\n",
              " 'novah',\n",
              " 'zora',\n",
              " 'ann',\n",
              " 'jana',\n",
              " 'taliyah',\n",
              " 'vada',\n",
              " 'giavanna',\n",
              " 'ingrid',\n",
              " 'valery',\n",
              " 'azaria',\n",
              " 'emmarie',\n",
              " 'esperanza',\n",
              " 'kailyn',\n",
              " 'aiyana',\n",
              " 'keilani',\n",
              " 'austyn',\n",
              " 'whitley',\n",
              " 'elina',\n",
              " 'kimora',\n",
              " 'maliah',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fa935c21",
      "metadata": {
        "id": "fa935c21"
      },
      "outputs": [],
      "source": [
        "# Dictionaries, {idx -> ch} and {ch -> idx}\n",
        "itos = defaultdict(int)\n",
        "stoi = defaultdict(int)\n",
        "# Number of characters used to predict the target character in the MLP Language Model\n",
        "block_size = 3\n",
        "# Batch size used in MLP Language Model\n",
        "batch_size = 32\n",
        "# Embedding dimension, per character\n",
        "d_model = 10\n",
        "# Hidden dimension for RNN and also MLP Language Models\n",
        "d_h = 200\n",
        "\n",
        "# START = START token\n",
        "stoi['.'] = 0\n",
        "itos[0] = '.'\n",
        "\n",
        "count = 1\n",
        "# Loop over all names and create mappings itos and stoi mapping a unique character to a idx\n",
        "for name in names:\n",
        "  for ch in name:\n",
        "    if not ch in stoi:\n",
        "      stoi[ch] = count\n",
        "      itos[count] = ch\n",
        "      count = count + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d556793e",
      "metadata": {
        "id": "d556793e"
      },
      "outputs": [],
      "source": [
        "assert len(stoi) == len(itos)\n",
        "vocab_size = len(stoi)\n",
        "assert vocab_size == 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3765e0fe",
      "metadata": {
        "id": "3765e0fe",
        "outputId": "7cdf3dbc-58d7-47ca-ac37-28b814d5492d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'.': 0,\n",
              "             'e': 1,\n",
              "             'm': 2,\n",
              "             'a': 3,\n",
              "             'o': 4,\n",
              "             'l': 5,\n",
              "             'i': 6,\n",
              "             'v': 7,\n",
              "             's': 8,\n",
              "             'b': 9,\n",
              "             'p': 10,\n",
              "             'h': 11,\n",
              "             'c': 12,\n",
              "             'r': 13,\n",
              "             't': 14,\n",
              "             'y': 15,\n",
              "             'n': 16,\n",
              "             'g': 17,\n",
              "             'z': 18,\n",
              "             'f': 19,\n",
              "             'd': 20,\n",
              "             'u': 21,\n",
              "             'k': 22,\n",
              "             'w': 23,\n",
              "             'q': 24,\n",
              "             'x': 25,\n",
              "             'j': 26})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "stoi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d706797e",
      "metadata": {
        "id": "d706797e"
      },
      "source": [
        "## BiGram Language Model\n",
        "- Implement the Bigram Language Model\n",
        "- Get all the relevent counts, then get the train dataset Perplexity\n",
        "- Use the class notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9cb554a5",
      "metadata": {
        "id": "9cb554a5"
      },
      "outputs": [],
      "source": [
        "# Using the formulas in class, loop over each name and get the parameters\n",
        "c1 = defaultdict(int)\n",
        "c2 = defaultdict(int)\n",
        "for name in open(names_path, 'r'):\n",
        "    # Lowercase and remove any whitespace at the end\n",
        "    name = name.lower().strip()\n",
        "    # Pad with START = '.' and STOP = '.'\n",
        "    name = '.'+name+'.'\n",
        "    # Transform to integer\n",
        "    name = [stoi[ch] for ch in name]\n",
        "    # Get the counts for Bigrams and Unigrams\n",
        "    for i in range(len(name)):\n",
        "      c1[name[i]] += 1\n",
        "\n",
        "      if i < len(name) - 1:\n",
        "        bigram = (name[i], name[i + 1])\n",
        "        c2[bigram] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "507e4525",
      "metadata": {
        "id": "507e4525",
        "outputId": "c4364b71-c1ae-4b6e-fbac-ac7dbd28f35b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preplexity:  7.258398056030273\n"
          ]
        }
      ],
      "source": [
        "# Get perplexity\n",
        "\n",
        "sumneglogp = 0\n",
        "T = 0\n",
        "for name in open(names_path, 'r'):\n",
        "    # Get rid of white space and lowercase\n",
        "    name = name.lower().strip()\n",
        "    # Get the length of the word, without padding\n",
        "    T += len(name)\n",
        "    # Don't pad the STOP since we are not generating; pad with START\n",
        "    name = '.' + name\n",
        "    # Transform to integrs\n",
        "    name = [stoi[ch] for ch in name]\n",
        "    # Get the loss -log(p(name)); use that the log of the product is the sum of the logs\n",
        "    for i in range(len(name) - 1):\n",
        "      ix, iy = name[i], name[i + 1]\n",
        "      num = c2[(ix, iy)] + 1\n",
        "      den = c1[ix]\n",
        "      p = num / den\n",
        "\n",
        "      sumneglogp += -math.log(p)\n",
        "# Print the Perplexity\n",
        "print('Preplexity: ', torch.pow(torch.tensor(sumneglogp / T ), 2).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b931b726",
      "metadata": {
        "id": "b931b726",
        "outputId": "6c780545-e849-4238-d285-2ead59014f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated name:  moryamayashion\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# When you generate STOP, terminate\n",
        "name = '.'\n",
        "while True:\n",
        "    c = stoi[name[-1]]\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    p = []\n",
        "    for d in range(vocab_size):\n",
        "        # Use the same indicies as the dictionary we set up\n",
        "        # Populate p\n",
        "        FILL_IN\n",
        "    #print(p)\n",
        "    assert len(p) == vocab_size\n",
        "    # Sample randmly from the probability using torch.Categorical\n",
        "    c = FILL_IN\n",
        "    # Offset by 1 since we want indices [1, 2, ..., vocab_size]\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[1:])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561cdacb",
      "metadata": {
        "id": "561cdacb"
      },
      "source": [
        "## MLP Language Model\n",
        "\n",
        "- Implement the MLP language model from below\n",
        "- Look at page 7, Equation (1)\n",
        "- https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982566bf",
      "metadata": {
        "id": "982566bf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb6328a5",
      "metadata": {
        "id": "fb6328a5"
      },
      "outputs": [],
      "source": [
        "x_data = []\n",
        "y_data = []\n",
        "for name in open('names.txt', 'r'):\n",
        "    name = name.lower().strip()\n",
        "    # Pad with block_size START tokens and 1 STOP token\n",
        "    name = ''.join(block_size * ['.']) + name + '.'\n",
        "    # Loop through name and get the (x, y) pairs\n",
        "    # Add (x, y) to x_data and y_data and make sure you transform to characters\n",
        "    # Make sure x_data and y_data have integers, use stoi\n",
        "    for i in range(len(name) - block_size):\n",
        "        FILL_IN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3142dd13",
      "metadata": {
        "id": "3142dd13"
      },
      "outputs": [],
      "source": [
        "class MLPLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # An embedding for each character; vocab_size of them\n",
        "        self.e = FILL_IN\n",
        "        # H; should take in block_size * d_model vector and output d_h\n",
        "        self.H = FILL_IN\n",
        "        # U; should take in d_h vector and output vocab_size\n",
        "        self.U = FILL_IN\n",
        "        # W; for the skip connection, should take in block_size * d_model and output vocab_size\n",
        "        self.W = FILL_IN\n",
        "\n",
        "    # x should be (batch_size, block_size)\n",
        "    def forward(self, x):\n",
        "        FILL_IN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4f35f7",
      "metadata": {
        "id": "4a4f35f7",
        "outputId": "c7f2cd4a-2779-41e2-af47-c75542c6379a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([0, 0, 0], 18)"
            ]
          },
          "execution_count": 300,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_data[0], y_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17bf7a0a",
      "metadata": {
        "id": "17bf7a0a"
      },
      "outputs": [],
      "source": [
        "# Define a dataloader with x_data and y_data with batch_size\n",
        "dl = FILL_IN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56ce82f",
      "metadata": {
        "id": "a56ce82f"
      },
      "outputs": [],
      "source": [
        "for xb, yb in dl:\n",
        "    assert xb.shape == (batch_size, 3)\n",
        "    assert yb.shape == (batch_size,)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1013903d",
      "metadata": {
        "id": "1013903d"
      },
      "outputs": [],
      "source": [
        "# Define the MLP model and the Adam optimizer learning rate 0.001\n",
        "model = MLPLanguageModel()\n",
        "optimizer = FILL_IN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0155215c",
      "metadata": {
        "id": "0155215c",
        "outputId": "b9359fd7-35dd-4473-cecb-55d15baa1513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5894072308540346\n",
            "2.457281219482422\n",
            "2.407582850694656\n",
            "2.3947769327163697\n",
            "2.386825907945633\n",
            "2.34425501203537\n",
            "2.3441826746463774\n",
            "2.3230127596855166\n",
            "2.314083107471466\n",
            "2.3187712411880494\n",
            "2.3163943738937376\n",
            "2.299647463083267\n",
            "2.3065791826248168\n",
            "2.2824714307785032\n",
            "2.264568898677826\n",
            "2.2898624875545504\n",
            "2.284603733062744\n",
            "2.256791707277298\n",
            "2.2530757277011872\n",
            "2.259875492334366\n",
            "2.2480121746063233\n",
            "2.2454004590511323\n",
            "2.237783147096634\n",
            "2.2407697353363036\n",
            "2.248863885164261\n",
            "2.2322758870124817\n",
            "2.2193622727394104\n",
            "2.223179480791092\n",
            "2.2356123917102813\n",
            "2.216988515138626\n",
            "2.22616584610939\n",
            "2.2130448422431948\n",
            "2.203174202203751\n",
            "2.2143791213035584\n",
            "2.2166521270275115\n",
            "2.2154732391834258\n",
            "2.1883920991420744\n",
            "2.2045855729579924\n",
            "2.2106930553913116\n",
            "2.211230798959732\n",
            "2.191891407251358\n",
            "2.1892505435943606\n",
            "2.196866712808609\n",
            "2.181224030017853\n",
            "2.1818389432430267\n",
            "2.186741435050964\n",
            "2.1679062938690183\n",
            "2.1833346343040465\n",
            "2.1822933118343353\n",
            "2.1826359000205993\n",
            "2.205969117641449\n",
            "2.1794409832954407\n",
            "2.1966153411865235\n",
            "2.195118425369263\n",
            "2.170777860403061\n",
            "2.168219382286072\n",
            "2.1821779885292054\n",
            "2.1704253492355345\n",
            "2.1621498544216156\n",
            "2.181074720859528\n",
            "2.1661094183921814\n",
            "2.172186158657074\n",
            "2.1562428092956543\n",
            "2.162130882978439\n",
            "2.1608908133506777\n",
            "2.1688261275291443\n",
            "2.1582104210853577\n",
            "2.1680945496559145\n",
            "2.1691222383975983\n",
            "2.1633048994541166\n",
            "2.152134317398071\n",
            "2.1508175642490386\n",
            "2.1616415870189667\n",
            "2.1374976155757905\n",
            "2.152686320066452\n",
            "2.1477243535518644\n",
            "2.1640802018642424\n",
            "2.1518292405605317\n",
            "2.152129183292389\n",
            "2.143674113750458\n",
            "2.1547027745246887\n",
            "2.159407968044281\n",
            "2.1475126917362215\n",
            "2.162065080881119\n",
            "2.1467708139419557\n",
            "2.1540623824596405\n",
            "2.1464312090873716\n",
            "2.128968066453934\n",
            "2.1382183706760407\n",
            "2.1496013498306272\n",
            "2.148638415813446\n",
            "2.133571047067642\n",
            "2.1516552584171293\n",
            "2.148024996519089\n",
            "2.145294527053833\n",
            "2.145066568374634\n",
            "2.145038235425949\n",
            "2.1378802795410157\n",
            "2.150877092123032\n",
            "2.124778664827347\n",
            "2.128606203556061\n",
            "2.1380219435691834\n",
            "2.140316452264786\n",
            "2.1450090050697326\n",
            "2.1483073034286497\n",
            "2.125716846227646\n",
            "2.1236170737743376\n",
            "2.1377969801425936\n",
            "2.1388748972415925\n",
            "2.140829493045807\n",
            "2.153381428003311\n",
            "2.1061591856479644\n",
            "2.1307544856071474\n",
            "2.1372199158668517\n",
            "2.1361645109653473\n",
            "2.119778084278107\n",
            "2.0978619689941405\n",
            "2.1331179332733154\n",
            "2.1190319130420683\n",
            "2.1416146104335785\n",
            "2.1346324706077575\n",
            "2.1361966743469236\n",
            "2.130432144165039\n",
            "2.11774720454216\n",
            "2.124930157661438\n",
            "2.132985883951187\n",
            "2.13123312997818\n",
            "2.1451848568916323\n",
            "2.1324277975559234\n",
            "2.121409861326218\n",
            "2.1230474712848664\n",
            "2.098636980772018\n",
            "2.118087923526764\n",
            "2.127527127981186\n",
            "2.1339828217029573\n",
            "2.1201842164993288\n",
            "2.1123575570583344\n",
            "2.128602982759476\n",
            "2.129157070398331\n",
            "2.1304832916259766\n",
            "2.1301893215179444\n",
            "2.1315822732448577\n",
            "2.114510152101517\n",
            "2.1158635640144348\n",
            "2.104869607925415\n",
            "2.1217241673469545\n",
            "2.116632284402847\n",
            "2.116808346271515\n",
            "2.1218403482437136\n",
            "2.108985232830048\n",
            "2.1268839735984804\n",
            "2.1111370854377745\n",
            "2.122393314361572\n",
            "2.1295010697841645\n",
            "2.1421598343849184\n",
            "2.1151951611042024\n",
            "2.1062611954212187\n",
            "2.1020620856285097\n",
            "2.1158076219558715\n",
            "2.123169098377228\n",
            "2.1105967876911165\n",
            "2.1142330892086028\n",
            "2.121842725992203\n",
            "2.1075729887485504\n",
            "2.116517170190811\n",
            "2.1107917795181272\n",
            "2.10650629734993\n",
            "2.139111280441284\n",
            "2.096643429040909\n",
            "2.1260074272155762\n",
            "2.120508100748062\n",
            "2.098237284183502\n",
            "2.112832992315292\n",
            "2.103355184793472\n",
            "2.097235008239746\n",
            "2.1176110644340516\n",
            "2.098088711977005\n",
            "2.127496450424194\n",
            "2.109674681901932\n",
            "2.1193545401096343\n",
            "2.1289110038280485\n",
            "2.103670207977295\n",
            "2.1201903245449065\n",
            "2.0963507046699523\n",
            "2.1147133910655977\n",
            "2.1300642762184143\n",
            "2.111483735084534\n",
            "2.100964566707611\n",
            "2.1067271945476533\n",
            "2.1055310201644897\n",
            "2.116045585155487\n",
            "2.112235931158066\n",
            "2.1058252339363097\n",
            "2.100637074232101\n",
            "2.1128933982849123\n",
            "2.0849624288082125\n",
            "2.0972255890369413\n",
            "2.1085698652267455\n",
            "2.1266038670539857\n",
            "2.100752772569656\n",
            "2.104310161113739\n",
            "2.0887409884929657\n",
            "2.1175877034664152\n",
            "2.1083028984069823\n",
            "2.102303539276123\n",
            "2.118862781763077\n",
            "2.1049597079753877\n",
            "2.118791632652283\n",
            "2.100370110988617\n",
            "2.117748591899872\n",
            "2.1125023913383485\n",
            "2.1087042751312257\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[296], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Get the logits\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlogits, target\u001b[38;5;241m=\u001b[39myb)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[291], line 27\u001b[0m, in \u001b[0;36mMLPLanguageModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTanh()(x)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# (batch_size, vocab_size)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mU\u001b[49m(x)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Add the skip connection\u001b[39;00m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x_skip\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1256\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_full_backward_hook\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_full_backward_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1258\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "total_loss = 0\n",
        "total_ct = 0\n",
        "total_epochs = 20\n",
        "\n",
        "for _ in range(total_epochs):\n",
        "    for xb, yb in dl:\n",
        "        # Zero the gradients\n",
        "        FILL_IN\n",
        "\n",
        "        # Get the logits\n",
        "        logits = FILL_IN\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = FILL_IN\n",
        "\n",
        "        # Get the new gradient\n",
        "        FILL_IN\n",
        "\n",
        "        # Clip the gradients to max norm 0.1\n",
        "        # Use clid_grad_norm from torch\n",
        "        FILL_IN\n",
        "\n",
        "        # Do a gradient update\n",
        "        FILL_IN\n",
        "\n",
        "        # Get the loss for the batch and get the number of batches\n",
        "        total_loss += loss.item()\n",
        "        total_ct += 1\n",
        "\n",
        "        # Print the loss\n",
        "        if total_ct and total_ct % 500 == 0:\n",
        "            print(total_loss / total_ct)\n",
        "            total_loss = 0\n",
        "            total_ct = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ecf5fb3",
      "metadata": {
        "id": "4ecf5fb3",
        "outputId": "16b0adc6-4be1-4ba8-fccf-bf15c9d10b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preplexity:  10.598803520202637\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    # Get perplexity\n",
        "    sumneglogp = 0\n",
        "    T = 0\n",
        "    for name in open('names.txt', 'r'):\n",
        "        name = name.lower().strip()\n",
        "        T += len(name)\n",
        "        # Pad with block_size START tokens\n",
        "        name = FILL_IN\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        # Gather all the terms over the loss\n",
        "        # Notice that we compute -log p(...abc)\n",
        "        # Which is -log p(a | ...) - log p(b | a..) - log p(c | ba.)\n",
        "        FILL_IN\n",
        "        # Gather the loss over the name, for each term\n",
        "        # You need to get the softmax loss for each term\n",
        "        # Can either use the CrossEntropyLoss or do this manually\n",
        "        # Compute the loss\n",
        "        logits = FILL_IN\n",
        "\n",
        "        # Use reduction \"sum\" so you don't need to worry about N\n",
        "        loss = FILL_IN\n",
        "\n",
        "        # Change to log base 2\n",
        "        loss *= FILL_IN\n",
        "\n",
        "        sumneglogp += loss\n",
        "\n",
        "    print('Preplexity: ', torch.pow(sumneglogp.clone().detach() / T , 2).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77131c8",
      "metadata": {
        "id": "e77131c8",
        "outputId": "6482fdda-a4c7-4f8b-c2c7-3848ae8786e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated name:  manue\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# When you generate STOP, terminate\n",
        "name = ''.join(block_size * ['.'])\n",
        "while True:\n",
        "    # Get the idx\n",
        "    c = FILL_IN\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    p = FILL_IN\n",
        "    # Randomly sample from p a new character\n",
        "    c = FILL_IN\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[block_size:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1b01ed",
      "metadata": {
        "id": "bb1b01ed"
      },
      "source": [
        "## RNN Language Model\n",
        "- For each name, run an RNN character by character\n",
        "- Use the recursion x = Tanh()(Wh @ h + Wx @ x + bh + bx) and y = Softmax()(Wy h + by)\n",
        "- Do not use the RNN Cell from PyTorch, do this manually as hinted below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c765e7fc",
      "metadata": {
        "id": "c765e7fc"
      },
      "outputs": [],
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Each token has an embedding of size vocab_size\n",
        "        self.e = FILL_IN\n",
        "        # Wh used to map hidden to hidden\n",
        "        self.Wh = FILL_IN\n",
        "        self.Wx = FILL_IN\n",
        "        self.Wy = FILL_IN\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Run through to get the embedding for the token\n",
        "        # The embedding per token is the feature vector x  we pass into the\n",
        "        # Represent x as an embedding\n",
        "        x = FILL_IN\n",
        "        # Get the hidden state\n",
        "        h = FILL_IN\n",
        "        # Get the logits we use to predict y\n",
        "        z = FILL_IN\n",
        "        # Return the z predicting y for the timestep we are at and the next hidden state\n",
        "        return z, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd798358",
      "metadata": {
        "id": "cd798358"
      },
      "outputs": [],
      "source": [
        "model = RNNLanguageModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f3a49ff",
      "metadata": {
        "id": "7f3a49ff",
        "outputId": "2b57f60b-b65f-4c68-a963-f976117d0b57"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'FILL_IN' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# name[:-1]\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m x_data \u001b[38;5;241m=\u001b[39m \u001b[43mFILL_IN\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# name[1:]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m y_data \u001b[38;5;241m=\u001b[39m FILL_IN\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FILL_IN' is not defined"
          ]
        }
      ],
      "source": [
        "total_loss = 0\n",
        "total_ct = 0\n",
        "total_epochs = 5\n",
        "\n",
        "for _ in range(total_epochs):\n",
        "    for name in open('names.txt', 'r'):\n",
        "        name = name.lower().strip()\n",
        "        # Add the start and end padding token\n",
        "        name = '.' + name + '.'\n",
        "        # name[:-1]\n",
        "        x_data = FILL_IN\n",
        "        # name[1:]\n",
        "        y_data = FILL_IN\n",
        "        logits = []\n",
        "        # Set the hidden state to random\n",
        "        h = FILL_IN\n",
        "        # Zero the grad\n",
        "        FILL_IN\n",
        "\n",
        "        # Loop through each token and get the new h and then pass it forward\n",
        "        # Accumulate all the logits\n",
        "        for x in x_data:\n",
        "            FILL_IN\n",
        "            FILL_IN\n",
        "\n",
        "        # Put all the logits into one tensor\n",
        "        logits = FILL_IN\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = FILL_IN\n",
        "\n",
        "        # Get the new gradient\n",
        "        FILL_IN\n",
        "\n",
        "        # Clip the gradients at max norm 0.1\n",
        "        FILL_IN\n",
        "\n",
        "        # Do a gradient update\n",
        "        FILL_IN\n",
        "\n",
        "        # Get the loss for the batch and get the number of batches\n",
        "        total_loss += loss.item()\n",
        "        total_ct += 1\n",
        "\n",
        "        if total_ct and total_ct % 100 == 0:\n",
        "            print(total_loss / total_ct)\n",
        "            total_loss = 0\n",
        "            total_ct = 0\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f5f990",
      "metadata": {
        "id": "83f5f990",
        "outputId": "a83e96ad-c681-49b9-ec3a-e8ee3cbf636f"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "forward() takes 2 positional arguments but 3 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[312], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_data:\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m stoi[x]\n\u001b[0;32m---> 21\u001b[0m     z, h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     logits\u001b[38;5;241m.\u001b[39mappend(z)\n\u001b[1;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    # Get perplexity\n",
        "    sumneglogp = 0\n",
        "    T = 0\n",
        "    for name in open('names.txt', 'r'):\n",
        "        name = name.lower().strip()\n",
        "        T += len(name)\n",
        "        name = '.' + name\n",
        "        # Get the name from index 0 to -1 exclusive end\n",
        "        x_data = FILL_IN\n",
        "        # Get the y from index 1 to end inclusive end\n",
        "        y_data = FILL_IN\n",
        "        # logits per token prediction\n",
        "        logits = []\n",
        "        # Initialize the h vector to random\n",
        "        h = FILL_IN\n",
        "        # Loop over each chracter in the name and pass h and this into the RNN\n",
        "        # Get the new logit\n",
        "        for x in x_data:\n",
        "            # Get the int for x\n",
        "            x = FILL_IN\n",
        "            # Get z and h\n",
        "            z, h = FILL_IN\n",
        "            # Append to logit\n",
        "            FILL_IN\n",
        "\n",
        "        # Get all the logits for each character\n",
        "        logits = FILL_IN\n",
        "\n",
        "        # Compute the loss across all characters\n",
        "        loss = FILL_IN\n",
        "\n",
        "        # Change to log base 2\n",
        "        # log2(x) = ln(x) / ln(2)\n",
        "        loss *= FILL_IN\n",
        "\n",
        "        sumneglogp += FILL_IN\n",
        "\n",
        "    # sumneglogp is -log(p('.' + name1)) -log(p('.' + name2)) -log(p('.' + name3)) ...\n",
        "    # Divide by the appropriate term to get the answer we want\n",
        "    print('Preplexity: ', torch.pow(sumneglogp.clone().detach() / T , 2).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7608536d",
      "metadata": {
        "id": "7608536d",
        "outputId": "1de9bac2-74c7-411d-8827-b6c7e3bc2566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated name:  zacarlan\n"
          ]
        }
      ],
      "source": [
        "# Generate a random word using this distributon\n",
        "# Intialize the word with\n",
        "name = '.'\n",
        "# Initialize h to random\n",
        "h = FILL_IN\n",
        "while True:\n",
        "    # Make c to an integer\n",
        "    c = FILL_IN\n",
        "    # Make the distribution from c to any other word other than START\n",
        "    logits, h = FILL_IN\n",
        "    # Get p; use Softmax\n",
        "    p = FILL_IN\n",
        "    # Sample from p\n",
        "    c = FILL_IN\n",
        "    # If we generate '.', stop\n",
        "    if c.item() == 0:\n",
        "        break\n",
        "    else:\n",
        "        name += itos[c.item()]\n",
        "print('Generated name: ' , name[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d790024",
      "metadata": {
        "id": "4d790024"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}