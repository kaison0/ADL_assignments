{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2BVH_JD9WiA"
      },
      "source": [
        "Below we will try and fit a Logisitc Regression Model step by step for the XOR problem.\n",
        "Fill in the code where there is a `_FILL_` specified. For this model, we have $x_1$ and $x_2$ are either 0/1 each and $y = x_1 + x_2 - 2x_1x_2$. Notice that this is True (1) if $x_1 = 1$ and $x_2 = 0$ OR $x_1 = 0$ and $x_2 = 1$; $y$ is zero otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "wiFGf-9H9X3d"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "# Don't fill this in\n",
        "_FILL_ = '_FILL_'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "1TRwUp469X-r"
      },
      "outputs": [],
      "source": [
        "x_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_data = [[0], [1], [1], [0]]\n",
        "x_data = torch.Tensor(x_data)\n",
        "y_data = torch.Tensor(y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "2FJM6ckGBRz_"
      },
      "outputs": [],
      "source": [
        "# Define each tensor to be 1x1 and have them require a gradient for tracking; these are parameters\n",
        "alpha = torch.rand([1, 1], requires_grad=True)\n",
        "beta_1 = torch.rand([1, 1], requires_grad=True)\n",
        "beta_2 = torch.rand([1, 1], requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BToqdBCr9YBI",
        "outputId": "b6852b5e-2995-4ede-e518-01bc4d6197c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Loss: 0.6994901896 Accuracy: 0.5\n",
            "Epoch: 1\n",
            "Loss: 0.6993739605 Accuracy: 0.5\n",
            "Epoch: 2\n",
            "Loss: 0.6992611885 Accuracy: 0.5\n",
            "Epoch: 3\n",
            "Loss: 0.6991516352 Accuracy: 0.5\n",
            "Epoch: 4\n",
            "Loss: 0.6990453005 Accuracy: 0.5\n",
            "Epoch: 5\n",
            "Loss: 0.6989420652 Accuracy: 0.5\n",
            "Epoch: 6\n",
            "Loss: 0.6988418102 Accuracy: 0.5\n",
            "Epoch: 7\n",
            "Loss: 0.6987444162 Accuracy: 0.5\n",
            "Epoch: 8\n",
            "Loss: 0.6986498833 Accuracy: 0.5\n",
            "Epoch: 9\n",
            "Loss: 0.6985580921 Accuracy: 0.5\n",
            "Epoch: 10\n",
            "Loss: 0.6984688640 Accuracy: 0.5\n",
            "Epoch: 11\n",
            "Loss: 0.6983822584 Accuracy: 0.5\n",
            "Epoch: 12\n",
            "Loss: 0.6982981563 Accuracy: 0.5\n",
            "Epoch: 13\n",
            "Loss: 0.6982164383 Accuracy: 0.5\n",
            "Epoch: 14\n",
            "Loss: 0.6981370449 Accuracy: 0.5\n",
            "Epoch: 15\n",
            "Loss: 0.6980599165 Accuracy: 0.5\n",
            "Epoch: 16\n",
            "Loss: 0.6979848146 Accuracy: 0.5\n",
            "Epoch: 17\n",
            "Loss: 0.6979120970 Accuracy: 0.5\n",
            "Epoch: 18\n",
            "Loss: 0.6978413463 Accuracy: 0.75\n",
            "Epoch: 19\n",
            "Loss: 0.6977725029 Accuracy: 0.75\n",
            "Epoch: 20\n",
            "Loss: 0.6977056265 Accuracy: 0.75\n",
            "Epoch: 21\n",
            "Loss: 0.6976407170 Accuracy: 0.75\n",
            "Epoch: 22\n",
            "Loss: 0.6975774765 Accuracy: 0.75\n",
            "Epoch: 23\n",
            "Loss: 0.6975160837 Accuracy: 0.75\n",
            "Epoch: 24\n",
            "Loss: 0.6974562407 Accuracy: 0.75\n",
            "Epoch: 25\n",
            "Loss: 0.6973982453 Accuracy: 0.75\n",
            "Epoch: 26\n",
            "Loss: 0.6973417401 Accuracy: 0.75\n",
            "Epoch: 27\n",
            "Loss: 0.6972869039 Accuracy: 0.75\n",
            "Epoch: 28\n",
            "Loss: 0.6972333193 Accuracy: 0.75\n",
            "Epoch: 29\n",
            "Loss: 0.6971814036 Accuracy: 0.75\n",
            "Epoch: 30\n",
            "Loss: 0.6971309185 Accuracy: 0.75\n",
            "Epoch: 31\n",
            "Loss: 0.6970816851 Accuracy: 0.75\n",
            "Epoch: 32\n",
            "Loss: 0.6970337629 Accuracy: 0.75\n",
            "Epoch: 33\n",
            "Loss: 0.6969871521 Accuracy: 0.75\n",
            "Epoch: 34\n",
            "Loss: 0.6969417930 Accuracy: 0.75\n",
            "Epoch: 35\n",
            "Loss: 0.6968976259 Accuracy: 0.75\n",
            "Epoch: 36\n",
            "Loss: 0.6968546510 Accuracy: 0.75\n",
            "Epoch: 37\n",
            "Loss: 0.6968128085 Accuracy: 0.75\n",
            "Epoch: 38\n",
            "Loss: 0.6967720389 Accuracy: 0.75\n",
            "Epoch: 39\n",
            "Loss: 0.6967322826 Accuracy: 0.75\n",
            "Epoch: 40\n",
            "Loss: 0.6966937184 Accuracy: 0.75\n",
            "Epoch: 41\n",
            "Loss: 0.6966560483 Accuracy: 0.75\n",
            "Epoch: 42\n",
            "Loss: 0.6966192722 Accuracy: 0.75\n",
            "Epoch: 43\n",
            "Loss: 0.6965835094 Accuracy: 0.75\n",
            "Epoch: 44\n",
            "Loss: 0.6965487003 Accuracy: 0.75\n",
            "Epoch: 45\n",
            "Loss: 0.6965146661 Accuracy: 0.75\n",
            "Epoch: 46\n",
            "Loss: 0.6964815855 Accuracy: 0.75\n",
            "Epoch: 47\n",
            "Loss: 0.6964492202 Accuracy: 0.75\n",
            "Epoch: 48\n",
            "Loss: 0.6964177489 Accuracy: 0.75\n",
            "Epoch: 49\n",
            "Loss: 0.6963870525 Accuracy: 0.75\n",
            "Epoch: 50\n",
            "Loss: 0.6963571310 Accuracy: 0.75\n",
            "Epoch: 51\n",
            "Loss: 0.6963278055 Accuracy: 0.75\n",
            "Epoch: 52\n",
            "Loss: 0.6962993145 Accuracy: 0.75\n",
            "Epoch: 53\n",
            "Loss: 0.6962714195 Accuracy: 0.75\n",
            "Epoch: 54\n",
            "Loss: 0.6962442398 Accuracy: 0.75\n",
            "Epoch: 55\n",
            "Loss: 0.6962177157 Accuracy: 0.75\n",
            "Epoch: 56\n",
            "Loss: 0.6961917877 Accuracy: 0.75\n",
            "Epoch: 57\n",
            "Loss: 0.6961665154 Accuracy: 0.75\n",
            "Epoch: 58\n",
            "Loss: 0.6961417794 Accuracy: 0.75\n",
            "Epoch: 59\n",
            "Loss: 0.6961176395 Accuracy: 0.75\n",
            "Epoch: 60\n",
            "Loss: 0.6960940361 Accuracy: 0.75\n",
            "Epoch: 61\n",
            "Loss: 0.6960710287 Accuracy: 0.75\n",
            "Epoch: 62\n",
            "Loss: 0.6960483789 Accuracy: 0.75\n",
            "Epoch: 63\n",
            "Loss: 0.6960264444 Accuracy: 0.75\n",
            "Epoch: 64\n",
            "Loss: 0.6960048676 Accuracy: 0.75\n",
            "Epoch: 65\n",
            "Loss: 0.6959837675 Accuracy: 0.75\n",
            "Epoch: 66\n",
            "Loss: 0.6959631443 Accuracy: 0.75\n",
            "Epoch: 67\n",
            "Loss: 0.6959429979 Accuracy: 0.75\n",
            "Epoch: 68\n",
            "Loss: 0.6959232092 Accuracy: 0.75\n",
            "Epoch: 69\n",
            "Loss: 0.6959038973 Accuracy: 0.75\n",
            "Epoch: 70\n",
            "Loss: 0.6958850622 Accuracy: 0.75\n",
            "Epoch: 71\n",
            "Loss: 0.6958664656 Accuracy: 0.75\n",
            "Epoch: 72\n",
            "Loss: 0.6958482862 Accuracy: 0.75\n",
            "Epoch: 73\n",
            "Loss: 0.6958305836 Accuracy: 0.75\n",
            "Epoch: 74\n",
            "Loss: 0.6958131790 Accuracy: 0.75\n",
            "Epoch: 75\n",
            "Loss: 0.6957960129 Accuracy: 0.75\n",
            "Epoch: 76\n",
            "Loss: 0.6957793236 Accuracy: 0.75\n",
            "Epoch: 77\n",
            "Loss: 0.6957629919 Accuracy: 0.75\n",
            "Epoch: 78\n",
            "Loss: 0.6957468390 Accuracy: 0.75\n",
            "Epoch: 79\n",
            "Loss: 0.6957311630 Accuracy: 0.75\n",
            "Epoch: 80\n",
            "Loss: 0.6957155466 Accuracy: 0.75\n",
            "Epoch: 81\n",
            "Loss: 0.6957004666 Accuracy: 0.75\n",
            "Epoch: 82\n",
            "Loss: 0.6956855655 Accuracy: 0.75\n",
            "Epoch: 83\n",
            "Loss: 0.6956709623 Accuracy: 0.75\n",
            "Epoch: 84\n",
            "Loss: 0.6956565976 Accuracy: 0.75\n",
            "Epoch: 85\n",
            "Loss: 0.6956425309 Accuracy: 0.75\n",
            "Epoch: 86\n",
            "Loss: 0.6956286430 Accuracy: 0.75\n",
            "Epoch: 87\n",
            "Loss: 0.6956150532 Accuracy: 0.75\n",
            "Epoch: 88\n",
            "Loss: 0.6956017017 Accuracy: 0.75\n",
            "Epoch: 89\n",
            "Loss: 0.6955884695 Accuracy: 0.75\n",
            "Epoch: 90\n",
            "Loss: 0.6955755949 Accuracy: 0.75\n",
            "Epoch: 91\n",
            "Loss: 0.6955627799 Accuracy: 0.75\n",
            "Epoch: 92\n",
            "Loss: 0.6955504417 Accuracy: 0.75\n",
            "Epoch: 93\n",
            "Loss: 0.6955381036 Accuracy: 0.75\n",
            "Epoch: 94\n",
            "Loss: 0.6955260038 Accuracy: 0.75\n",
            "Epoch: 95\n",
            "Loss: 0.6955140829 Accuracy: 0.75\n",
            "Epoch: 96\n",
            "Loss: 0.6955023408 Accuracy: 0.75\n",
            "Epoch: 97\n",
            "Loss: 0.6954908371 Accuracy: 0.75\n",
            "Epoch: 98\n",
            "Loss: 0.6954795122 Accuracy: 0.75\n",
            "Epoch: 99\n",
            "Loss: 0.6954682469 Accuracy: 0.75\n"
          ]
        }
      ],
      "source": [
        "lr = 0.01\n",
        "\n",
        "sigmoid = nn.Sigmoid()\n",
        "for epoch in range(100):\n",
        "  for x, y in zip(x_data, y_data):\n",
        "\n",
        "    # Have z be beta_2*x[0] + beta_1*x[1] + alpha\n",
        "    z = beta_2*x[0] + beta_1*x[1] + alpha\n",
        "\n",
        "    # Push z through a nn.Sigmoid layer to get the p(y=1)\n",
        "    a = sigmoid(z)\n",
        "\n",
        "    # Write the loss manually between y and a\n",
        "    loss = -(y * torch.log(a) + (1 - y) * torch.log(1-a))\n",
        "    # Get the loss gradients; the gradients with respect to alpha, beta_1, beta_2\n",
        "    loss.backward()\n",
        "\n",
        "    # Manually update the gradients\n",
        "    # What we do below is wrapped within this clause because weights have required_grad=True but we don't need to track this in autograd\n",
        "    with torch.no_grad():\n",
        "        # Do an update for each parameter\n",
        "        alpha -= lr * alpha.grad\n",
        "        beta_1 -= lr * beta_1.grad\n",
        "        beta_2 -= lr * beta_2.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        alpha.grad.zero_()\n",
        "        beta_1.grad.zero_()\n",
        "        beta_2.grad.zero_()\n",
        "\n",
        "  # Manually get the accuracy of the model after each epoch\n",
        "  with torch.no_grad():\n",
        "    print(f'Epoch: {epoch}')\n",
        "    y_pred = []\n",
        "    loss = 0.0\n",
        "\n",
        "    for x, y in zip(x_data, y_data):\n",
        "      # Get z\n",
        "      z = beta_2*x[0] + beta_1*x[1] + alpha\n",
        "\n",
        "      # Get a\n",
        "      a = sigmoid(z)\n",
        "\n",
        "      # Get the loss\n",
        "      loss += -(y * torch.log(a) + (1 - y) * torch.log(1-a))\n",
        "\n",
        "      # Get the prediction given a\n",
        "      y_pred.append(a > 0.5)\n",
        "\n",
        "    # Get the current accuracy over 4 points; make this a tensor\n",
        "    y_pred = torch.Tensor(y_pred)\n",
        "\n",
        "    accuracy = (y_data.view(-1) == y_pred).sum() / 4\n",
        "    loss = loss / 4\n",
        "\n",
        "    # Print the accuracy and the loss\n",
        "    # You want the item in the tensor thats 1x1\n",
        "    print(f\"Loss: {loss.item():.10f} Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iojtw_rFAjhY"
      },
      "source": [
        "Exercise 1: Create a 2D tensor and then add a dimension of size 1 inserted at the 0th axis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fgdImVPpAm6d"
      },
      "outputs": [],
      "source": [
        "x = torch.Tensor([2, 2])\n",
        "x = x.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0yfuo7fAneJ"
      },
      "source": [
        "Exercise 2: Remove the extra dimension you just added to the previous tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "goe1-DBRAnnj"
      },
      "outputs": [],
      "source": [
        "x = x.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAhtAtk5Any4"
      },
      "source": [
        "Exercise 3: Create a random tensor of shape 5x3 in the interval [3, 7)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ZCcFowEjAn8w"
      },
      "outputs": [],
      "source": [
        "x = 3 + 4 * torch.rand(5, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvNprVRlAoEC"
      },
      "source": [
        "Exercise 4: Create a tensor with values from a normal distribution (mean=0, std=1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Dgirc4kGAoKa"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(5, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1nIIGp8AoQL"
      },
      "source": [
        "exercise 5: Retrieve the indexes of all the non zero elements in the tensor torch.Tensor([1, 1, 1, 0, 1]).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCv5zbq3AoV-",
        "outputId": "28de6d6c-b100-4721-c1d5-014bee0cf0a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0],\n",
              "        [1],\n",
              "        [2],\n",
              "        [4]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "result = torch.nonzero(torch.Tensor([1, 1, 1, 0, 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckErX5U1Aocz"
      },
      "source": [
        "Exercise 6: Create a random tensor of size (3,1) and then horizonally stack 4 copies together.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D3XYAnoAoig",
        "outputId": "2c5cbacf-09b5-4e93-a747-4e557d78c896"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.9250],\n",
              "         [0.0397],\n",
              "         [0.1040]]),\n",
              " tensor([[0.9250, 0.9250, 0.9250, 0.9250],\n",
              "         [0.0397, 0.0397, 0.0397, 0.0397],\n",
              "         [0.1040, 0.1040, 0.1040, 0.1040]]))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "x = torch.rand(3, 1)\n",
        "y = x.repeat(1, 4)\n",
        "x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKV3ChJrAopD"
      },
      "source": [
        "Exercise 7: Return the batch matrix-matrix product of two 3 dimensional matrices (a=torch.rand(3,4,5), b=torch.rand(3,5,4)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge6IErGdAovX",
        "outputId": "26e35923-059e-4e1e-b2b4-62b7426fa0a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.6348, 0.2725, 0.2501, 0.4860, 0.2934],\n",
              "          [0.9260, 0.2047, 0.8937, 0.4566, 0.2098],\n",
              "          [0.7305, 0.7653, 0.3733, 0.6413, 0.0547],\n",
              "          [0.1454, 0.6189, 0.6580, 0.3643, 0.1724]],\n",
              " \n",
              "         [[0.6883, 0.2452, 0.8855, 0.9905, 0.8135],\n",
              "          [0.6893, 0.6772, 0.2069, 0.4175, 0.1831],\n",
              "          [0.6751, 0.1996, 0.2804, 0.0087, 0.5793],\n",
              "          [0.2693, 0.7480, 0.3078, 0.4336, 0.7088]],\n",
              " \n",
              "         [[0.8103, 0.3029, 0.4939, 0.0853, 0.0329],\n",
              "          [0.8171, 0.0466, 0.9702, 0.8670, 0.5576],\n",
              "          [0.7162, 0.1998, 0.2009, 0.9989, 0.1979],\n",
              "          [0.8298, 0.6766, 0.1627, 0.7537, 0.3901]]]),\n",
              " tensor([[[0.1222, 0.1961, 0.7390, 0.7704],\n",
              "          [0.6497, 0.6177, 0.5646, 0.4525],\n",
              "          [0.6548, 0.4661, 0.6659, 0.6355],\n",
              "          [0.7970, 0.3389, 0.1691, 0.3490],\n",
              "          [0.8177, 0.3725, 0.1289, 0.9859]],\n",
              " \n",
              "         [[0.4424, 0.0505, 0.2097, 0.4959],\n",
              "          [0.3744, 0.9181, 0.2167, 0.7170],\n",
              "          [0.8459, 0.5364, 0.2008, 0.8231],\n",
              "          [0.5475, 0.3092, 0.3270, 0.9522],\n",
              "          [0.8332, 0.8411, 0.8620, 0.3134]],\n",
              " \n",
              "         [[0.4608, 0.9207, 0.7955, 0.7803],\n",
              "          [0.6871, 0.9143, 0.9906, 0.7700],\n",
              "          [0.5778, 0.7979, 0.8622, 0.1835],\n",
              "          [0.0810, 0.3220, 0.7340, 0.2893],\n",
              "          [0.8619, 0.8379, 0.9656, 0.0529]]]),\n",
              " tensor([[[1.0456, 0.6834, 0.9095, 1.2301],\n",
              "          [1.3668, 0.9575, 1.4993, 1.7401],\n",
              "          [1.3867, 1.0277, 1.3360, 1.4239],\n",
              "          [1.2821, 0.9052, 0.9789, 1.1074]],\n",
              " \n",
              "         [[2.3654, 1.7254, 1.4004, 2.4442],\n",
              "          [1.1147, 1.0507, 0.6272, 1.4526],\n",
              "          [1.0980, 0.8577, 0.7433, 0.8985],\n",
              "          [1.4875, 1.5957, 1.0331, 1.5583]],\n",
              " \n",
              "         [[0.9021, 1.4721, 1.4650, 0.9825],\n",
              "          [1.5200, 2.3154, 2.7076, 1.1318],\n",
              "          [0.8349, 1.4898, 1.8653, 1.0490],\n",
              "          [1.3385, 2.0819, 2.4005, 1.4369]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "a = torch.rand(3, 4, 5)\n",
        "b = torch.rand(3, 5, 4)\n",
        "c = torch.bmm(a, b)\n",
        "a, b, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVI_LI_PA_2e"
      },
      "source": [
        "Exercise 8: Return the batch matrix-matrix product of a 3D matrix and a 2D matrix (a=torch.rand(3,4,5), b=torch.rand(5,4)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "kpLgovtyBAA6"
      },
      "outputs": [],
      "source": [
        "a = torch.rand(3, 4, 5)\n",
        "b = torch.rand(5, 4)\n",
        "c = torch.matmul(a, b)\n",
        "a, b, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW6NxQIeBAJA"
      },
      "source": [
        "Exercise 9: Create a 1x1 random tensor and get the value inside of this tensor as a scalar. No tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_OFj9hEBAPO",
        "outputId": "de492f6e-5054-448f-899d-7b1826ee51a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(float, tensor([[0.8244]]), 0.8243800401687622)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "x = torch.rand(1, 1)\n",
        "s = x.item()\n",
        "type(s), x, s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_zAwiqrBAVd"
      },
      "source": [
        "Exercise 10: Create a 2x1 tensor and have it require a gradient. Have $x$, this tensor, hold [-2, 1]. Set $y=x_1^2 + x_2^2$ and get the gradient of y wirht respect to $x_1$ and then $x_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z98hDPfEBAcv",
        "outputId": "13c84b45-3573-45a6-cbec-73a22a2c330a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.],\n",
              "        [ 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "x = torch.tensor([[-2.0], [1.0]], requires_grad=True)\n",
        "y = x[0, 0] ** 2 + x[1, 0] ** 2\n",
        "y.backward()\n",
        "x.grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGfmkpF3BAjy"
      },
      "source": [
        "Exercise 11: Check if cuda is available (it shuld be if in the Runtime setting for colab you choose the GPU). If it is, move $x$ above to a CUDA device. Create a new tensor of the same shape as $x$ and put it on the cpu. Try and add these tensors. What happens. How do you fix this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M_Suz2XBAsX",
        "outputId": "b52da75f-ec69-4b06-c205-d8440882aace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.is_available()\n",
        "x = torch.rand(2,2)\n",
        "y = torch.rand_like(x)\n",
        "x = x.to(\"cuda\")\n",
        "try:\n",
        "    z = x + y\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n",
        "\n",
        "Move y above to a CUDA device too."
      ],
      "metadata": {
        "id": "kNv8a2dg9U_z"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}