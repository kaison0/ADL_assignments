{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRfgHr2GyNTO"
      },
      "source": [
        "For this notebook, please insert where there is `_FILL_` either code or logic to make this work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5XzVh-J0-fu"
      },
      "source": [
        "# MNIST CNN Digit Recognition Network\n",
        "\n",
        "For this problem, you will code a basic digit recognition network. The data are images which specify the digits 1 to 10 as (1, 28, 28) data - this data is black and white images. Each pixed of the image is an intensity between 0 and 255, and together the (1, 28, 28) pixel image can be visualized as a picture of a digit. The data is given to you as $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}$ where $y$ is the given label and x is the (1, 28, 28) data. This data will be gotten from `torchvision`, a repository of computer vision data and models.\n",
        "\n",
        "Highlevel, the model and notebook goes as follows:\n",
        "*   You first download the data and specify the batch size of B = 16. Each image will need to be turned from a (1, 28, 28) volume into a serious of other volumes either via convolutional layers or max pooling layers.\n",
        "*   You will pass the data through several layers to built a CNN classfier. Use the hints below to get the right dimensions and figure out what the layers should be. Be careful with the loss function. Add regularization (L1 and L2) manually.\n",
        "\n",
        "See the comments below and fill in the analysis where there is `_FILL_` specified. All asserts should pass and Test accuracy should be about 95%.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ftf9cgvf16r"
      },
      "outputs": [],
      "source": [
        "!pip uninstall --yes torchtune torchaudio torchvision\n",
        "!pip install torch==2.3.0 torchtext==0.18.0 torchdata==0.8.0 portalocker>=2.0.0 torchvision==0.18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8UDIb4ldyj2C"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PdLoOr08AUY5"
      },
      "outputs": [],
      "source": [
        "SEED = 1\n",
        "torch.manual_seed(SEED)\n",
        "_FILL_ = '_FILL_'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jLD0oQmgxxlR",
        "outputId": "be448e74-6dbb-400a-f234-caeedacfa5ce"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Dataset not found. You can use download=True to download it",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a552a73cc897>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_FILL_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m mnist_train_dataset = torchvision.datasets.MNIST(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0m_FILL_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset not found. You can use download=True to download it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found. You can use download=True to download it"
          ]
        }
      ],
      "source": [
        "image_path = './'\n",
        "\n",
        "# Use ToTensor\n",
        "transform = transforms.Compose(_FILL_)\n",
        "\n",
        "mnist_train_dataset = torchvision.datasets.MNIST(\n",
        "    _FILL_\n",
        "  )\n",
        "\n",
        "mnist_test_dataset = torchvision.datasets.MNIST(\n",
        "   _FILL_\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xGLuLaEXyzoD"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "LR = 0.1\n",
        "L1_WEIGHT = 1e-10\n",
        "L2_WEIGHT = 1e-12\n",
        "EPOCHS = 20\n",
        "# Get the dataloader for train and test\n",
        "train_dl = _FILL_\n",
        "test_dl = _FILL_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PjLznvm8xqaT"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cnn1 = _FILL_\n",
        "    self.cnn2 = _FILL_\n",
        "    self.cnn3 = _FILL_\n",
        "    self.linear = _FILL_\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Flatten x to be of last dimension 784\n",
        "    assert(x.shape == (BATCH_SIZE, 1, 28, 28))\n",
        "\n",
        "    # Pass through cnn layer 1\n",
        "    # (28, 28, 1) -> (26, 26, 32)\n",
        "    x = _FILL_\n",
        "    assert(x.shape == (BATCH_SIZE, 32, 26, 26))\n",
        "\n",
        "    # Pass through max pooling to give the result shape below\n",
        "    # (26, 26, 32) -> (13, 13, 32)\n",
        "    x = _FILL_\n",
        "    assert(x.shape == (BATCH_SIZE, 32, 13, 13))\n",
        "\n",
        "    # Apply ReLU\n",
        "    x = _FILL_\n",
        "\n",
        "    # Pass through cnn layer 2 to give the result below\n",
        "    # (13, 13, 32) -> (11, 11, 16)\n",
        "    x = _FILL_\n",
        "    assert(x.shape == (BATCH_SIZE, 16, 11, 11))\n",
        "\n",
        "    # Pass through max pooling pool to give the result below\n",
        "    # (11, 11, 16) -> (5, 5, 16)\n",
        "    x = _FILL_\n",
        "    assert(x.shape == (BATCH_SIZE, 16, 5, 5))\n",
        "\n",
        "    # Apply rely\n",
        "    x = _FILL_\n",
        "\n",
        "    # Pass through cnn layer 3 to give the result below\n",
        "    # (5, 5, 16) -> (5, 5, 1)\n",
        "    x = _FILL_\n",
        "    assert(x.shape == (BATCH_SIZE, 1, 5, 5))\n",
        "\n",
        "    # Apply rely\n",
        "    x = _FILL_\n",
        "\n",
        "    # Flatten to get the result below\n",
        "    # (5, 5, 1) - > (25, )\n",
        "    x = _FILL_\n",
        "    assert(x.shape == (BATCH_SIZE, 25))\n",
        "\n",
        "    # Pass through linear layer to get the result below\n",
        "    # (25, ) -> (16, )\n",
        "    x = _FILL_\n",
        "    assert(x.shape == (BATCH_SIZE, 10))\n",
        "\n",
        "    # Return the logits\n",
        "    return _FILL_\n",
        "\n",
        "model = CNNClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zu4Z0aptxqcu"
      },
      "outputs": [],
      "source": [
        "# Get the loss function; remember you are outputting the logits\n",
        "loss_fn = _FILL_\n",
        "\n",
        "# Set the optimizer to SGD and let the learning rate be LR\n",
        "# Do not add L2 regularization; add it manually below ...\n",
        "optimizer = _FILL_\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "for epoch in range(EPOCHS):\n",
        "    accuracy_hist_train = 0\n",
        "    auroc_hist_train = 0.0\n",
        "    loss_hist_train = 0\n",
        "    # Loop through the x and y pairs of data\n",
        "    for x_batch, y_batch in train_dl:\n",
        "        # Get he the model predictions\n",
        "        y_pred = _FILL_\n",
        "\n",
        "        # Get the loss\n",
        "        loss = _FILL_\n",
        "\n",
        "        # Add an L1 regularizaton with a weight of L1_WEIGHT to the objective\n",
        "        l1_reg = _FILL_\n",
        "\n",
        "        # Add an L2 regularization with a weight of L2_WEIGHT to the objective\n",
        "        l2_reg = _FILL_\n",
        "\n",
        "        # Add the regularizers to the objective\n",
        "        loss += _FILL_\n",
        "\n",
        "        # Get the gradients\n",
        "        _FILL_\n",
        "\n",
        "        # Add to the loss\n",
        "        # Remember loss: is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
        "        loss_hist_train += _FILL_\n",
        "\n",
        "        # Update the parameters\n",
        "        _FILL_\n",
        "\n",
        "        # Zero out the gradient\n",
        "        _FILL_\n",
        "\n",
        "        # Get the number of correct predictions, do this directly\n",
        "        is_correct = _FILL_\n",
        "\n",
        "        accuracy_hist_train += is_correct\n",
        "    accuracy_hist_train /= len(train_dl.dataset)\n",
        "    loss_hist_train /= len(train_dl.dataset)\n",
        "    print(f'Train Metrics Epoch {epoch} Loss {loss_hist_train:.4f} Accuracy {accuracy_hist_train:.4f}')\n",
        "\n",
        "    accuracy_hist_test = 0\n",
        "    loss_hist_test = 00\n",
        "    # Get the average value of each metric across the test batches\n",
        "    with torch.no_grad():\n",
        "      accuracy_hist_test = 0\n",
        "      auroc_hist_test = 0.0\n",
        "      # Loop through the x and y pairs of data\n",
        "      for x_batch, y_batch in test_dl:\n",
        "          # Get he the model predictions\n",
        "          y_batch_pred = _FILL_\n",
        "\n",
        "          # Get the loss\n",
        "          loss = _FILL_\n",
        "\n",
        "          # Add an L1 regularizaton with a weight of L1_WEIGHT to the objective\n",
        "          l1_reg = _FILL_\n",
        "\n",
        "          # Add an L2 regularization with a weight of L2_WEIGHT to the objective\n",
        "          l2_reg = _FILL_\n",
        "\n",
        "          # Add the regularizers to the objective\n",
        "          loss += _FILL_\n",
        "\n",
        "          # Add to the loss\n",
        "          # Remember loss: is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
        "          loss_hist_test += _FILL_\n",
        "\n",
        "          # Get the number of correct predictions\n",
        "          is_correct = _FILL_\n",
        "\n",
        "          # Get the accuracy\n",
        "          accuracy_hist_test += is_correct\n",
        "\n",
        "      # Normalize the metrics by the right number\n",
        "      accuracy_hist_test /= len(test_dl.dataset)\n",
        "      loss_hist_test /= len(test_dl.dataset)\n",
        "      print(f'Test Metrics Epoch {epoch} Loss {loss_hist_test:.4f} Accuracy {accuracy_hist_test:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}