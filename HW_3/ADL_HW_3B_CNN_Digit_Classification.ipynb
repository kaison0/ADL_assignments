{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRfgHr2GyNTO"
      },
      "source": [
        "For this notebook, please insert where there is `_FILL_` either code or logic to make this work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5XzVh-J0-fu"
      },
      "source": [
        "# MNIST CNN Digit Recognition Network\n",
        "\n",
        "For this problem, you will code a basic digit recognition network. The data are images which specify the digits 1 to 10 as (1, 28, 28) data - this data is black and white images. Each pixed of the image is an intensity between 0 and 255, and together the (1, 28, 28) pixel image can be visualized as a picture of a digit. The data is given to you as $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}$ where $y$ is the given label and x is the (1, 28, 28) data. This data will be gotten from `torchvision`, a repository of computer vision data and models.\n",
        "\n",
        "Highlevel, the model and notebook goes as follows:\n",
        "*   You first download the data and specify the batch size of B = 16. Each image will need to be turned from a (1, 28, 28) volume into a serious of other volumes either via convolutional layers or max pooling layers.\n",
        "*   You will pass the data through several layers to built a CNN classfier. Use the hints below to get the right dimensions and figure out what the layers should be. Be careful with the loss function. Add regularization (L1 and L2) manually.\n",
        "\n",
        "See the comments below and fill in the analysis where there is `_FILL_` specified. All asserts should pass and Test accuracy should be about 95%.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7Ftf9cgvf16r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0bba828-10e0-4856-c5a0-454b63a1ff27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torchtune as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torchvision 0.18.0\n",
            "Uninstalling torchvision-0.18.0:\n",
            "  Successfully uninstalled torchvision-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall --yes torchtune torchaudio torchvision\n",
        "!pip install torch==2.3.0 torchtext==0.18.0 torchdata==0.8.0 portalocker>=2.0.0 torchvision==0.18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8UDIb4ldyj2C"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PdLoOr08AUY5"
      },
      "outputs": [],
      "source": [
        "SEED = 1\n",
        "torch.manual_seed(SEED)\n",
        "_FILL_ = '_FILL_'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jLD0oQmgxxlR"
      },
      "outputs": [],
      "source": [
        "image_path = './'\n",
        "\n",
        "# Use ToTensor\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "mnist_train_dataset = torchvision.datasets.MNIST(\n",
        "  root=image_path,\n",
        "  train=True,\n",
        "  transform=transform,\n",
        "  download=True\n",
        ")\n",
        "\n",
        "mnist_test_dataset = torchvision.datasets.MNIST(\n",
        "   root=image_path,\n",
        "   train=False,\n",
        "   transform=transform,\n",
        "   download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"mnist train {mnist_train_dataset}  mnist_test_dataset {mnist_test_dataset}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULwJ8kIf04zO",
        "outputId": "b1b25f49-ed87-4c0b-d2ba-397bd03de9d3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mnist train Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )  mnist_test_dataset Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xGLuLaEXyzoD"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "LR = 0.1\n",
        "L1_WEIGHT = 1e-10\n",
        "L2_WEIGHT = 1e-12\n",
        "EPOCHS = 20\n",
        "# Get the dataloader for train and test\n",
        "train_dl = DataLoader(mnist_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dl = DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "PjLznvm8xqaT"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cnn1 = nn.Conv2d(1, 32, 3, stride=1)\n",
        "    self.cnn2 = nn.Conv2d(32, 16, 3, stride=1)\n",
        "    self.cnn3 = nn.Conv2d(16, 1, 3, stride=1, padding=1)\n",
        "    self.linear = nn.Linear(25, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Flatten x to be of last dimension 784\n",
        "    assert(x.shape == (BATCH_SIZE, 1, 28, 28))\n",
        "\n",
        "    # Pass through cnn layer 1\n",
        "    # (28, 28, 1) -> (26, 26, 32)\n",
        "    x = self.cnn1(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 32, 26, 26))\n",
        "\n",
        "    # Pass through max pooling to give the result shape below\n",
        "    # (26, 26, 32) -> (13, 13, 32)\n",
        "    x = nn.MaxPool2d(2, stride=2)(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 32, 13, 13))\n",
        "\n",
        "    # Apply ReLU\n",
        "    x = nn.functional.relu(x)\n",
        "\n",
        "    # Pass through cnn layer 2 to give the result below\n",
        "    # (13, 13, 32) -> (11, 11, 16)\n",
        "    x = self.cnn2(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 16, 11, 11))\n",
        "\n",
        "    # Pass through max pooling pool to give the result below\n",
        "    # (11, 11, 16) -> (5, 5, 16)\n",
        "    x = nn.MaxPool2d(2, stride=2)(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 16, 5, 5))\n",
        "\n",
        "    # Apply rely\n",
        "    x = nn.functional.relu(x)\n",
        "\n",
        "    # Pass through cnn layer 3 to give the result below\n",
        "    # (5, 5, 16) -> (5, 5, 1)\n",
        "    x = self.cnn3(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 1, 5, 5))\n",
        "\n",
        "    # Apply rely\n",
        "    x = nn.functional.relu(x)\n",
        "\n",
        "    # Flatten to get the result below\n",
        "    # (5, 5, 1) - > (25, )\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "    assert(x.shape == (BATCH_SIZE, 25))\n",
        "\n",
        "    # Pass through linear layer to get the result below\n",
        "    # (25, ) -> (10, )\n",
        "    x = self.linear(x)\n",
        "    assert(x.shape == (BATCH_SIZE, 10))\n",
        "\n",
        "    # Return the logits\n",
        "    return x\n",
        "\n",
        "model = CNNClassifier().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu4Z0aptxqcu",
        "outputId": "17ee898c-0a47-4249-f8ad-2bed194640fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Metrics Epoch 0 Loss 0.0184 Accuracy 0.9076\n",
            "Test Metrics Epoch 0 Loss 0.0083 Accuracy 0.9593\n",
            "Train Metrics Epoch 1 Loss 0.0093 Accuracy 0.9539\n",
            "Test Metrics Epoch 1 Loss 0.0066 Accuracy 0.9675\n",
            "Train Metrics Epoch 2 Loss 0.0078 Accuracy 0.9622\n",
            "Test Metrics Epoch 2 Loss 0.0068 Accuracy 0.9674\n",
            "Train Metrics Epoch 3 Loss 0.0070 Accuracy 0.9664\n",
            "Test Metrics Epoch 3 Loss 0.0065 Accuracy 0.9698\n",
            "Train Metrics Epoch 4 Loss 0.0064 Accuracy 0.9679\n",
            "Test Metrics Epoch 4 Loss 0.0060 Accuracy 0.9717\n",
            "Train Metrics Epoch 5 Loss 0.0060 Accuracy 0.9705\n",
            "Test Metrics Epoch 5 Loss 0.0059 Accuracy 0.9704\n",
            "Train Metrics Epoch 6 Loss 0.0058 Accuracy 0.9716\n",
            "Test Metrics Epoch 6 Loss 0.0051 Accuracy 0.9753\n",
            "Train Metrics Epoch 7 Loss 0.0056 Accuracy 0.9724\n",
            "Test Metrics Epoch 7 Loss 0.0052 Accuracy 0.9754\n",
            "Train Metrics Epoch 8 Loss 0.0054 Accuracy 0.9728\n",
            "Test Metrics Epoch 8 Loss 0.0054 Accuracy 0.9729\n",
            "Train Metrics Epoch 9 Loss 0.0051 Accuracy 0.9751\n",
            "Test Metrics Epoch 9 Loss 0.0044 Accuracy 0.9795\n",
            "Train Metrics Epoch 10 Loss 0.0050 Accuracy 0.9748\n",
            "Test Metrics Epoch 10 Loss 0.0055 Accuracy 0.9731\n",
            "Train Metrics Epoch 11 Loss 0.0048 Accuracy 0.9761\n",
            "Test Metrics Epoch 11 Loss 0.0062 Accuracy 0.9702\n",
            "Train Metrics Epoch 12 Loss 0.0047 Accuracy 0.9766\n",
            "Test Metrics Epoch 12 Loss 0.0047 Accuracy 0.9774\n",
            "Train Metrics Epoch 13 Loss 0.0046 Accuracy 0.9772\n",
            "Test Metrics Epoch 13 Loss 0.0043 Accuracy 0.9795\n"
          ]
        }
      ],
      "source": [
        "# Get the loss function; remember you are outputting the logits\n",
        "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "# Set the optimizer to SGD and let the learning rate be LR\n",
        "# Do not add L2 regularization; add it manually below ...\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "model.train()\n",
        "torch.manual_seed(SEED)\n",
        "for epoch in range(EPOCHS):\n",
        "    accuracy_hist_train = 0.0\n",
        "    loss_hist_train = 0.0\n",
        "    # Loop through the x and y pairs of data\n",
        "    for x_batch, y_batch in train_dl:\n",
        "        # Get he the model predictions\n",
        "        y_pred = model(x_batch)\n",
        "        # print(\"logits mean/std:\", y_pred.mean().item(), y_pred.std().item())\n",
        "\n",
        "        # print(f\" model predict {y_pred} label {y_batch}\")\n",
        "        # Get the loss\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        # print(f\"y_pred {y_pred[0:2]} y_batch: {y_batch[0:2]} loss: {loss}\")\n",
        "\n",
        "        # Add an L1 regularizaton with a weight of L1_WEIGHT to the objective\n",
        "        l1_reg = L1_WEIGHT * sum(p.abs().sum() for p in model.parameters())\n",
        "\n",
        "        # Add an L2 regularization with a weight of L2_WEIGHT to the objective\n",
        "        l2_reg = L2_WEIGHT * sum((p**2).sum() for p in model.parameters())\n",
        "\n",
        "        # Add the regularizers to the objective\n",
        "        loss += l1_reg + l2_reg\n",
        "\n",
        "        # Get the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Add to the loss\n",
        "        # Remember loss: is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
        "        loss_hist_train += loss\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero out the gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the number of correct predictions, do this directly\n",
        "        # print(f\" prediction {torch.argmax(y_pred, dim=1)} truth {y_batch}\")\n",
        "        is_correct = (torch.argmax(y_pred, dim=1) == y_batch).sum()\n",
        "\n",
        "        accuracy_hist_train += is_correct\n",
        "\n",
        "        # print(f\"loss in epoch {loss} {accuracy_hist_train}\")\n",
        "\n",
        "    accuracy_hist_train /= len(train_dl.dataset)\n",
        "    loss_hist_train /= len(train_dl.dataset)\n",
        "    print(f'Train Metrics Epoch {epoch} Loss {loss_hist_train:.4f} Accuracy {accuracy_hist_train:.4f}')\n",
        "\n",
        "    # Get the average value of each metric across the test batches\n",
        "    with torch.no_grad():\n",
        "      loss_hist_test = 0.0\n",
        "      accuracy_hist_test = 0.0\n",
        "      # Loop through the x and y pairs of data\n",
        "      for x_batch, y_batch in test_dl:\n",
        "          # Get he the model predictions\n",
        "          y_batch_pred = model(x_batch)\n",
        "\n",
        "          # Get the loss\n",
        "          loss = loss_fn(y_batch_pred, y_batch)\n",
        "\n",
        "          # Add an L1 regularizaton with a weight of L1_WEIGHT to the objective\n",
        "          l1_reg = L1_WEIGHT * sum(p.abs().sum() for p in model.parameters())\n",
        "\n",
        "          # Add an L2 regularization with a weight of L2_WEIGHT to the objective\n",
        "          l2_reg = L2_WEIGHT * sum((p**2).sum() for p in model.parameters())\n",
        "\n",
        "          # Add the regularizers to the objective\n",
        "          loss += l1_reg + l2_reg\n",
        "\n",
        "          # Add to the loss\n",
        "          # Remember loss: is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
        "          loss_hist_test += loss\n",
        "\n",
        "          # Get the number of correct predictions\n",
        "          is_correct = (torch.argmax(y_batch_pred, dim=1) == y_batch).sum()\n",
        "\n",
        "          # Get the accuracy\n",
        "          accuracy_hist_test += is_correct\n",
        "\n",
        "      # Normalize the metrics by the right number\n",
        "      accuracy_hist_test /= len(test_dl.dataset)\n",
        "      loss_hist_test /= len(test_dl.dataset)\n",
        "      print(f'Test Metrics Epoch {epoch} Loss {loss_hist_test:.4f} Accuracy {accuracy_hist_test:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}