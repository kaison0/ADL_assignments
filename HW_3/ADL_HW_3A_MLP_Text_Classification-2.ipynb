{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall --yes torchtune torchaudio torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "288evvQ4H-bH",
        "outputId": "d9813f0e-98ba-4c0d-c0ff-707ec042a7fe"
      },
      "id": "288evvQ4H-bH",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtune 0.6.1\n",
            "Uninstalling torchtune-0.6.1:\n",
            "  Successfully uninstalled torchtune-0.6.1\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.0 torchtext==0.18.0 torchdata==0.8.0 portalocker>=2.0.0"
      ],
      "metadata": {
        "id": "ybx6U-fuIEzb",
        "outputId": "39925fb1-a494-4f58-facf-d7a08faea811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ybx6U-fuIEzb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.5 requires torchvision>=0.11, which is not installed.\n",
            "timm 1.0.21 requires torchvision, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "cbgu6eWfXN49",
        "outputId": "0bea41bd-0c71-4ec7-d478-2a514605792a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cbgu6eWfXN49",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.3.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import torchdata\n",
        "torch.__version__, torchtext.__version__, torchdata.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubo2bJ5bH5Fc",
        "outputId": "62e99d3c-2432-4a90-ccf0-6234b3d8fa84"
      },
      "id": "Ubo2bJ5bH5Fc",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.3.0+cu121', '0.18.0+cpu', '0.8.0+cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OzgMJwpmc0iC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9b4694-375d-4284-ce47-950b31e413d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import logging\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import torchtext\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
        "from torchtext.datasets import DATASETS\n",
        "from torchtext.utils import download_from_url\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "_FILL_ = '_FILL_'\n",
        "SEED = 1"
      ],
      "id": "OzgMJwpmc0iC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh4pAKXgy0tH"
      },
      "source": [
        "For any of these questions, insert code where there is `_FILL_` so that this notebooks runs correctly."
      ],
      "id": "kh4pAKXgy0tH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHdrXN4d1M_2"
      },
      "source": [
        "Short Question\n",
        "\n",
        "Set up the optimization problem below where we take a random y of data and want theta to converge to this y."
      ],
      "id": "fHdrXN4d1M_2"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OsAbUVezIwD",
        "outputId": "e1e5aaa4-e64e-47cd-b82c-03cfcfa0df0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0 Loss: 0.22048303484916687\n",
            "Epoch:1 Loss: 0.21175193786621094\n",
            "Epoch:2 Loss: 0.20336654782295227\n",
            "Epoch:3 Loss: 0.1953132301568985\n",
            "Epoch:4 Loss: 0.18757882714271545\n",
            "Epoch:5 Loss: 0.18015070259571075\n",
            "Epoch:6 Loss: 0.173016756772995\n",
            "Epoch:7 Loss: 0.1661653071641922\n",
            "Epoch:8 Loss: 0.1595851331949234\n",
            "Epoch:9 Loss: 0.153265580534935\n",
            "Epoch:10 Loss: 0.14719624817371368\n",
            "Epoch:11 Loss: 0.14136730134487152\n",
            "Epoch:12 Loss: 0.13576911389827728\n",
            "Epoch:13 Loss: 0.1303926706314087\n",
            "Epoch:14 Loss: 0.1252291053533554\n",
            "Epoch:15 Loss: 0.12027003616094589\n",
            "Epoch:16 Loss: 0.11550736427307129\n",
            "Epoch:17 Loss: 0.11093325167894363\n",
            "Epoch:18 Loss: 0.10654031485319138\n",
            "Epoch:19 Loss: 0.1023213118314743\n",
            "Epoch:20 Loss: 0.09826936572790146\n",
            "Epoch:21 Loss: 0.09437789022922516\n",
            "Epoch:22 Loss: 0.09064052999019623\n",
            "Epoch:23 Loss: 0.0870511382818222\n",
            "Epoch:24 Loss: 0.08360393345355988\n",
            "Epoch:25 Loss: 0.08029323071241379\n",
            "Epoch:26 Loss: 0.07711360603570938\n",
            "Epoch:27 Loss: 0.07405988872051239\n",
            "Epoch:28 Loss: 0.07112710922956467\n",
            "Epoch:29 Loss: 0.0683104619383812\n",
            "Epoch:30 Loss: 0.06560537219047546\n",
            "Epoch:31 Loss: 0.0630073994398117\n",
            "Epoch:32 Loss: 0.060512322932481766\n",
            "Epoch:33 Loss: 0.05811603367328644\n",
            "Epoch:34 Loss: 0.05581463873386383\n",
            "Epoch:35 Loss: 0.0536043718457222\n",
            "Epoch:36 Loss: 0.05148164555430412\n",
            "Epoch:37 Loss: 0.04944297671318054\n",
            "Epoch:38 Loss: 0.04748504236340523\n",
            "Epoch:39 Loss: 0.04560462757945061\n",
            "Epoch:40 Loss: 0.04379867762327194\n",
            "Epoch:41 Loss: 0.04206424206495285\n",
            "Epoch:42 Loss: 0.0403984971344471\n",
            "Epoch:43 Loss: 0.03879871219396591\n",
            "Epoch:44 Loss: 0.037262290716171265\n",
            "Epoch:45 Loss: 0.0357867032289505\n",
            "Epoch:46 Loss: 0.03436955437064171\n",
            "Epoch:47 Loss: 0.03300851210951805\n",
            "Epoch:48 Loss: 0.031701378524303436\n",
            "Epoch:49 Loss: 0.030446002259850502\n",
            "Epoch:50 Loss: 0.02924034371972084\n",
            "Epoch:51 Loss: 0.02808242477476597\n",
            "Epoch:52 Loss: 0.026970358565449715\n",
            "Epoch:53 Loss: 0.02590232342481613\n",
            "Epoch:54 Loss: 0.02487660013139248\n",
            "Epoch:55 Loss: 0.023891478776931763\n",
            "Epoch:56 Loss: 0.022945379838347435\n",
            "Epoch:57 Loss: 0.022036734968423843\n",
            "Epoch:58 Loss: 0.021164076402783394\n",
            "Epoch:59 Loss: 0.020325977355241776\n",
            "Epoch:60 Loss: 0.019521065056324005\n",
            "Epoch:61 Loss: 0.018748033791780472\n",
            "Epoch:62 Loss: 0.01800561510026455\n",
            "Epoch:63 Loss: 0.017292596399784088\n",
            "Epoch:64 Loss: 0.016607806086540222\n",
            "Epoch:65 Loss: 0.01595013402402401\n",
            "Epoch:66 Loss: 0.015318509191274643\n",
            "Epoch:67 Loss: 0.014711892232298851\n",
            "Epoch:68 Loss: 0.014129300601780415\n",
            "Epoch:69 Loss: 0.013569780625402927\n",
            "Epoch:70 Loss: 0.013032417744398117\n",
            "Epoch:71 Loss: 0.012516333721578121\n",
            "Epoch:72 Loss: 0.012020686641335487\n",
            "Epoch:73 Loss: 0.01154466811567545\n",
            "Epoch:74 Loss: 0.01108749769628048\n",
            "Epoch:75 Loss: 0.010648432187736034\n",
            "Epoch:76 Loss: 0.01022675447165966\n",
            "Epoch:77 Loss: 0.009821776300668716\n",
            "Epoch:78 Loss: 0.009432836435735226\n",
            "Epoch:79 Loss: 0.009059296920895576\n",
            "Epoch:80 Loss: 0.008700547739863396\n",
            "Epoch:81 Loss: 0.008356007747352123\n",
            "Epoch:82 Loss: 0.008025111630558968\n",
            "Epoch:83 Loss: 0.007707316428422928\n",
            "Epoch:84 Loss: 0.007402104325592518\n",
            "Epoch:85 Loss: 0.007108980789780617\n",
            "Epoch:86 Loss: 0.00682746572420001\n",
            "Epoch:87 Loss: 0.006557097192853689\n",
            "Epoch:88 Loss: 0.006297435145825148\n",
            "Epoch:89 Loss: 0.0060480572283267975\n",
            "Epoch:90 Loss: 0.005808555521070957\n",
            "Epoch:91 Loss: 0.005578537005931139\n",
            "Epoch:92 Loss: 0.005357626359909773\n",
            "Epoch:93 Loss: 0.005145462695509195\n",
            "Epoch:94 Loss: 0.0049417042173445225\n",
            "Epoch:95 Loss: 0.004746011458337307\n",
            "Epoch:96 Loss: 0.0045580691657960415\n",
            "Epoch:97 Loss: 0.004377569537609816\n",
            "Epoch:98 Loss: 0.004204219207167625\n",
            "Epoch:99 Loss: 0.004037732724100351\n",
            "Epoch:100 Loss: 0.003877837909385562\n",
            "Epoch:101 Loss: 0.003724275855347514\n",
            "Epoch:102 Loss: 0.0035767932422459126\n",
            "Epoch:103 Loss: 0.0034351504873484373\n",
            "Epoch:104 Loss: 0.0032991187181323767\n",
            "Epoch:105 Loss: 0.0031684753485023975\n",
            "Epoch:106 Loss: 0.003043003845959902\n",
            "Epoch:107 Loss: 0.0029225002508610487\n",
            "Epoch:108 Loss: 0.002806768985465169\n",
            "Epoch:109 Loss: 0.0026956198271363974\n",
            "Epoch:110 Loss: 0.002588873030617833\n",
            "Epoch:111 Loss: 0.002486353972926736\n",
            "Epoch:112 Loss: 0.002387894783169031\n",
            "Epoch:113 Loss: 0.0022933350410312414\n",
            "Epoch:114 Loss: 0.002202519215643406\n",
            "Epoch:115 Loss: 0.0021153001580387354\n",
            "Epoch:116 Loss: 0.0020315339788794518\n",
            "Epoch:117 Loss: 0.0019510859856382012\n",
            "Epoch:118 Loss: 0.001873823581263423\n",
            "Epoch:119 Loss: 0.0017996206879615784\n",
            "Epoch:120 Loss: 0.0017283557681366801\n",
            "Epoch:121 Loss: 0.0016599128721281886\n",
            "Epoch:122 Loss: 0.001594180939719081\n",
            "Epoch:123 Loss: 0.0015310512389987707\n",
            "Epoch:124 Loss: 0.001470422837883234\n",
            "Epoch:125 Loss: 0.0014121932908892632\n",
            "Epoch:126 Loss: 0.0013562694657593966\n",
            "Epoch:127 Loss: 0.0013025603257119656\n",
            "Epoch:128 Loss: 0.0012509783264249563\n",
            "Epoch:129 Loss: 0.0012014388339594007\n",
            "Epoch:130 Loss: 0.0011538625694811344\n",
            "Epoch:131 Loss: 0.0011081704869866371\n",
            "Epoch:132 Loss: 0.001064286450855434\n",
            "Epoch:133 Loss: 0.001022141077555716\n",
            "Epoch:134 Loss: 0.0009816644014790654\n",
            "Epoch:135 Loss: 0.0009427902405150235\n",
            "Epoch:136 Loss: 0.0009054566617123783\n",
            "Epoch:137 Loss: 0.0008696006261743605\n",
            "Epoch:138 Loss: 0.0008351639844477177\n",
            "Epoch:139 Loss: 0.0008020920213311911\n",
            "Epoch:140 Loss: 0.0007703290320932865\n",
            "Epoch:141 Loss: 0.0007398247253149748\n",
            "Epoch:142 Loss: 0.0007105268305167556\n",
            "Epoch:143 Loss: 0.0006823905277997255\n",
            "Epoch:144 Loss: 0.0006553673883900046\n",
            "Epoch:145 Loss: 0.0006294143386185169\n",
            "Epoch:146 Loss: 0.0006044899928383529\n",
            "Epoch:147 Loss: 0.0005805515684187412\n",
            "Epoch:148 Loss: 0.0005575615796260536\n",
            "Epoch:149 Loss: 0.0005354814929887652\n",
            "Epoch:150 Loss: 0.0005142761510796845\n",
            "Epoch:151 Loss: 0.0004939108621329069\n",
            "Epoch:152 Loss: 0.00047435189480893314\n",
            "Epoch:153 Loss: 0.0004555674677249044\n",
            "Epoch:154 Loss: 0.00043752705096267164\n",
            "Epoch:155 Loss: 0.00042020060936920345\n",
            "Epoch:156 Loss: 0.0004035603778902441\n",
            "Epoch:157 Loss: 0.00038757931906729937\n",
            "Epoch:158 Loss: 0.0003722315304912627\n",
            "Epoch:159 Loss: 0.0003574914007913321\n",
            "Epoch:160 Loss: 0.00034333515213802457\n",
            "Epoch:161 Loss: 0.00032973906490951777\n",
            "Epoch:162 Loss: 0.00031668099109083414\n",
            "Epoch:163 Loss: 0.00030414017965085804\n",
            "Epoch:164 Loss: 0.00029209599597379565\n",
            "Epoch:165 Loss: 0.00028052920242771506\n",
            "Epoch:166 Loss: 0.0002694204158615321\n",
            "Epoch:167 Loss: 0.0002587518247310072\n",
            "Epoch:168 Loss: 0.00024850532645359635\n",
            "Epoch:169 Loss: 0.00023866436094976962\n",
            "Epoch:170 Loss: 0.0002292133867740631\n",
            "Epoch:171 Loss: 0.00022013633861206472\n",
            "Epoch:172 Loss: 0.0002114188246196136\n",
            "Epoch:173 Loss: 0.0002030463656410575\n",
            "Epoch:174 Loss: 0.00019500544294714928\n",
            "Epoch:175 Loss: 0.00018728333816397935\n",
            "Epoch:176 Loss: 0.00017986686725635082\n",
            "Epoch:177 Loss: 0.00017274465062655509\n",
            "Epoch:178 Loss: 0.00016590405721217394\n",
            "Epoch:179 Loss: 0.00015933434769976884\n",
            "Epoch:180 Loss: 0.00015302472456824034\n",
            "Epoch:181 Loss: 0.00014696513244416565\n",
            "Epoch:182 Loss: 0.00014114519581198692\n",
            "Epoch:183 Loss: 0.00013555612531490624\n",
            "Epoch:184 Loss: 0.0001301880256505683\n",
            "Epoch:185 Loss: 0.00012503295147325844\n",
            "Epoch:186 Loss: 0.00012008164048893377\n",
            "Epoch:187 Loss: 0.00011532647476997226\n",
            "Epoch:188 Loss: 0.00011075971269747242\n",
            "Epoch:189 Loss: 0.00010637361992849037\n",
            "Epoch:190 Loss: 0.00010216100054094568\n",
            "Epoch:191 Loss: 9.811528434511274e-05\n",
            "Epoch:192 Loss: 9.422990115126595e-05\n",
            "Epoch:193 Loss: 9.049828804563731e-05\n",
            "Epoch:194 Loss: 8.691455877851695e-05\n",
            "Epoch:195 Loss: 8.347280527232215e-05\n",
            "Epoch:196 Loss: 8.016727224458009e-05\n",
            "Epoch:197 Loss: 7.699256093474105e-05\n",
            "Epoch:198 Loss: 7.394376007141545e-05\n",
            "Epoch:199 Loss: 7.101573282852769e-05\n",
            "tensor([[0.7576, 0.2793, 0.4031]])\n",
            "tensor([[0.7572, 0.2749, 0.4100]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "from torch._functorch.vmap import lazy_load_decompositions\n",
        "import torch.nn.functional as F\n",
        "# Short Question\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "# Define y to be a target of dimension (1, 3) without a gradient\n",
        "y = torch.rand(1, 3)\n",
        "\n",
        "# Define theta to be a random tensor of dimension (1, 3) which requires a gradient; we want theta to converge to y\n",
        "theta = torch.rand(1, 3)\n",
        "theta.requires_grad_(True)\n",
        "\n",
        "\n",
        "# Define an SGD optimizer with learning rate 0.01 which acts on theta\n",
        "optimizer = torch.optim.SGD([theta], lr=0.01)\n",
        "\n",
        "# Fil in the code below using the optimizer above to get theta to converge to y\n",
        "for epoch in range(200):\n",
        "  # Zero out the gradients of l with respect to theta\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Define a loss manually which is ||theta-x||_{2}^{2}, the L2 loss across all components\n",
        "  loss = torch.norm(y-theta) ** 2\n",
        "\n",
        "  print('Epoch:{} Loss: {}'.format(epoch, loss))\n",
        "\n",
        "  # Get teh gradients of l with respect to theta\n",
        "  loss.backward()\n",
        "\n",
        "  # Update theta\n",
        "  optimizer.step()\n",
        "\n",
        "# These should look very similar\n",
        "print(y)\n",
        "print(theta)\n",
        "with torch.no_grad():\n",
        "  # Check the y and theta have converged to almost the same thing\n",
        "  loss = torch.norm(y-theta) ** 2\n",
        "  assert (loss.item() - 0.0)**2 <= 0.001"
      ],
      "id": "8OsAbUVezIwD"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MgA6DK8R3BUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5ab436-6c51-4248-cb15-46a5298e4085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0 Loss: 0.22048303484916687\n",
            "Epoch:1 Loss: 0.21175193786621094\n",
            "Epoch:2 Loss: 0.19498465955257416\n",
            "Epoch:3 Loss: 0.17151597142219543\n",
            "Epoch:4 Loss: 0.1432139128446579\n",
            "Epoch:5 Loss: 0.1123313382267952\n",
            "Epoch:6 Loss: 0.08132652938365936\n",
            "Epoch:7 Loss: 0.052667442709207535\n",
            "Epoch:8 Loss: 0.028635352849960327\n",
            "Epoch:9 Loss: 0.01114320382475853\n",
            "Epoch:10 Loss: 0.0015833789948374033\n",
            "Epoch:11 Loss: 0.0007168386364355683\n",
            "Epoch:12 Loss: 0.008612564764916897\n",
            "Epoch:13 Loss: 0.02464204467833042\n",
            "Epoch:14 Loss: 0.04752934351563454\n",
            "Epoch:15 Loss: 0.07545263320207596\n",
            "Epoch:16 Loss: 0.10618922114372253\n",
            "Epoch:17 Loss: 0.13729245960712433\n",
            "Epoch:18 Loss: 0.16628655791282654\n",
            "Epoch:19 Loss: 0.19086354970932007\n",
            "Epoch:20 Loss: 0.20906715095043182\n",
            "Epoch:21 Loss: 0.2194482833147049\n",
            "Epoch:22 Loss: 0.2211807370185852\n",
            "Epoch:23 Loss: 0.21412649750709534\n",
            "Epoch:24 Loss: 0.19884708523750305\n",
            "Epoch:25 Loss: 0.17655879259109497\n",
            "Epoch:26 Loss: 0.14903569221496582\n",
            "Epoch:27 Loss: 0.11846872419118881\n",
            "Epoch:28 Loss: 0.08729098737239838\n",
            "Epoch:29 Loss: 0.05798417702317238\n",
            "Epoch:30 Loss: 0.03288116306066513\n",
            "Epoch:31 Loss: 0.013980129733681679\n",
            "Epoch:32 Loss: 0.002785602817311883\n",
            "Epoch:33 Loss: 0.00018866524624172598\n",
            "Epoch:34 Loss: 0.006396032869815826\n",
            "Epoch:35 Loss: 0.020913604646921158\n",
            "Epoch:36 Loss: 0.042585764080286026\n",
            "Epoch:37 Loss: 0.06968742609024048\n",
            "Epoch:38 Loss: 0.10006130486726761\n",
            "Epoch:39 Loss: 0.13128960132598877\n",
            "Epoch:40 Loss: 0.16088660061359406\n",
            "Epoch:41 Loss: 0.18649634718894958\n",
            "Epoch:42 Loss: 0.20608027279376984\n",
            "Epoch:43 Loss: 0.21807953715324402\n",
            "Epoch:44 Loss: 0.22153902053833008\n",
            "Epoch:45 Loss: 0.21618328988552094\n",
            "Epoch:46 Loss: 0.20243871212005615\n",
            "Epoch:47 Loss: 0.18139933049678802\n",
            "Epoch:48 Loss: 0.1547398865222931\n",
            "Epoch:49 Loss: 0.12458249926567078\n",
            "Epoch:50 Loss: 0.09332762658596039\n",
            "Epoch:51 Loss: 0.0634632259607315\n",
            "Epoch:52 Loss: 0.03736645355820656\n",
            "Epoch:53 Loss: 0.01711464114487171\n",
            "Epoch:54 Loss: 0.0043198224157094955\n",
            "Epoch:55 Loss: 4.7159662130979996e-07\n",
            "Epoch:56 Loss: 0.004500404931604862\n",
            "Epoch:57 Loss: 0.017461426556110382\n",
            "Epoch:58 Loss: 0.03785184398293495\n",
            "Epoch:59 Loss: 0.06404857337474823\n",
            "Epoch:60 Loss: 0.09396635740995407\n",
            "Epoch:61 Loss: 0.12522374093532562\n",
            "Epoch:62 Loss: 0.15533265471458435\n",
            "Epoch:63 Loss: 0.18189637362957\n",
            "Epoch:64 Loss: 0.202800452709198\n",
            "Epoch:65 Loss: 0.2163809835910797\n",
            "Epoch:66 Loss: 0.2215569168329239\n",
            "Epoch:67 Loss: 0.21791620552539825\n",
            "Epoch:68 Loss: 0.20574870705604553\n",
            "Epoch:69 Loss: 0.18602292239665985\n",
            "Epoch:70 Loss: 0.16030903160572052\n",
            "Epoch:71 Loss: 0.13065387308597565\n",
            "Epoch:72 Loss: 0.09941796958446503\n",
            "Epoch:73 Loss: 0.06908773630857468\n",
            "Epoch:74 Loss: 0.0420774407684803\n",
            "Epoch:75 Loss: 0.02053709700703621\n",
            "Epoch:76 Loss: 0.006181320175528526\n",
            "Epoch:77 Loss: 0.00015283444372471422\n",
            "Epoch:78 Loss: 0.002931505674496293\n",
            "Epoch:79 Loss: 0.0142961535602808\n",
            "Epoch:80 Loss: 0.0333421416580677\n",
            "Epoch:81 Loss: 0.058553434908390045\n",
            "Epoch:82 Loss: 0.08792319148778915\n",
            "Epoch:83 Loss: 0.11911357939243317\n",
            "Epoch:84 Loss: 0.14964185655117035\n",
            "Epoch:85 Loss: 0.17707796394824982\n",
            "Epoch:86 Loss: 0.19923795759677887\n",
            "Epoch:87 Loss: 0.21435795724391937\n",
            "Epoch:88 Loss: 0.22123439610004425\n",
            "Epoch:89 Loss: 0.21931986510753632\n",
            "Epoch:90 Loss: 0.20876681804656982\n",
            "Epoch:91 Loss: 0.19041524827480316\n",
            "Epoch:92 Loss: 0.1657259315252304\n",
            "Epoch:93 Loss: 0.13666419684886932\n",
            "Epoch:94 Loss: 0.10554327815771103\n",
            "Epoch:95 Loss: 0.07484046369791031\n",
            "Epoch:96 Loss: 0.04699963703751564\n",
            "Epoch:97 Loss: 0.024236973375082016\n",
            "Epoch:98 Loss: 0.008364369161427021\n",
            "Epoch:99 Loss: 0.0006452839588746428\n",
            "Epoch:100 Loss: 0.0016941531794145703\n",
            "Epoch:101 Loss: 0.011427485384047031\n",
            "Epoch:102 Loss: 0.029070517048239708\n",
            "Epoch:103 Loss: 0.05321885272860527\n",
            "Epoch:104 Loss: 0.08195028454065323\n",
            "Epoch:105 Loss: 0.11297780275344849\n",
            "Epoch:106 Loss: 0.14383161067962646\n",
            "Epoch:107 Loss: 0.17205573618412018\n",
            "Epoch:108 Loss: 0.1954035758972168\n",
            "Epoch:109 Loss: 0.21201655268669128\n",
            "Epoch:110 Loss: 0.2205723524093628\n",
            "Epoch:111 Loss: 0.2203899621963501\n",
            "Epoch:112 Loss: 0.21148379147052765\n",
            "Epoch:113 Loss: 0.1945628523826599\n",
            "Epoch:114 Loss: 0.17097404599189758\n",
            "Epoch:115 Loss: 0.14259502291679382\n",
            "Epoch:116 Loss: 0.11168473958969116\n",
            "Epoch:117 Loss: 0.08070368319749832\n",
            "Epoch:118 Loss: 0.052117932587862015\n",
            "Epoch:119 Loss: 0.02820291928946972\n",
            "Epoch:120 Loss: 0.01086227223277092\n",
            "Epoch:121 Loss: 0.0014763076324015856\n",
            "Epoch:122 Loss: 0.0007921507931314409\n",
            "Epoch:123 Loss: 0.008864258415997028\n",
            "Epoch:124 Loss: 0.025050101801753044\n",
            "Epoch:125 Loss: 0.04806126281619072\n",
            "Epoch:126 Loss: 0.07606607675552368\n",
            "Epoch:127 Loss: 0.10683534294366837\n",
            "Epoch:128 Loss: 0.1379198580980301\n",
            "Epoch:129 Loss: 0.16684526205062866\n",
            "Epoch:130 Loss: 0.1913090944290161\n",
            "Epoch:131 Loss: 0.20936404168605804\n",
            "Epoch:132 Loss: 0.21957294642925262\n",
            "Epoch:133 Loss: 0.22112315893173218\n",
            "Epoch:134 Loss: 0.21389128267765045\n",
            "Epoch:135 Loss: 0.19845294952392578\n",
            "Epoch:136 Loss: 0.17603713274002075\n",
            "Epoch:137 Loss: 0.1484280526638031\n",
            "Epoch:138 Loss: 0.11782342940568924\n",
            "Epoch:139 Loss: 0.08665937930345535\n",
            "Epoch:140 Loss: 0.057416558265686035\n",
            "Epoch:141 Loss: 0.032422710210084915\n",
            "Epoch:142 Loss: 0.013667330145835876\n",
            "Epoch:143 Loss: 0.0026433486491441727\n",
            "Epoch:144 Loss: 0.0002282706118421629\n",
            "Epoch:145 Loss: 0.00661433907225728\n",
            "Epoch:146 Loss: 0.02129322662949562\n",
            "Epoch:147 Loss: 0.043096497654914856\n",
            "Epoch:148 Loss: 0.07028859853744507\n",
            "Epoch:149 Loss: 0.10070502758026123\n",
            "Epoch:150 Loss: 0.13192467391490936\n",
            "Epoch:151 Loss: 0.1614624261856079\n",
            "Epoch:152 Loss: 0.18696708977222443\n",
            "Epoch:153 Loss: 0.2064085453748703\n",
            "Epoch:154 Loss: 0.21823912858963013\n",
            "Epoch:155 Loss: 0.22151722013950348\n",
            "Epoch:156 Loss: 0.2159818410873413\n",
            "Epoch:157 Loss: 0.20207363367080688\n",
            "Epoch:158 Loss: 0.1808997094631195\n",
            "Epoch:159 Loss: 0.15414544939994812\n",
            "Epoch:160 Loss: 0.12394054979085922\n",
            "Epoch:161 Loss: 0.09268929809331894\n",
            "Epoch:162 Loss: 0.0628792867064476\n",
            "Epoch:163 Loss: 0.03688341751694679\n",
            "Epoch:164 Loss: 0.016770945861935616\n",
            "Epoch:165 Loss: 0.0041428194381296635\n",
            "Epoch:166 Loss: 4.24563995693461e-06\n",
            "Epoch:167 Loss: 0.004684651270508766\n",
            "Epoch:168 Loss: 0.017811480909585953\n",
            "Epoch:169 Loss: 0.03833983838558197\n",
            "Epoch:170 Loss: 0.06463566422462463\n",
            "Epoch:171 Loss: 0.09460580348968506\n",
            "Epoch:172 Loss: 0.1258646547794342\n",
            "Epoch:173 Loss: 0.1559239774942398\n",
            "Epoch:174 Loss: 0.18239106237888336\n",
            "Epoch:175 Loss: 0.20315919816493988\n",
            "Epoch:176 Loss: 0.21657514572143555\n",
            "Epoch:177 Loss: 0.22157105803489685\n",
            "Epoch:178 Loss: 0.2177491933107376\n",
            "Epoch:179 Loss: 0.2054138332605362\n",
            "Epoch:180 Loss: 0.1855468600988388\n",
            "Epoch:181 Loss: 0.15972967445850372\n",
            "Epoch:182 Loss: 0.13001732528209686\n",
            "Epoch:183 Loss: 0.09877490252256393\n",
            "Epoch:184 Loss: 0.06848932802677155\n",
            "Epoch:185 Loss: 0.04157131910324097\n",
            "Epoch:186 Loss: 0.02016356587409973\n",
            "Epoch:187 Loss: 0.005970112979412079\n",
            "Epoch:188 Loss: 0.00012076291022822261\n",
            "Epoch:189 Loss: 0.003081123111769557\n",
            "Epoch:190 Loss: 0.014615548774600029\n",
            "Epoch:191 Loss: 0.03380589932203293\n",
            "Epoch:192 Loss: 0.059124622493982315\n",
            "Epoch:193 Loss: 0.08855634182691574\n",
            "Epoch:194 Loss: 0.11975831538438797\n",
            "Epoch:195 Loss: 0.15024684369564056\n",
            "Epoch:196 Loss: 0.17759504914283752\n",
            "Epoch:197 Loss: 0.19962596893310547\n",
            "Epoch:198 Loss: 0.2145860493183136\n",
            "Epoch:199 Loss: 0.22128437459468842\n",
            "tensor([[0.7576, 0.2793, 0.4031]])\n",
            "tensor([[0.7805, 0.5286, 0.0074]], requires_grad=True)\n",
            "test loss:0.21918779611587524\n"
          ]
        }
      ],
      "source": [
        "# Suppose we forget optimizer.zero_grad()\n",
        "# Given an example of what this does and why we WOULD want to do this\n",
        "# Hint: if you are doing batch gradient descent and call optimizer.zero_grad() every 3 batches, what is the gradient represent?\n",
        "\n",
        "# Answer: If we forget optimizer.zero_grad(), we always accumulate gradient every train epoch.\n",
        "# We want to do this when we simulate larger batches with limited GPU memory. We cann run multiple times on small batches and accumulate them, as if we work on large batches.\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "# Define y to be a target of dimension (1, 3) without a gradient\n",
        "y = torch.rand(1, 3)\n",
        "\n",
        "# Define theta to be a random tensor of dimension (1, 3) which requires a gradient; we want theta to converge to y\n",
        "theta = torch.rand(1, 3)\n",
        "theta.requires_grad_(True)\n",
        "\n",
        "# Define an SGD optimizer with learning rate 0.01 which acts on theta\n",
        "optimizer = torch.optim.SGD([theta], lr=0.01)\n",
        "\n",
        "# Fil in the code below using the optimizer above to get theta to converge to y\n",
        "for epoch in range(200):\n",
        "  # Define a loss manually which is ||theta-x||_{2}^{2}, the L2 loss across all components\n",
        "  loss = torch.norm(y-theta) ** 2\n",
        "\n",
        "  print('Epoch:{} Loss: {}'.format(epoch, loss))\n",
        "\n",
        "  # Get teh gradients of l with respect to theta\n",
        "  loss.backward()\n",
        "\n",
        "  # Update theta\n",
        "  optimizer.step()\n",
        "\n",
        "# These should look very similar\n",
        "print(y)\n",
        "print(theta)\n",
        "with torch.no_grad():\n",
        "  # Check the y and theta have converged to almost the same thing\n",
        "  loss = torch.norm(y-theta) ** 2\n",
        "  print(f\"test loss:{loss}\")"
      ],
      "id": "MgA6DK8R3BUU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5tQSyJ9cIOU"
      },
      "source": [
        "# Neural Text Classifier - Information\n",
        "\n",
        "For this problem, you will build a basic Neural Text Classifier. The problem will take you through some of the steps needed to be done, including the preprocessing.\n",
        "\n",
        "There is alot of helper code here, but your task is to add in code that has `_FILL_` specified. All assertions should pass.\n",
        "\n",
        "The at a high level, the idea of this model goes as follows.\n",
        "- We are given a training set $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}$ where each $x^{(i)}$ is a sentence and $y^{(i)}$ is a class label.\n",
        "- First, we need to loop over $\\{x^{(i)}\\}_{i=1}^{N}$ and get the Vocabulary, the number of unique words we see.\n",
        "- Once we do this, we will express each word as a one-hot representation. To do this, we will use a mapping from a unique word to an integer. For example, \"the\" might get index 3 and if there are 10 words (in the entire Vocabulary) then \"the\" would have a vector representation $x_{the} = (0,0,1,0,0,0,0,0,0,0)$. There will be many words in this Vocabulary, over 13,000. For this example, each word is mapped to a unique integer.\n",
        "- We will feed batches of data to the model and each batch will be transformed into a tensor with words each word transformed to its integer index in VOCAB below.\n",
        "- For example, we might get [[\"the man walks\"], [\"this is a sentence\"]] -> [[\"the\", \"man\", \"walks\"], [\"this\", \"is\", \"a\", \"sentence\"]] -> [[1, 4, 5], [6, 7, 8, 15]]. It depends on what unique integer each word gets.\n",
        "- Different sentences have different numbers of tokens but all batches need to be the same dimension (this is how PyTorch works), so we need a padding token. So, for example, if the batch size is B = 2 and we given two sentences like [\"a b c\", \"a b c d e\"] then as a tensor this will become [[1, 2, 3, 0, 0], [1, 2, 3, 4, 5]] and notice that we padded the first example so that the tensor is of dimension (2, 5) with M = 5. In some sense, in each batch we need to figure out the maximum number of tokens for an instance and pad each instance to have the same length as this longest instance. To do the above, use the [collate function](https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders). The idea here is that the Dataloader takes in raw data and the collate function is applied to this data, returning formatting tensors we can use later on in the optimization. You'll fill this in, using the hints.  \n",
        "- After padding, we feed batches of data to the classifier, these are of dimension (B, M). For example, we have a batch size of 2 above and M = 5. This will depend on the batch but here the batch size is B.\n",
        "- Once we feed in (B, M) data to the network, we rewrite this as (B, M, vocab_size) by using a one-hot representation for each word.\n",
        "- Then, we do as it hints in the model's forward method. We first take an average agross all the M elements of each element of the batch to get a (B, vocab_size) tensor that represents each instance. We pass this tensor through linear layer and nonlinear layers as unusual. The model returns logits, without the Softmax applied. This is a multiclass classfication task.\n",
        "\n",
        "Finally, we optimize the network and check it's train and validation set accuracies. We'll use both direct methods and torchmetrics to do this. See the Comments for hints on what you need to fill in."
      ],
      "id": "f5tQSyJ9cIOU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcb35e9c"
      },
      "source": [
        "### Information (if interested in more)\n",
        "- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n",
        "- torchtext documentation: https://pytorch.org/text/stable/index.html\n",
        "- collate function: https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders\n",
        "- embedding layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
      ],
      "id": "dcb35e9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9140e3c"
      },
      "source": [
        "### Constants"
      ],
      "id": "b9140e3c"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "judjCThvgmmq"
      },
      "id": "judjCThvgmmq",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c9ace94f"
      },
      "outputs": [],
      "source": [
        "# This is the dataset we will use\n",
        "DATASET = \"AG_NEWS\"\n",
        "DATA_DIR = \".data\"\n",
        "# We will just use CPU here, but if you have time try \"cuda\"\n",
        "DEVICE = \"cpu\"\n",
        "LR = 8.0\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 5\n",
        "MIN_FREQUENCY = 20\n",
        "# Padding valued used; if we have a tensor data x = [[1,2,3], [4, 5], [1,2,3,4,5]] this needs padding\n",
        "# As a tensor, this is t = [[1, 2, 3, 0, 0], [4, 5, 0, 0, 0], [1, 2, 3, 4, 5]]\n",
        "PADDING_VALUE = 0\n",
        "PADDING_IDX = PADDING_VALUE\n",
        "\n",
        "SEED = 1"
      ],
      "id": "c9ace94f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3zuWGeqcDsI"
      },
      "source": [
        "# Get the tokenizer"
      ],
      "id": "K3zuWGeqcDsI"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "16f471ac"
      },
      "outputs": [],
      "source": [
        "# A basic tokenizer by using get_tokenizer; pass \"basic_english\"\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "basic_english_tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "id": "16f471ac"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1b61bba",
        "outputId": "d0acd495-ddbd-409c-9999-d8018042fd90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'some', 'text', '.', '.', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "basic_english_tokenizer(\"This is some text ...\")"
      ],
      "id": "d1b61bba"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "68a50055"
      },
      "outputs": [],
      "source": [
        "# Save the tokenizer as a contant; this is needed later\n",
        "TOKENIZER = basic_english_tokenizer"
      ],
      "id": "68a50055"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8620a436"
      },
      "source": [
        "### Get the data and get the vocabulary."
      ],
      "id": "8620a436"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1c84225a"
      },
      "outputs": [],
      "source": [
        "# Loop through all the (label, text) data and yield a tokenized version of text\n",
        "def yield_tokens(data_iter):\n",
        "    sentence_tokens =[]\n",
        "    for _, text in data_iter:\n",
        "        sentence_tokens.append(TOKENIZER(text))\n",
        "    return sentence_tokens"
      ],
      "id": "1c84225a"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rEChT6jDeLXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee027708-e107-402a-ecee-df252598a202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
            "################################################################################\n",
            "WARNING!\n",
            "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
            "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
            "to learn more and leave feedback.\n",
            "################################################################################\n",
            "\n",
            "  deprecation_warning()\n"
          ]
        }
      ],
      "source": [
        "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")"
      ],
      "id": "rEChT6jDeLXF"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4q-FQ75eM1R",
        "outputId": "2ff83745-5aa2-4c95-8be8-221eb89c635d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y:3, x:Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
          ]
        }
      ],
      "source": [
        "# An example of what this data looks like\n",
        "for y, x in train_iter:\n",
        "  print(f\"y:{y}, x:{x}\")\n",
        "  break"
      ],
      "id": "J4q-FQ75eM1R"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affa3375",
        "outputId": "beb357cf-63bc-4ab0-ec49-e58d3b7c5dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ],
      "source": [
        "# Use build_vocab_from_iterator to get the the vocabulary\n",
        "# This is essentially a dictionary going from a word to a unique integer\n",
        "# Make sure to specify the specials\n",
        "VOCAB = build_vocab_from_iterator(\n",
        "    yield_tokens(train_iter),\n",
        "    min_freq = MIN_FREQUENCY,\n",
        "    specials=('<pad>', '<unk>')\n",
        ")\n",
        "\n",
        "# Set the default index to 1\n",
        "# Otherwise, VOCAB['unknownbigword'] will raise an Exception\n",
        "# I.e. we want '<unk>' to be the unknown word\n",
        "VOCAB.set_default_index(VOCAB['<unk>'])"
      ],
      "id": "affa3375"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "62336be9"
      },
      "outputs": [],
      "source": [
        "assert VOCAB['<unk>'] == 1"
      ],
      "id": "62336be9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "518918c0"
      },
      "source": [
        "Examples"
      ],
      "id": "518918c0"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de48bde8",
        "outputId": "e7c41392-38ec-4101-c9b0-0fa93719acf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 437, 0, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "VOCAB['yoyooyoyoy'], VOCAB['house'], VOCAB['<pad>'], VOCAB['<unk>']"
      ],
      "id": "de48bde8"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24247d56",
        "outputId": "8428e1ab-fd86-4795-de84-f4afc46e64d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13798\n"
          ]
        }
      ],
      "source": [
        "print(len(VOCAB))"
      ],
      "id": "24247d56"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "574cce1b",
        "outputId": "da8fee66-c03e-4272-b6c5-d78aee272424"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[437, 437, 4548, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "VOCAB(TOKENIZER(\"House house houses ThisisnotaKNownWord\"))"
      ],
      "id": "574cce1b"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1be1e272"
      },
      "outputs": [],
      "source": [],
      "id": "1be1e272"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df651a44"
      },
      "source": [
        "### Helper functions"
      ],
      "id": "df651a44"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "94741f76"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab.vocab_factory import Vocab\n",
        "# Utility to transform text into a list of ints\n",
        "# This shoould go \"a b c\" -> [\"a\", \"b\", \"c\"] -> [1, 2, 3], for example\n",
        "def text_pipeline(x):\n",
        "    # Apply tokenizer to x\n",
        "    tokens = TOKENIZER(x)\n",
        "\n",
        "    # Return the Vocab at those tokens\n",
        "    return VOCAB(tokens)\n",
        "\n",
        "# Return a 0 starting version of x\n",
        "# If x = \"1\" this should return 0\n",
        "# If x = \"3\" this should return 2, Etc.\n",
        "def label_pipeline(x):\n",
        "    return int(x) - 1"
      ],
      "id": "94741f76"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e903610"
      },
      "source": [
        "Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"
      ],
      "id": "1e903610"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "95311731"
      },
      "outputs": [],
      "source": [
        "# For a batch of data that might not be a tensor, return the batch in ternsor version\n",
        "# batch is a length B lsit of tuples where each element is (label, text)\n",
        "# label is a raw string like \"1\" here; text is a sentence like \"this is about soccer\"\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (label, text) in batch:\n",
        "        # Get the label from {1, 2, 3, 4} to {0, 1, 2, 3} and append it to label list\n",
        "        label_list.append(label_pipeline(label))\n",
        "\n",
        "        # Return a list of ints\n",
        "        processed_text = text_pipeline(text)\n",
        "        text_list.append(torch.tensor(processed_text, dtype=torch.int64))\n",
        "\n",
        "    # Make label_list into a tensor of dtype=torch.int64\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "\n",
        "    # Pad the sequence\n",
        "    # For Exmaple: if we had 2 elements and [[1, 2], [1,2,3,4]] in the text_list then we want\n",
        "    # to have [[1, 2, 0, 0], [1, 2, 3, 4]] in text_list and text_list is a tensor\n",
        "    # Look up pad_sequence and make sure you specify batch_first=True and specify the padding_value=0\n",
        "    text_list = pad_sequence(text_list, batch_first = True)\n",
        "\n",
        "    # Return the data and put it on a GPU or CPU, as needed\n",
        "    return label_list.to(DEVICE), text_list.to(DEVICE)"
      ],
      "id": "95311731"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "b1292c44"
      },
      "outputs": [],
      "source": [],
      "id": "b1292c44"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5da047d"
      },
      "source": [
        "### Get the data"
      ],
      "id": "b5da047d"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d2ae25e",
        "outputId": "97ed8031-77eb-4af6-9e4c-d287676e2640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of classes is 4 ...\n"
          ]
        }
      ],
      "source": [
        "# Get an iterator for the AG_NEWS dataset and get the train version\n",
        "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
        "\n",
        "# Use the above to get the number of class elements\n",
        "num_class = len(set(label for (label, text) in train_iter))\n",
        "# What are the classes?\n",
        "print(f\"The number of classes is {num_class} ...\")"
      ],
      "id": "5d2ae25e"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fc52d408"
      },
      "outputs": [],
      "source": [],
      "id": "fc52d408"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eca51b36"
      },
      "source": [
        "### Set up the model"
      ],
      "id": "eca51b36"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "8ab8cb7c"
      },
      "outputs": [],
      "source": [
        "# A very naive model used to classify text\n",
        "class OneHotTextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, num_class, use_embedding_layer):\n",
        "        super(OneHotTextClassificationModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_class = num_class\n",
        "\n",
        "        self.use_embedding_layer = use_embedding_layer\n",
        "        self.e = nn.Embedding(vocab_size, 100)\n",
        "        # Have this layer take in data of dimension vocab_size and return data of dimension 100\n",
        "        # Don't use a bias\n",
        "        self.fc1 = nn.Linear(in_features=vocab_size, out_features=100, bias = False)\n",
        "        # Have this layer take in 100 and return data of dimension num_class\n",
        "        # Don't use a bias\n",
        "        self.fc2 = nn.Linear(in_features=100, out_features=num_class, bias = False)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize the weights of fc1 to the same exact data as what self.e has\n",
        "        # You need to access the data within these layers\n",
        "        # Initialize the bias to zero\n",
        "        # Hint: look at self.e.weight.data and similarly for fc\n",
        "        # Make sure you have the dimensions line up right\n",
        "        self.fc1.weight.data = self.e.weight.data\n",
        "\n",
        "        # Unitialize fc2 to uniform between -0.5 and 0.5\n",
        "        # Hint: \"uniform_\"\n",
        "        self.fc2.weight.data = self.fc2.weight.data.uniform_(-0.5, 0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, K = x.shape\n",
        "        # x is of dimension (B, K), where K is the maximum number of tokens in an element of the batch\n",
        "        # Note: We will make this faster later on by using the nn.Embedding layer\n",
        "\n",
        "        # We will not use nn.Embedding, but the code below, a combination of F.one_hot and fc1, should be the SAME effect as the else clause\n",
        "        if not self.use_embedding_layer:\n",
        "          # Transform x to a tensor where each element is one-hot encoded\n",
        "          x = F.one_hot(x, num_classes=self.vocab_size)\n",
        "          assert(x.shape == (B, K, self.vocab_size))\n",
        "\n",
        "          # Pass x through fc1 to get the row in fc1 correspondng to the row x is\n",
        "          x = self.fc1(x)\n",
        "          assert(x.shape == (B, K, 100))\n",
        "        else:\n",
        "          # Note: the above two steps should be the same as doing the command below\n",
        "          x = self.e(x)\n",
        "          assert(x.shape == (B, K, 100))\n",
        "\n",
        "        # Take the mean of the embedings for all words in each sentence\n",
        "        x = torch.mean(x, dim=1)\n",
        "        assert(x.shape == (B, 100))\n",
        "\n",
        "        # Apply ReLU to x\n",
        "        x = F.relu(x)\n",
        "        assert(x.shape == (B, 100))\n",
        "\n",
        "        # Pass through fc2\n",
        "        x = self.fc2(x)\n",
        "        assert(x.shape == (B, self.num_class))\n",
        "\n",
        "        # Return the Logits\n",
        "        return x"
      ],
      "id": "8ab8cb7c"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a43d569e",
        "outputId": "dc6c8af6-9102-4976-a97d-f762445b67c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ee930f5b3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "torch.manual_seed(SEED)"
      ],
      "id": "a43d569e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8994ae19"
      },
      "source": [
        "### Set up the data"
      ],
      "id": "8994ae19"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "eaaa82a2"
      },
      "outputs": [],
      "source": [
        "# Map the data to the right format\n",
        "train_iter, test_iter = DATASETS[DATASET]()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# Split data into train and validation\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "# Set up different DataLoaders\n",
        "# Make sure you pass collate_fn as the function you wrote above\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "id": "eaaa82a2"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7a5af374"
      },
      "outputs": [],
      "source": [],
      "id": "7a5af374"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72b5bb91"
      },
      "source": [
        "### Train the model"
      ],
      "id": "72b5bb91"
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "d58cc1a9"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, optimizer, criterion, epoch):\n",
        "    # Put the model in train mode; this does not matter right now\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    total_loss = 0.0\n",
        "    log_interval = 200\n",
        "    total_samples = len(dataloader.dataset)\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        # Zero out the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the predictions\n",
        "        logits = model(text)\n",
        "\n",
        "        # Get the loss.\n",
        "        loss = criterion(input=logits, target=label)\n",
        "\n",
        "        # The loss is computed by taking a mean, get the sum of the terms on the numerator\n",
        "        with torch.no_grad():\n",
        "          total_loss += loss.item() * label.size(0)\n",
        "\n",
        "        # Do back propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to have max norm 0.1\n",
        "        # Look up torch.nn.utils.clip_grad_norm\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "\n",
        "        # Do an optimization step.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the accuracy\n",
        "        # predicted_label is (B, num_class) so take the argmax over the right dimension to get the actual label\n",
        "        # Make sure you do .item() on whaht you get so that you update the accuracy\n",
        "        predicted_label = torch.argmax(logits, dim=1)\n",
        "        total_acc += (label == predicted_label).sum()\n",
        "\n",
        "        # Update the total number of items\n",
        "        total_count += predicted_label.shape[0]\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| accuracy {:8.3f} \"\n",
        "                \"| loss {:8.3f}\".format(\n",
        "                    epoch, idx,\n",
        "                    total_samples,\n",
        "                    total_acc/total_count,\n",
        "                    loss\n",
        "                    )\n",
        "            )\n",
        "            total_acc, total_count, total_loss = 0, 0, 0.0"
      ],
      "id": "d58cc1a9"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "85722617"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.classification import Accuracy\n",
        "\n",
        "\n",
        "def evaluate(dataloader, model):\n",
        "    # Put the model in eval model; this does not matter right now\n",
        "    model.eval()\n",
        "    accuracy_fn = Accuracy(task=\"multiclass\", num_classes=num_class).to(DEVICE)\n",
        "\n",
        "    # Get the accuracy\n",
        "    total_acc = 0.0\n",
        "    total_count = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            # Get the predictions\n",
        "            logits = model(text)\n",
        "            predicted_label = torch.argmax(logits, dim=1)\n",
        "            # Get the number of samples we have, the denominator of accuracy\n",
        "            total_count += label.shape[0]\n",
        "\n",
        "            # Get the total number of times we have the correct predictions, use accuracy_fn\n",
        "            accuracy_from_accu_fn = accuracy_fn(logits, label)\n",
        "            accuracy_from_mine = (label == predicted_label).sum()\n",
        "            total_acc += accuracy_from_mine\n",
        "\n",
        "            # Use accuracy_fn from torchmetrics to check that the total number of correct predictions is the same as if you use argmax on predicted_label\n",
        "            # I.e. I want you to use torchmetrics to compute this AND use the same metod as in train above\n",
        "            # Remember to use .item() on the tensor you get and also rememeber number_or_samples * accuracy = total_times_we_have_equality (the numerator of accuracy)\n",
        "            assert (\n",
        "                accuracy_from_accu_fn * label.shape[0] == accuracy_from_mine\n",
        "            )\n",
        "\n",
        "    accuracy = total_acc / total_count\n",
        "    return accuracy"
      ],
      "id": "85722617"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3LjZHTdrWW6"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "You should get an accuracy > 80% for the training set. This might take quite a bit of time to run since we use one-hot. Use nn.Embedding if you want to check this quickly. You should get the SAME answer using either method."
      ],
      "id": "W3LjZHTdrWW6"
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "eTEl16pIBkTe"
      },
      "outputs": [],
      "source": [
        "# Set up the loss function\n",
        "# Note that this should be a multiclass classification problem and you take in logits\n",
        "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "# Instantiate the model\n",
        "# Pass in the number of elements in VOCAB and num_class\n",
        "model = OneHotTextClassificationModel(vocab_size=len(VOCAB), num_class=num_class,use_embedding_layer=True).to(DEVICE)\n",
        "\n",
        "# Instantiate the SGD optimizer with parameters LR\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)"
      ],
      "id": "eTEl16pIBkTe"
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ba24f3",
        "outputId": "570eef84-9e05-4521-84c3-0c165d3ed5a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/114000 batches | accuracy    0.264 | loss    1.437\n",
            "| epoch   1 |   400/114000 batches | accuracy    0.297 | loss    1.301\n",
            "| epoch   1 |   600/114000 batches | accuracy    0.351 | loss    1.389\n",
            "| epoch   1 |   800/114000 batches | accuracy    0.373 | loss    1.193\n",
            "| epoch   1 |  1000/114000 batches | accuracy    0.380 | loss    1.303\n",
            "| epoch   1 |  1200/114000 batches | accuracy    0.417 | loss    1.179\n",
            "| epoch   1 |  1400/114000 batches | accuracy    0.423 | loss    1.273\n",
            "| epoch   1 |  1600/114000 batches | accuracy    0.437 | loss    1.349\n",
            "| epoch   1 |  1800/114000 batches | accuracy    0.447 | loss    1.110\n",
            "| epoch   1 |  2000/114000 batches | accuracy    0.469 | loss    1.278\n",
            "| epoch   1 |  2200/114000 batches | accuracy    0.482 | loss    1.174\n",
            "| epoch   1 |  2400/114000 batches | accuracy    0.477 | loss    1.192\n",
            "| epoch   1 |  2600/114000 batches | accuracy    0.488 | loss    1.143\n",
            "| epoch   1 |  2800/114000 batches | accuracy    0.502 | loss    1.102\n",
            "| epoch   1 |  3000/114000 batches | accuracy    0.500 | loss    1.060\n",
            "| epoch   1 |  3200/114000 batches | accuracy    0.508 | loss    1.359\n",
            "| epoch   1 |  3400/114000 batches | accuracy    0.521 | loss    1.026\n",
            "| epoch   1 |  3600/114000 batches | accuracy    0.531 | loss    1.070\n",
            "| epoch   1 |  3800/114000 batches | accuracy    0.548 | loss    1.019\n",
            "| epoch   1 |  4000/114000 batches | accuracy    0.543 | loss    1.101\n",
            "| epoch   1 |  4200/114000 batches | accuracy    0.560 | loss    1.061\n",
            "| epoch   1 |  4400/114000 batches | accuracy    0.562 | loss    1.182\n",
            "| epoch   1 |  4600/114000 batches | accuracy    0.565 | loss    1.325\n",
            "| epoch   1 |  4800/114000 batches | accuracy    0.585 | loss    1.233\n",
            "| epoch   1 |  5000/114000 batches | accuracy    0.568 | loss    0.881\n",
            "| epoch   1 |  5200/114000 batches | accuracy    0.589 | loss    0.971\n",
            "| epoch   1 |  5400/114000 batches | accuracy    0.609 | loss    0.817\n",
            "| epoch   1 |  5600/114000 batches | accuracy    0.589 | loss    0.984\n",
            "| epoch   1 |  5800/114000 batches | accuracy    0.598 | loss    0.797\n",
            "| epoch   1 |  6000/114000 batches | accuracy    0.601 | loss    1.043\n",
            "| epoch   1 |  6200/114000 batches | accuracy    0.602 | loss    0.846\n",
            "| epoch   1 |  6400/114000 batches | accuracy    0.613 | loss    1.142\n",
            "| epoch   1 |  6600/114000 batches | accuracy    0.622 | loss    0.933\n",
            "| epoch   1 |  6800/114000 batches | accuracy    0.620 | loss    1.012\n",
            "| epoch   1 |  7000/114000 batches | accuracy    0.629 | loss    0.878\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 20.59s | valid accuracy    0.637 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   200/114000 batches | accuracy    0.645 | loss    0.657\n",
            "| epoch   2 |   400/114000 batches | accuracy    0.631 | loss    1.168\n",
            "| epoch   2 |   600/114000 batches | accuracy    0.648 | loss    0.818\n",
            "| epoch   2 |   800/114000 batches | accuracy    0.641 | loss    1.243\n",
            "| epoch   2 |  1000/114000 batches | accuracy    0.665 | loss    0.731\n",
            "| epoch   2 |  1200/114000 batches | accuracy    0.660 | loss    0.688\n",
            "| epoch   2 |  1400/114000 batches | accuracy    0.659 | loss    1.013\n",
            "| epoch   2 |  1600/114000 batches | accuracy    0.661 | loss    1.257\n",
            "| epoch   2 |  1800/114000 batches | accuracy    0.658 | loss    0.981\n",
            "| epoch   2 |  2000/114000 batches | accuracy    0.668 | loss    0.753\n",
            "| epoch   2 |  2200/114000 batches | accuracy    0.669 | loss    0.505\n",
            "| epoch   2 |  2400/114000 batches | accuracy    0.671 | loss    1.025\n",
            "| epoch   2 |  2600/114000 batches | accuracy    0.658 | loss    1.033\n",
            "| epoch   2 |  2800/114000 batches | accuracy    0.681 | loss    0.761\n",
            "| epoch   2 |  3000/114000 batches | accuracy    0.680 | loss    0.533\n",
            "| epoch   2 |  3200/114000 batches | accuracy    0.677 | loss    0.986\n",
            "| epoch   2 |  3400/114000 batches | accuracy    0.687 | loss    1.097\n",
            "| epoch   2 |  3600/114000 batches | accuracy    0.687 | loss    0.834\n",
            "| epoch   2 |  3800/114000 batches | accuracy    0.683 | loss    0.993\n",
            "| epoch   2 |  4000/114000 batches | accuracy    0.677 | loss    1.005\n",
            "| epoch   2 |  4200/114000 batches | accuracy    0.681 | loss    1.025\n",
            "| epoch   2 |  4400/114000 batches | accuracy    0.695 | loss    0.738\n",
            "| epoch   2 |  4600/114000 batches | accuracy    0.689 | loss    0.962\n",
            "| epoch   2 |  4800/114000 batches | accuracy    0.709 | loss    0.861\n",
            "| epoch   2 |  5000/114000 batches | accuracy    0.700 | loss    0.754\n",
            "| epoch   2 |  5200/114000 batches | accuracy    0.702 | loss    0.640\n",
            "| epoch   2 |  5400/114000 batches | accuracy    0.701 | loss    0.781\n",
            "| epoch   2 |  5600/114000 batches | accuracy    0.692 | loss    0.867\n",
            "| epoch   2 |  5800/114000 batches | accuracy    0.710 | loss    0.664\n",
            "| epoch   2 |  6000/114000 batches | accuracy    0.714 | loss    0.739\n",
            "| epoch   2 |  6200/114000 batches | accuracy    0.709 | loss    0.702\n",
            "| epoch   2 |  6400/114000 batches | accuracy    0.711 | loss    0.808\n",
            "| epoch   2 |  6600/114000 batches | accuracy    0.702 | loss    0.542\n",
            "| epoch   2 |  6800/114000 batches | accuracy    0.723 | loss    0.518\n",
            "| epoch   2 |  7000/114000 batches | accuracy    0.726 | loss    0.928\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 20.12s | valid accuracy    0.734 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   200/114000 batches | accuracy    0.729 | loss    0.823\n",
            "| epoch   3 |   400/114000 batches | accuracy    0.724 | loss    0.708\n",
            "| epoch   3 |   600/114000 batches | accuracy    0.719 | loss    1.021\n",
            "| epoch   3 |   800/114000 batches | accuracy    0.738 | loss    0.335\n",
            "| epoch   3 |  1000/114000 batches | accuracy    0.722 | loss    0.445\n",
            "| epoch   3 |  1200/114000 batches | accuracy    0.756 | loss    0.771\n",
            "| epoch   3 |  1400/114000 batches | accuracy    0.729 | loss    0.427\n",
            "| epoch   3 |  1600/114000 batches | accuracy    0.725 | loss    0.944\n",
            "| epoch   3 |  1800/114000 batches | accuracy    0.743 | loss    0.525\n",
            "| epoch   3 |  2000/114000 batches | accuracy    0.745 | loss    0.792\n",
            "| epoch   3 |  2200/114000 batches | accuracy    0.728 | loss    0.980\n",
            "| epoch   3 |  2400/114000 batches | accuracy    0.742 | loss    0.414\n",
            "| epoch   3 |  2600/114000 batches | accuracy    0.738 | loss    0.646\n",
            "| epoch   3 |  2800/114000 batches | accuracy    0.743 | loss    0.482\n",
            "| epoch   3 |  3000/114000 batches | accuracy    0.744 | loss    0.650\n",
            "| epoch   3 |  3200/114000 batches | accuracy    0.744 | loss    0.545\n",
            "| epoch   3 |  3400/114000 batches | accuracy    0.756 | loss    0.926\n",
            "| epoch   3 |  3600/114000 batches | accuracy    0.753 | loss    0.566\n",
            "| epoch   3 |  3800/114000 batches | accuracy    0.754 | loss    0.522\n",
            "| epoch   3 |  4000/114000 batches | accuracy    0.764 | loss    0.571\n",
            "| epoch   3 |  4200/114000 batches | accuracy    0.750 | loss    0.350\n",
            "| epoch   3 |  4400/114000 batches | accuracy    0.749 | loss    0.820\n",
            "| epoch   3 |  4600/114000 batches | accuracy    0.746 | loss    0.885\n",
            "| epoch   3 |  4800/114000 batches | accuracy    0.748 | loss    0.572\n",
            "| epoch   3 |  5000/114000 batches | accuracy    0.751 | loss    0.898\n",
            "| epoch   3 |  5200/114000 batches | accuracy    0.762 | loss    0.639\n",
            "| epoch   3 |  5400/114000 batches | accuracy    0.757 | loss    0.188\n",
            "| epoch   3 |  5600/114000 batches | accuracy    0.772 | loss    0.523\n",
            "| epoch   3 |  5800/114000 batches | accuracy    0.754 | loss    0.454\n",
            "| epoch   3 |  6000/114000 batches | accuracy    0.761 | loss    0.721\n",
            "| epoch   3 |  6200/114000 batches | accuracy    0.765 | loss    0.492\n",
            "| epoch   3 |  6400/114000 batches | accuracy    0.770 | loss    0.733\n",
            "| epoch   3 |  6600/114000 batches | accuracy    0.768 | loss    0.234\n",
            "| epoch   3 |  6800/114000 batches | accuracy    0.768 | loss    0.952\n",
            "| epoch   3 |  7000/114000 batches | accuracy    0.776 | loss    0.492\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 19.60s | valid accuracy    0.784 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   200/114000 batches | accuracy    0.775 | loss    0.664\n",
            "| epoch   4 |   400/114000 batches | accuracy    0.768 | loss    0.831\n",
            "| epoch   4 |   600/114000 batches | accuracy    0.758 | loss    0.614\n",
            "| epoch   4 |   800/114000 batches | accuracy    0.761 | loss    0.692\n",
            "| epoch   4 |  1000/114000 batches | accuracy    0.772 | loss    0.663\n",
            "| epoch   4 |  1200/114000 batches | accuracy    0.768 | loss    0.598\n",
            "| epoch   4 |  1400/114000 batches | accuracy    0.779 | loss    0.846\n",
            "| epoch   4 |  1600/114000 batches | accuracy    0.770 | loss    1.201\n",
            "| epoch   4 |  1800/114000 batches | accuracy    0.777 | loss    0.386\n",
            "| epoch   4 |  2000/114000 batches | accuracy    0.774 | loss    0.653\n",
            "| epoch   4 |  2200/114000 batches | accuracy    0.779 | loss    0.387\n",
            "| epoch   4 |  2400/114000 batches | accuracy    0.774 | loss    0.829\n",
            "| epoch   4 |  2600/114000 batches | accuracy    0.775 | loss    0.324\n",
            "| epoch   4 |  2800/114000 batches | accuracy    0.773 | loss    0.271\n",
            "| epoch   4 |  3000/114000 batches | accuracy    0.783 | loss    0.761\n",
            "| epoch   4 |  3200/114000 batches | accuracy    0.779 | loss    0.314\n",
            "| epoch   4 |  3400/114000 batches | accuracy    0.772 | loss    0.991\n",
            "| epoch   4 |  3600/114000 batches | accuracy    0.786 | loss    0.506\n",
            "| epoch   4 |  3800/114000 batches | accuracy    0.796 | loss    0.975\n",
            "| epoch   4 |  4000/114000 batches | accuracy    0.786 | loss    0.707\n",
            "| epoch   4 |  4200/114000 batches | accuracy    0.784 | loss    0.871\n",
            "| epoch   4 |  4400/114000 batches | accuracy    0.780 | loss    0.607\n",
            "| epoch   4 |  4600/114000 batches | accuracy    0.788 | loss    0.433\n",
            "| epoch   4 |  4800/114000 batches | accuracy    0.797 | loss    0.664\n",
            "| epoch   4 |  5000/114000 batches | accuracy    0.794 | loss    0.692\n",
            "| epoch   4 |  5200/114000 batches | accuracy    0.785 | loss    0.579\n",
            "| epoch   4 |  5400/114000 batches | accuracy    0.785 | loss    0.581\n",
            "| epoch   4 |  5600/114000 batches | accuracy    0.782 | loss    0.902\n",
            "| epoch   4 |  5800/114000 batches | accuracy    0.791 | loss    0.304\n",
            "| epoch   4 |  6000/114000 batches | accuracy    0.779 | loss    0.828\n",
            "| epoch   4 |  6200/114000 batches | accuracy    0.798 | loss    0.545\n",
            "| epoch   4 |  6400/114000 batches | accuracy    0.811 | loss    0.861\n",
            "| epoch   4 |  6600/114000 batches | accuracy    0.792 | loss    0.616\n",
            "| epoch   4 |  6800/114000 batches | accuracy    0.790 | loss    0.676\n",
            "| epoch   4 |  7000/114000 batches | accuracy    0.799 | loss    0.464\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 20.45s | valid accuracy    0.803 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   200/114000 batches | accuracy    0.793 | loss    0.726\n",
            "| epoch   5 |   400/114000 batches | accuracy    0.794 | loss    0.336\n",
            "| epoch   5 |   600/114000 batches | accuracy    0.785 | loss    0.506\n",
            "| epoch   5 |   800/114000 batches | accuracy    0.803 | loss    0.312\n",
            "| epoch   5 |  1000/114000 batches | accuracy    0.803 | loss    0.758\n",
            "| epoch   5 |  1200/114000 batches | accuracy    0.799 | loss    0.247\n",
            "| epoch   5 |  1400/114000 batches | accuracy    0.793 | loss    0.328\n",
            "| epoch   5 |  1600/114000 batches | accuracy    0.795 | loss    0.409\n",
            "| epoch   5 |  1800/114000 batches | accuracy    0.803 | loss    0.637\n",
            "| epoch   5 |  2000/114000 batches | accuracy    0.816 | loss    0.676\n",
            "| epoch   5 |  2200/114000 batches | accuracy    0.800 | loss    0.520\n",
            "| epoch   5 |  2400/114000 batches | accuracy    0.806 | loss    0.219\n",
            "| epoch   5 |  2600/114000 batches | accuracy    0.795 | loss    0.798\n",
            "| epoch   5 |  2800/114000 batches | accuracy    0.804 | loss    0.639\n",
            "| epoch   5 |  3000/114000 batches | accuracy    0.807 | loss    0.894\n",
            "| epoch   5 |  3200/114000 batches | accuracy    0.794 | loss    0.471\n",
            "| epoch   5 |  3400/114000 batches | accuracy    0.794 | loss    0.649\n",
            "| epoch   5 |  3600/114000 batches | accuracy    0.808 | loss    0.604\n",
            "| epoch   5 |  3800/114000 batches | accuracy    0.809 | loss    0.839\n",
            "| epoch   5 |  4000/114000 batches | accuracy    0.793 | loss    0.583\n",
            "| epoch   5 |  4200/114000 batches | accuracy    0.799 | loss    0.614\n",
            "| epoch   5 |  4400/114000 batches | accuracy    0.806 | loss    0.244\n",
            "| epoch   5 |  4600/114000 batches | accuracy    0.804 | loss    0.477\n",
            "| epoch   5 |  4800/114000 batches | accuracy    0.803 | loss    0.650\n",
            "| epoch   5 |  5000/114000 batches | accuracy    0.809 | loss    0.368\n",
            "| epoch   5 |  5200/114000 batches | accuracy    0.803 | loss    0.466\n",
            "| epoch   5 |  5400/114000 batches | accuracy    0.812 | loss    0.489\n",
            "| epoch   5 |  5600/114000 batches | accuracy    0.799 | loss    0.436\n",
            "| epoch   5 |  5800/114000 batches | accuracy    0.811 | loss    1.049\n",
            "| epoch   5 |  6000/114000 batches | accuracy    0.814 | loss    0.197\n",
            "| epoch   5 |  6200/114000 batches | accuracy    0.811 | loss    0.302\n",
            "| epoch   5 |  6400/114000 batches | accuracy    0.820 | loss    0.130\n",
            "| epoch   5 |  6600/114000 batches | accuracy    0.800 | loss    0.367\n",
            "| epoch   5 |  6800/114000 batches | accuracy    0.797 | loss    0.529\n",
            "| epoch   5 |  7000/114000 batches | accuracy    0.818 | loss    0.229\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 20.15s | valid accuracy    0.799 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   200/114000 batches | accuracy    0.808 | loss    0.330\n",
            "| epoch   6 |   400/114000 batches | accuracy    0.827 | loss    0.396\n",
            "| epoch   6 |   600/114000 batches | accuracy    0.787 | loss    0.706\n",
            "| epoch   6 |   800/114000 batches | accuracy    0.815 | loss    0.407\n",
            "| epoch   6 |  1000/114000 batches | accuracy    0.817 | loss    0.251\n",
            "| epoch   6 |  1200/114000 batches | accuracy    0.805 | loss    0.492\n",
            "| epoch   6 |  1400/114000 batches | accuracy    0.799 | loss    0.448\n",
            "| epoch   6 |  1600/114000 batches | accuracy    0.824 | loss    0.724\n",
            "| epoch   6 |  1800/114000 batches | accuracy    0.820 | loss    0.595\n",
            "| epoch   6 |  2000/114000 batches | accuracy    0.812 | loss    0.555\n",
            "| epoch   6 |  2200/114000 batches | accuracy    0.820 | loss    0.360\n",
            "| epoch   6 |  2400/114000 batches | accuracy    0.812 | loss    1.123\n",
            "| epoch   6 |  2600/114000 batches | accuracy    0.819 | loss    0.663\n",
            "| epoch   6 |  2800/114000 batches | accuracy    0.808 | loss    0.529\n",
            "| epoch   6 |  3000/114000 batches | accuracy    0.812 | loss    0.195\n",
            "| epoch   6 |  3200/114000 batches | accuracy    0.814 | loss    0.325\n",
            "| epoch   6 |  3400/114000 batches | accuracy    0.822 | loss    0.565\n",
            "| epoch   6 |  3600/114000 batches | accuracy    0.830 | loss    0.491\n",
            "| epoch   6 |  3800/114000 batches | accuracy    0.823 | loss    0.580\n",
            "| epoch   6 |  4000/114000 batches | accuracy    0.822 | loss    0.498\n",
            "| epoch   6 |  4200/114000 batches | accuracy    0.810 | loss    0.811\n",
            "| epoch   6 |  4400/114000 batches | accuracy    0.829 | loss    0.460\n",
            "| epoch   6 |  4600/114000 batches | accuracy    0.814 | loss    0.492\n",
            "| epoch   6 |  4800/114000 batches | accuracy    0.814 | loss    0.737\n",
            "| epoch   6 |  5000/114000 batches | accuracy    0.821 | loss    0.709\n",
            "| epoch   6 |  5200/114000 batches | accuracy    0.826 | loss    0.576\n",
            "| epoch   6 |  5400/114000 batches | accuracy    0.823 | loss    0.367\n",
            "| epoch   6 |  5600/114000 batches | accuracy    0.838 | loss    0.680\n",
            "| epoch   6 |  5800/114000 batches | accuracy    0.819 | loss    0.360\n",
            "| epoch   6 |  6000/114000 batches | accuracy    0.822 | loss    0.261\n",
            "| epoch   6 |  6200/114000 batches | accuracy    0.824 | loss    0.339\n",
            "| epoch   6 |  6400/114000 batches | accuracy    0.822 | loss    0.385\n",
            "| epoch   6 |  6600/114000 batches | accuracy    0.825 | loss    0.384\n",
            "| epoch   6 |  6800/114000 batches | accuracy    0.823 | loss    0.762\n",
            "| epoch   6 |  7000/114000 batches | accuracy    0.817 | loss    0.340\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 19.42s | valid accuracy    0.834 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   200/114000 batches | accuracy    0.831 | loss    0.063\n",
            "| epoch   7 |   400/114000 batches | accuracy    0.809 | loss    0.944\n",
            "| epoch   7 |   600/114000 batches | accuracy    0.822 | loss    0.669\n",
            "| epoch   7 |   800/114000 batches | accuracy    0.821 | loss    0.527\n",
            "| epoch   7 |  1000/114000 batches | accuracy    0.838 | loss    0.257\n",
            "| epoch   7 |  1200/114000 batches | accuracy    0.816 | loss    0.404\n",
            "| epoch   7 |  1400/114000 batches | accuracy    0.837 | loss    0.679\n",
            "| epoch   7 |  1600/114000 batches | accuracy    0.841 | loss    0.536\n",
            "| epoch   7 |  1800/114000 batches | accuracy    0.827 | loss    0.566\n",
            "| epoch   7 |  2000/114000 batches | accuracy    0.823 | loss    0.379\n",
            "| epoch   7 |  2200/114000 batches | accuracy    0.837 | loss    0.199\n",
            "| epoch   7 |  2400/114000 batches | accuracy    0.831 | loss    0.717\n",
            "| epoch   7 |  2600/114000 batches | accuracy    0.830 | loss    0.272\n",
            "| epoch   7 |  2800/114000 batches | accuracy    0.820 | loss    0.368\n",
            "| epoch   7 |  3000/114000 batches | accuracy    0.821 | loss    0.358\n",
            "| epoch   7 |  3200/114000 batches | accuracy    0.833 | loss    0.684\n",
            "| epoch   7 |  3400/114000 batches | accuracy    0.825 | loss    0.314\n",
            "| epoch   7 |  3600/114000 batches | accuracy    0.824 | loss    0.825\n",
            "| epoch   7 |  3800/114000 batches | accuracy    0.831 | loss    0.245\n",
            "| epoch   7 |  4000/114000 batches | accuracy    0.826 | loss    0.556\n",
            "| epoch   7 |  4200/114000 batches | accuracy    0.831 | loss    0.428\n",
            "| epoch   7 |  4400/114000 batches | accuracy    0.823 | loss    0.428\n",
            "| epoch   7 |  4600/114000 batches | accuracy    0.836 | loss    0.557\n",
            "| epoch   7 |  4800/114000 batches | accuracy    0.844 | loss    0.650\n",
            "| epoch   7 |  5000/114000 batches | accuracy    0.820 | loss    0.525\n",
            "| epoch   7 |  5200/114000 batches | accuracy    0.835 | loss    0.086\n",
            "| epoch   7 |  5400/114000 batches | accuracy    0.832 | loss    0.160\n",
            "| epoch   7 |  5600/114000 batches | accuracy    0.832 | loss    0.387\n",
            "| epoch   7 |  5800/114000 batches | accuracy    0.826 | loss    0.207\n",
            "| epoch   7 |  6000/114000 batches | accuracy    0.829 | loss    0.438\n",
            "| epoch   7 |  6200/114000 batches | accuracy    0.832 | loss    0.235\n",
            "| epoch   7 |  6400/114000 batches | accuracy    0.842 | loss    0.354\n",
            "| epoch   7 |  6600/114000 batches | accuracy    0.837 | loss    0.373\n",
            "| epoch   7 |  6800/114000 batches | accuracy    0.839 | loss    0.967\n",
            "| epoch   7 |  7000/114000 batches | accuracy    0.832 | loss    0.478\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 19.70s | valid accuracy    0.844 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   200/114000 batches | accuracy    0.835 | loss    0.580\n",
            "| epoch   8 |   400/114000 batches | accuracy    0.828 | loss    0.462\n",
            "| epoch   8 |   600/114000 batches | accuracy    0.848 | loss    0.487\n",
            "| epoch   8 |   800/114000 batches | accuracy    0.835 | loss    0.359\n",
            "| epoch   8 |  1000/114000 batches | accuracy    0.840 | loss    0.432\n",
            "| epoch   8 |  1200/114000 batches | accuracy    0.841 | loss    1.119\n",
            "| epoch   8 |  1400/114000 batches | accuracy    0.844 | loss    0.241\n",
            "| epoch   8 |  1600/114000 batches | accuracy    0.841 | loss    0.042\n",
            "| epoch   8 |  1800/114000 batches | accuracy    0.834 | loss    0.488\n",
            "| epoch   8 |  2000/114000 batches | accuracy    0.842 | loss    0.570\n",
            "| epoch   8 |  2200/114000 batches | accuracy    0.836 | loss    0.489\n",
            "| epoch   8 |  2400/114000 batches | accuracy    0.842 | loss    0.297\n",
            "| epoch   8 |  2600/114000 batches | accuracy    0.841 | loss    0.708\n",
            "| epoch   8 |  2800/114000 batches | accuracy    0.834 | loss    0.533\n",
            "| epoch   8 |  3000/114000 batches | accuracy    0.847 | loss    0.493\n",
            "| epoch   8 |  3200/114000 batches | accuracy    0.837 | loss    0.407\n",
            "| epoch   8 |  3400/114000 batches | accuracy    0.835 | loss    0.671\n",
            "| epoch   8 |  3600/114000 batches | accuracy    0.851 | loss    0.160\n",
            "| epoch   8 |  3800/114000 batches | accuracy    0.828 | loss    0.784\n",
            "| epoch   8 |  4000/114000 batches | accuracy    0.829 | loss    0.631\n",
            "| epoch   8 |  4200/114000 batches | accuracy    0.830 | loss    0.526\n",
            "| epoch   8 |  4400/114000 batches | accuracy    0.832 | loss    0.196\n",
            "| epoch   8 |  4600/114000 batches | accuracy    0.836 | loss    0.573\n",
            "| epoch   8 |  4800/114000 batches | accuracy    0.838 | loss    0.471\n",
            "| epoch   8 |  5000/114000 batches | accuracy    0.830 | loss    0.256\n",
            "| epoch   8 |  5200/114000 batches | accuracy    0.848 | loss    0.305\n",
            "| epoch   8 |  5400/114000 batches | accuracy    0.832 | loss    0.342\n",
            "| epoch   8 |  5600/114000 batches | accuracy    0.840 | loss    0.644\n",
            "| epoch   8 |  5800/114000 batches | accuracy    0.839 | loss    0.367\n",
            "| epoch   8 |  6000/114000 batches | accuracy    0.840 | loss    0.385\n",
            "| epoch   8 |  6200/114000 batches | accuracy    0.847 | loss    0.462\n",
            "| epoch   8 |  6400/114000 batches | accuracy    0.843 | loss    0.154\n",
            "| epoch   8 |  6600/114000 batches | accuracy    0.847 | loss    0.156\n",
            "| epoch   8 |  6800/114000 batches | accuracy    0.843 | loss    0.524\n",
            "| epoch   8 |  7000/114000 batches | accuracy    0.836 | loss    0.660\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 19.33s | valid accuracy    0.842 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   200/114000 batches | accuracy    0.828 | loss    0.535\n",
            "| epoch   9 |   400/114000 batches | accuracy    0.837 | loss    0.596\n",
            "| epoch   9 |   600/114000 batches | accuracy    0.842 | loss    0.460\n",
            "| epoch   9 |   800/114000 batches | accuracy    0.843 | loss    0.567\n",
            "| epoch   9 |  1000/114000 batches | accuracy    0.838 | loss    0.549\n",
            "| epoch   9 |  1200/114000 batches | accuracy    0.844 | loss    0.799\n",
            "| epoch   9 |  1400/114000 batches | accuracy    0.839 | loss    0.198\n",
            "| epoch   9 |  1600/114000 batches | accuracy    0.842 | loss    0.976\n",
            "| epoch   9 |  1800/114000 batches | accuracy    0.842 | loss    0.399\n",
            "| epoch   9 |  2000/114000 batches | accuracy    0.848 | loss    0.305\n",
            "| epoch   9 |  2200/114000 batches | accuracy    0.834 | loss    0.744\n",
            "| epoch   9 |  2400/114000 batches | accuracy    0.856 | loss    0.296\n",
            "| epoch   9 |  2600/114000 batches | accuracy    0.835 | loss    0.300\n",
            "| epoch   9 |  2800/114000 batches | accuracy    0.846 | loss    0.124\n",
            "| epoch   9 |  3000/114000 batches | accuracy    0.855 | loss    0.422\n",
            "| epoch   9 |  3200/114000 batches | accuracy    0.839 | loss    0.404\n",
            "| epoch   9 |  3400/114000 batches | accuracy    0.848 | loss    0.384\n",
            "| epoch   9 |  3600/114000 batches | accuracy    0.837 | loss    0.932\n",
            "| epoch   9 |  3800/114000 batches | accuracy    0.857 | loss    0.394\n",
            "| epoch   9 |  4000/114000 batches | accuracy    0.849 | loss    0.808\n",
            "| epoch   9 |  4200/114000 batches | accuracy    0.845 | loss    0.166\n",
            "| epoch   9 |  4400/114000 batches | accuracy    0.846 | loss    0.206\n",
            "| epoch   9 |  4600/114000 batches | accuracy    0.854 | loss    0.087\n",
            "| epoch   9 |  4800/114000 batches | accuracy    0.853 | loss    0.454\n",
            "| epoch   9 |  5000/114000 batches | accuracy    0.844 | loss    0.163\n",
            "| epoch   9 |  5200/114000 batches | accuracy    0.857 | loss    0.190\n",
            "| epoch   9 |  5400/114000 batches | accuracy    0.843 | loss    0.367\n",
            "| epoch   9 |  5600/114000 batches | accuracy    0.835 | loss    0.245\n",
            "| epoch   9 |  5800/114000 batches | accuracy    0.842 | loss    0.462\n",
            "| epoch   9 |  6000/114000 batches | accuracy    0.850 | loss    0.276\n",
            "| epoch   9 |  6200/114000 batches | accuracy    0.853 | loss    0.478\n",
            "| epoch   9 |  6400/114000 batches | accuracy    0.853 | loss    0.272\n",
            "| epoch   9 |  6600/114000 batches | accuracy    0.839 | loss    0.299\n",
            "| epoch   9 |  6800/114000 batches | accuracy    0.837 | loss    0.437\n",
            "| epoch   9 |  7000/114000 batches | accuracy    0.846 | loss    0.044\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 19.77s | valid accuracy    0.844 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   200/114000 batches | accuracy    0.846 | loss    0.668\n",
            "| epoch  10 |   400/114000 batches | accuracy    0.848 | loss    0.533\n",
            "| epoch  10 |   600/114000 batches | accuracy    0.851 | loss    0.912\n",
            "| epoch  10 |   800/114000 batches | accuracy    0.848 | loss    0.148\n",
            "| epoch  10 |  1000/114000 batches | accuracy    0.848 | loss    0.606\n",
            "| epoch  10 |  1200/114000 batches | accuracy    0.851 | loss    0.107\n",
            "| epoch  10 |  1400/114000 batches | accuracy    0.848 | loss    0.356\n",
            "| epoch  10 |  1600/114000 batches | accuracy    0.845 | loss    0.485\n",
            "| epoch  10 |  1800/114000 batches | accuracy    0.848 | loss    0.187\n",
            "| epoch  10 |  2000/114000 batches | accuracy    0.851 | loss    0.316\n",
            "| epoch  10 |  2200/114000 batches | accuracy    0.856 | loss    0.271\n",
            "| epoch  10 |  2400/114000 batches | accuracy    0.847 | loss    0.483\n",
            "| epoch  10 |  2600/114000 batches | accuracy    0.852 | loss    0.500\n",
            "| epoch  10 |  2800/114000 batches | accuracy    0.852 | loss    0.219\n",
            "| epoch  10 |  3000/114000 batches | accuracy    0.847 | loss    0.810\n",
            "| epoch  10 |  3200/114000 batches | accuracy    0.838 | loss    0.043\n",
            "| epoch  10 |  3400/114000 batches | accuracy    0.853 | loss    0.545\n",
            "| epoch  10 |  3600/114000 batches | accuracy    0.855 | loss    0.153\n",
            "| epoch  10 |  3800/114000 batches | accuracy    0.851 | loss    0.506\n",
            "| epoch  10 |  4000/114000 batches | accuracy    0.853 | loss    0.391\n",
            "| epoch  10 |  4200/114000 batches | accuracy    0.847 | loss    0.238\n",
            "| epoch  10 |  4400/114000 batches | accuracy    0.860 | loss    0.189\n",
            "| epoch  10 |  4600/114000 batches | accuracy    0.855 | loss    0.484\n",
            "| epoch  10 |  4800/114000 batches | accuracy    0.850 | loss    0.534\n",
            "| epoch  10 |  5000/114000 batches | accuracy    0.854 | loss    0.443\n",
            "| epoch  10 |  5200/114000 batches | accuracy    0.850 | loss    0.712\n",
            "| epoch  10 |  5400/114000 batches | accuracy    0.855 | loss    0.049\n",
            "| epoch  10 |  5600/114000 batches | accuracy    0.858 | loss    0.679\n",
            "| epoch  10 |  5800/114000 batches | accuracy    0.849 | loss    0.393\n",
            "| epoch  10 |  6000/114000 batches | accuracy    0.845 | loss    0.278\n",
            "| epoch  10 |  6200/114000 batches | accuracy    0.848 | loss    0.455\n",
            "| epoch  10 |  6400/114000 batches | accuracy    0.852 | loss    0.167\n",
            "| epoch  10 |  6600/114000 batches | accuracy    0.860 | loss    0.212\n",
            "| epoch  10 |  6800/114000 batches | accuracy    0.851 | loss    0.130\n",
            "| epoch  10 |  7000/114000 batches | accuracy    0.856 | loss    0.514\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 19.78s | valid accuracy    0.862 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.856\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, 10 + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader, model, optimizer, loss_fn, epoch)\n",
        "    accu_val = evaluate(valid_dataloader, model)\n",
        "    print(\"-\" * 59)\n",
        "    print(\n",
        "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
        "        \"valid accuracy {:8.3f} \".format(\n",
        "            epoch,\n",
        "            time.time() - epoch_start_time,\n",
        "            accu_val\n",
        "            )\n",
        "    )\n",
        "    print(\"-\" * 59)\n",
        "\n",
        "print(\"Checking the results of test dataset.\")\n",
        "accu_test = evaluate(test_dataloader, model)\n",
        "print(\"test accuracy {:8.3f}\".format(accu_test))"
      ],
      "id": "21ba24f3"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "71904d5f",
        "outputId": "bc2a5402-d4cb-44b1-eac8-2dbfe9f00645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input tensor([[ 1.3121,  0.6457,  2.2339, -0.3085, -0.3124],\n",
            "        [-0.2546,  1.1068,  0.2541, -1.4846,  0.3374],\n",
            "        [-0.4230,  1.3519, -1.6076,  2.0032,  1.3166]], requires_grad=True) target tensor([0, 2, 3])\n"
          ]
        }
      ],
      "source": [],
      "id": "71904d5f"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}